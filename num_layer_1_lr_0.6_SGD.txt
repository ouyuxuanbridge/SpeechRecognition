Namespace(seed=123, train_json='/rds/user/yo279/hpc-work/MLMI2/fbanks/train_fbank.json', val_json='/rds/user/yo279/hpc-work/MLMI2/fbanks/dev_fbank.json', test_json='/rds/user/yo279/hpc-work/MLMI2/fbanks/test_fbank.json', batch_size=4, num_layers=1, fbank_dims=23, model_dims=128, concat=1, lr=0.6, vocab='vocab_39.txt', report_interval=50, num_epochs=20, dropout_rate=0.5, clip_max_norm=1)
Total number of model parameters is 166952
EPOCH 1:
  batch 50 loss: 4.318649053573608
  batch 100 loss: 3.1401747560501097
  batch 150 loss: 2.9893379402160645
  batch 200 loss: 2.8802622175216674
  batch 250 loss: 2.7543557929992675
  batch 300 loss: 2.5560878467559816
  batch 350 loss: 2.4337836647033693
  batch 400 loss: 2.3586631321907046
  batch 450 loss: 2.338863115310669
  batch 500 loss: 2.237923948764801
  batch 550 loss: 2.2509891963005066
  batch 600 loss: 2.1178224587440493
  batch 650 loss: 1.981765398979187
  batch 700 loss: 1.9769461393356322
  batch 750 loss: 1.9175062251091004
  batch 800 loss: 1.9265344524383545
  batch 850 loss: 1.8914166235923766
  batch 900 loss: 1.8145058393478393
LOSS train 1.81451 valid 1.80047, valid PER 65.73%
EPOCH 2:
  batch 50 loss: 1.7794163203239441
  batch 100 loss: 1.7104371500015259
  batch 150 loss: 1.6801788878440858
  batch 200 loss: 1.7537147188186646
  batch 250 loss: 1.755789680480957
  batch 300 loss: 1.7395689868927002
  batch 350 loss: 1.586254563331604
  batch 400 loss: 1.6025311279296874
  batch 450 loss: 1.5430786848068236
  batch 500 loss: 1.5796259212493897
  batch 550 loss: 1.5636030864715575
  batch 600 loss: 1.5120016741752624
  batch 650 loss: 1.5560903573036193
  batch 700 loss: 1.54068363904953
  batch 750 loss: 1.504983036518097
  batch 800 loss: 1.4495848321914673
  batch 850 loss: 1.4520482802391053
  batch 900 loss: 1.4782782149314881
LOSS train 1.47828 valid 1.40562, valid PER 43.41%
EPOCH 3:
  batch 50 loss: 1.446610836982727
  batch 100 loss: 1.3984971308708192
  batch 150 loss: 1.3906735372543335
  batch 200 loss: 1.3331082797050475
  batch 250 loss: 1.3213484168052674
  batch 300 loss: 1.2819706726074218
  batch 350 loss: 1.3775618600845336
  batch 400 loss: 1.3340742564201356
  batch 450 loss: 1.3716267609596253
  batch 500 loss: 1.300491771697998
  batch 550 loss: 1.3875148367881776
  batch 600 loss: 1.3885480391979217
  batch 650 loss: 1.287306044101715
  batch 700 loss: 1.3062516713142396
  batch 750 loss: 1.454638167619705
  batch 800 loss: 1.3100422048568725
  batch 850 loss: 1.3277415657043456
  batch 900 loss: 1.2950764417648315
LOSS train 1.29508 valid 1.32184, valid PER 40.25%
EPOCH 4:
  batch 50 loss: 1.241822190284729
  batch 100 loss: 1.28457270860672
  batch 150 loss: 1.212028671503067
  batch 200 loss: 1.2511508083343506
  batch 250 loss: 1.2509975600242615
  batch 300 loss: 1.2604702663421632
  batch 350 loss: 1.1673107600212098
  batch 400 loss: 1.2124768555164338
  batch 450 loss: 1.199230592250824
  batch 500 loss: 1.1780495274066924
  batch 550 loss: 1.231975622177124
  batch 600 loss: 1.2190316438674926
  batch 650 loss: 1.2407779455184937
  batch 700 loss: 1.176679301261902
  batch 750 loss: 1.1598240482807158
  batch 800 loss: 1.1279406082630157
  batch 850 loss: 1.2010004222393036
  batch 900 loss: 1.2629912447929383
LOSS train 1.26299 valid 1.31131, valid PER 40.06%
EPOCH 5:
  batch 50 loss: 1.1565808641910553
  batch 100 loss: 1.1531107783317567
  batch 150 loss: 1.189241313934326
  batch 200 loss: 1.1212860691547393
  batch 250 loss: 1.1556786119937896
  batch 300 loss: 1.132653796672821
  batch 350 loss: 1.142139196395874
  batch 400 loss: 1.1345308303833008
  batch 450 loss: 1.1405745434761048
  batch 500 loss: 1.1400353169441224
  batch 550 loss: 1.0762940275669097
  batch 600 loss: 1.1503548884391785
  batch 650 loss: 1.108268827199936
  batch 700 loss: 1.1291175448894502
  batch 750 loss: 1.0652492928504944
  batch 800 loss: 1.1022430491447448
  batch 850 loss: 1.1055291962623597
  batch 900 loss: 1.1408141648769379
LOSS train 1.14081 valid 1.12070, valid PER 35.14%
EPOCH 6:
  batch 50 loss: 1.1403206074237824
  batch 100 loss: 1.1155197930335998
  batch 150 loss: 1.0473651969432831
  batch 200 loss: 1.0659956896305085
  batch 250 loss: 1.0946189439296723
  batch 300 loss: 1.064481040239334
  batch 350 loss: 1.0978796565532685
  batch 400 loss: 1.0825162816047669
  batch 450 loss: 1.1136959743499757
  batch 500 loss: 1.0619878625869752
  batch 550 loss: 1.1214358067512513
  batch 600 loss: 1.0410192787647248
  batch 650 loss: 1.0931199169158936
  batch 700 loss: 1.104540935754776
  batch 750 loss: 1.0998743498325347
  batch 800 loss: 1.0552908456325532
  batch 850 loss: 1.0838817131519318
  batch 900 loss: 1.0626391577720642
LOSS train 1.06264 valid 1.12281, valid PER 34.71%
EPOCH 7:
  batch 50 loss: 1.1551113188266755
  batch 100 loss: 1.10194779753685
  batch 150 loss: 1.048793100118637
  batch 200 loss: 0.9950208723545074
  batch 250 loss: 1.0320525121688844
  batch 300 loss: 1.0488349103927612
  batch 350 loss: 1.060773847103119
  batch 400 loss: 1.0555189263820648
  batch 450 loss: 1.0788461208343505
  batch 500 loss: 1.0462019169330596
  batch 550 loss: 1.043119262456894
  batch 600 loss: 1.0266050338745116
  batch 650 loss: 0.9976606822013855
  batch 700 loss: 1.0377456164360046
  batch 750 loss: 1.0351937186717988
  batch 800 loss: 1.0103268468379973
  batch 850 loss: 1.0340052211284638
  batch 900 loss: 1.0625414955615997
LOSS train 1.06254 valid 1.07271, valid PER 34.18%
EPOCH 8:
  batch 50 loss: 0.9875766682624817
  batch 100 loss: 0.9882709181308746
  batch 150 loss: 0.9696292293071747
  batch 200 loss: 0.9499614357948303
  batch 250 loss: 1.0537117099761963
  batch 300 loss: 1.0090962648391724
  batch 350 loss: 1.0688216519355773
  batch 400 loss: 0.9820220923423767
  batch 450 loss: 0.9997713541984559
  batch 500 loss: 1.0459381330013275
  batch 550 loss: 0.9872432208061218
  batch 600 loss: 1.0302163469791412
  batch 650 loss: 1.0385751616954804
  batch 700 loss: 0.9834252977371216
  batch 750 loss: 1.0324015295505524
  batch 800 loss: 1.0325998222827912
  batch 850 loss: 1.0142181527614593
  batch 900 loss: 0.9990796840190888
LOSS train 0.99908 valid 1.04869, valid PER 32.51%
EPOCH 9:
  batch 50 loss: 0.9254114019870758
  batch 100 loss: 0.9842539393901825
  batch 150 loss: 1.0280605828762055
  batch 200 loss: 0.9703265142440796
  batch 250 loss: 0.9982609879970551
  batch 300 loss: 0.9732551896572113
  batch 350 loss: 0.9789380979537964
  batch 400 loss: 0.9661691856384277
  batch 450 loss: 0.9941295909881592
  batch 500 loss: 0.943492124080658
  batch 550 loss: 0.9770988857746125
  batch 600 loss: 1.0069198203086853
  batch 650 loss: 0.975420196056366
  batch 700 loss: 0.9686946606636048
  batch 750 loss: 0.9384078216552735
  batch 800 loss: 0.9872705447673797
  batch 850 loss: 0.9995174670219421
  batch 900 loss: 0.9496684741973876
LOSS train 0.94967 valid 1.04191, valid PER 32.82%
EPOCH 10:
  batch 50 loss: 0.9107765114307403
  batch 100 loss: 0.9278742563724518
  batch 150 loss: 0.9453951394557953
  batch 200 loss: 0.9605213630199433
  batch 250 loss: 0.9509698486328125
  batch 300 loss: 0.9023710000514984
  batch 350 loss: 0.9556478416919708
  batch 400 loss: 0.9114395534992218
  batch 450 loss: 0.9485289990901947
  batch 500 loss: 0.993615974187851
  batch 550 loss: 0.9826788854598999
  batch 600 loss: 0.9420642149448395
  batch 650 loss: 0.9199059247970581
  batch 700 loss: 0.954955884218216
  batch 750 loss: 0.9271933507919311
  batch 800 loss: 0.9560846531391144
  batch 850 loss: 1.0060055696964263
  batch 900 loss: 0.9398015713691712
LOSS train 0.93980 valid 1.03756, valid PER 32.58%
EPOCH 11:
  batch 50 loss: 0.8811880314350128
  batch 100 loss: 0.87411550283432
  batch 150 loss: 0.8988291335105896
  batch 200 loss: 0.9653826510906219
  batch 250 loss: 0.9497356986999512
  batch 300 loss: 0.9265797328948975
  batch 350 loss: 0.9288980174064636
  batch 400 loss: 0.9459702157974244
  batch 450 loss: 0.9166688776016235
  batch 500 loss: 0.9144881618022919
  batch 550 loss: 0.9295454549789429
  batch 600 loss: 0.9248460245132446
  batch 650 loss: 0.9563748931884766
  batch 700 loss: 0.8861912047863006
  batch 750 loss: 0.9323239779472351
  batch 800 loss: 0.9608312714099884
  batch 850 loss: 0.9781498265266418
  batch 900 loss: 0.9725722551345826
LOSS train 0.97257 valid 1.03502, valid PER 32.73%
EPOCH 12:
  batch 50 loss: 0.9083806478977203
  batch 100 loss: 0.8882128727436066
  batch 150 loss: 0.8769333529472351
  batch 200 loss: 0.8864292180538178
  batch 250 loss: 0.9307560276985168
  batch 300 loss: 0.9015120542049408
  batch 350 loss: 0.9132136309146881
  batch 400 loss: 0.9536181688308716
  batch 450 loss: 1.190347535610199
  batch 500 loss: 1.021551367044449
  batch 550 loss: 0.9056784093379975
  batch 600 loss: 0.91876229763031
  batch 650 loss: 0.9419187295436859
  batch 700 loss: 0.9714320194721222
  batch 750 loss: 0.9170035541057586
  batch 800 loss: 0.9128467690944672
  batch 850 loss: 0.9456724834442138
  batch 900 loss: 0.9608751010894775
LOSS train 0.96088 valid 1.06286, valid PER 33.20%
EPOCH 13:
  batch 50 loss: 0.9244266211986542
  batch 100 loss: 0.905909287929535
  batch 150 loss: 0.8848262906074524
  batch 200 loss: 0.8955532515048981
  batch 250 loss: 0.8764127814769744
  batch 300 loss: 0.864637644290924
  batch 350 loss: 0.8854913902282715
  batch 400 loss: 0.8881259095668793
  batch 450 loss: 0.8928273510932923
  batch 500 loss: 0.8594535768032074
  batch 550 loss: 0.8970001232624054
  batch 600 loss: 0.9170175981521607
  batch 650 loss: 0.9059086620807648
  batch 700 loss: 0.8876006960868835
  batch 750 loss: 0.854136757850647
  batch 800 loss: 0.8587327826023102
  batch 850 loss: 0.8888472235202789
  batch 900 loss: 0.9119710469245911
LOSS train 0.91197 valid 1.01629, valid PER 31.51%
EPOCH 14:
  batch 50 loss: 0.8577654576301574
  batch 100 loss: 0.8740785026550293
  batch 150 loss: 0.8709517395496369
  batch 200 loss: 0.8786935651302338
  batch 250 loss: 0.8653276348114014
  batch 300 loss: 0.8879260003566742
  batch 350 loss: 0.8577866351604462
  batch 400 loss: 0.8395109784603119
  batch 450 loss: 0.8412681806087494
  batch 500 loss: 0.869460608959198
  batch 550 loss: 0.8586638605594635
  batch 600 loss: 0.8264293479919433
  batch 650 loss: 0.8797977876663208
  batch 700 loss: 0.8814011549949646
  batch 750 loss: 0.8430992889404297
  batch 800 loss: 0.829334979057312
  batch 850 loss: 0.8825249195098877
  batch 900 loss: 0.8806910812854767
LOSS train 0.88069 valid 1.03055, valid PER 32.05%
EPOCH 15:
  batch 50 loss: 0.8311730754375458
  batch 100 loss: 0.8124262702465057
  batch 150 loss: 0.8164323318004608
  batch 200 loss: 0.8671286308765411
  batch 250 loss: 0.8372759437561035
  batch 300 loss: 0.827485340833664
  batch 350 loss: 0.8421427023410797
  batch 400 loss: 0.9308863723278046
  batch 450 loss: 0.918594880104065
  batch 500 loss: 0.8728740513324738
  batch 550 loss: 0.9484381997585296
  batch 600 loss: 0.9333960318565369
  batch 650 loss: 0.9062174224853515
  batch 700 loss: 0.9444811952114105
  batch 750 loss: 0.9173948323726654
  batch 800 loss: 0.8962902700901032
  batch 850 loss: 0.9232263803482056
  batch 900 loss: 0.899879869222641
LOSS train 0.89988 valid 1.06300, valid PER 32.02%
EPOCH 16:
  batch 50 loss: 0.8757507431507111
  batch 100 loss: 0.8263503587245942
  batch 150 loss: 0.8501909577846527
  batch 200 loss: 0.8413840246200561
  batch 250 loss: 0.8991825687885284
  batch 300 loss: 0.8705098223686218
  batch 350 loss: 1.0004064977169036
  batch 400 loss: 1.0062497866153717
  batch 450 loss: 0.9227196764945984
  batch 500 loss: 0.8618906092643738
  batch 550 loss: 0.9051333880424499
  batch 600 loss: 0.884214231967926
  batch 650 loss: 0.9065169191360474
  batch 700 loss: 0.8692910504341126
  batch 750 loss: 0.9604026877880096
  batch 800 loss: 0.9615247404575348
  batch 850 loss: 0.9187795865535736
  batch 900 loss: 0.9414524888992309
LOSS train 0.94145 valid 1.13024, valid PER 33.08%
EPOCH 17:
  batch 50 loss: 0.9568531930446624
  batch 100 loss: 0.9126900577545166
  batch 150 loss: 0.8982900440692901
  batch 200 loss: 0.8639547896385192
  batch 250 loss: 0.9027981925010681
  batch 300 loss: 0.9111159205436706
  batch 350 loss: 0.9392761874198914
  batch 400 loss: 0.9287405514717102
  batch 450 loss: 0.9389006638526917
  batch 500 loss: 0.864581435918808
  batch 550 loss: 0.9477427721023559
  batch 600 loss: 0.9687721526622772
  batch 650 loss: 0.8872447311878204
  batch 700 loss: 1.0494907701015472
  batch 750 loss: 0.97800626039505
  batch 800 loss: 0.9805908560752868
  batch 850 loss: 0.9684404921531677
  batch 900 loss: 0.9285387825965882
LOSS train 0.92854 valid 1.07481, valid PER 33.00%
EPOCH 18:
  batch 50 loss: 0.9506856322288513
  batch 100 loss: 0.9502297604084015
  batch 150 loss: 0.9373010861873626
  batch 200 loss: 0.8969799101352691
  batch 250 loss: 0.9103291404247283
  batch 300 loss: 0.8512399029731751
  batch 350 loss: 0.9174059653282165
  batch 400 loss: 0.8637848627567292
  batch 450 loss: 0.9071707391738891
  batch 500 loss: 0.8970276415348053
  batch 550 loss: 0.907443437576294
  batch 600 loss: 0.9451851582527161
  batch 650 loss: 0.9403173613548279
  batch 700 loss: 0.9724780917167664
  batch 750 loss: 0.9291293692588806
  batch 800 loss: 0.9036366677284241
  batch 850 loss: 0.9211485922336579
  batch 900 loss: 0.9447474694252014
LOSS train 0.94475 valid 1.05319, valid PER 32.86%
EPOCH 19:
  batch 50 loss: 0.8323663926124573
  batch 100 loss: 0.9822360038757324
  batch 150 loss: 1.2120324659347534
  batch 200 loss: 1.049947509765625
  batch 250 loss: 0.9837455463409424
  batch 300 loss: 0.9620835757255555
  batch 350 loss: 0.9083879745006561
  batch 400 loss: 0.9625439023971558
  batch 450 loss: 0.980954247713089
  batch 500 loss: 0.9542341184616089
  batch 550 loss: 1.0366831886768342
  batch 600 loss: 0.9697059428691864
  batch 650 loss: 1.0236571824550629
  batch 700 loss: 0.9413183677196503
  batch 750 loss: 0.8941127431392669
  batch 800 loss: 0.9464186096191406
  batch 850 loss: 0.9483751511573791
  batch 900 loss: 0.9220548355579377
LOSS train 0.92205 valid 1.08828, valid PER 33.90%
EPOCH 20:
  batch 50 loss: 0.883014897108078
  batch 100 loss: 0.8870148551464081
  batch 150 loss: 0.8753894639015197
  batch 200 loss: 0.8764930236339569
  batch 250 loss: 0.8898374164104461
  batch 300 loss: 0.9264127326011657
  batch 350 loss: 0.894859355688095
  batch 400 loss: 0.8939511382579803
  batch 450 loss: 0.9081100487709045
  batch 500 loss: 0.8775323760509491
  batch 550 loss: 0.9242912662029267
  batch 600 loss: 0.9325506091117859
  batch 650 loss: 0.9300059318542481
  batch 700 loss: 0.9518723285198212
  batch 750 loss: 0.8900048971176148
  batch 800 loss: 0.9221365320682525
  batch 850 loss: 0.8940708339214325
  batch 900 loss: 0.9051292836666107
LOSS train 0.90513 valid 1.07065, valid PER 32.28%
train_loss
[1.8145058393478393, 1.4782782149314881, 1.2950764417648315, 1.2629912447929383, 1.1408141648769379, 1.0626391577720642, 1.0625414955615997, 0.9990796840190888, 0.9496684741973876, 0.9398015713691712, 0.9725722551345826, 0.9608751010894775, 0.9119710469245911, 0.8806910812854767, 0.899879869222641, 0.9414524888992309, 0.9285387825965882, 0.9447474694252014, 0.9220548355579377, 0.9051292836666107]
valid_loss
[1.8004660606384277, 1.4056200981140137, 1.3218410015106201, 1.3113127946853638, 1.1207045316696167, 1.1228060722351074, 1.0727148056030273, 1.0486942529678345, 1.041908621788025, 1.0375572443008423, 1.035018801689148, 1.0628628730773926, 1.016286849975586, 1.0305511951446533, 1.0629996061325073, 1.1302435398101807, 1.0748088359832764, 1.0531939268112183, 1.0882761478424072, 1.0706452131271362]
valid_per
[65.73123583522197, 43.41421143847487, 40.24796693774164, 40.061325156645786, 35.14198106919078, 34.70870550593254, 34.18210905212638, 32.50899880015998, 32.81562458338888, 32.575656579122786, 32.72896947073723, 33.202239701373145, 31.509132115717904, 32.04906012531662, 32.0223970137315, 33.07558992134382, 32.99560058658845, 32.855619250766566, 33.9021463804826, 32.275696573790164]
Training finished in 3.0 minutes.
Model saved to checkpoints/20231208_090912/model_13
Loading model from checkpoints/20231208_090912/model_13
SUB: 18.30%, DEL: 12.54%, INS: 2.67%, COR: 69.16%, PER: 33.50%
