Namespace(seed=123, train_json='/rds/user/yo279/hpc-work/MLMI2/fbanks/train_fbank.json', val_json='/rds/user/yo279/hpc-work/MLMI2/fbanks/dev_fbank.json', test_json='/rds/user/yo279/hpc-work/MLMI2/fbanks/test_fbank.json', batch_size=4, num_layers=1, fbank_dims=23, model_dims=256, concat=1, lr=0.5, vocab='vocab_39.txt', report_interval=50, num_epochs=20, dropout_rate=0.5, clip_max_norm=1)
Total number of model parameters is 596008
EPOCH 1:
  batch 50 loss: 4.131501455307006
  batch 100 loss: 3.1822887563705446
  batch 150 loss: 3.0208824729919432
  batch 200 loss: 2.9258014965057373
  batch 250 loss: 2.9141006660461426
  batch 300 loss: 2.673811130523682
  batch 350 loss: 2.480140118598938
  batch 400 loss: 2.437259931564331
  batch 450 loss: 2.3810118532180784
  batch 500 loss: 2.276589982509613
  batch 550 loss: 2.1431336402893066
  batch 600 loss: 2.0519166016578674
  batch 650 loss: 1.984303321838379
  batch 700 loss: 1.966110622882843
  batch 750 loss: 1.8874748158454895
  batch 800 loss: 1.8716907739639281
  batch 850 loss: 1.8042434716224671
  batch 900 loss: 1.7826184821128845
LOSS train 1.78262 valid 1.74777, valid PER 66.66%
EPOCH 2:
  batch 50 loss: 1.7411404061317444
  batch 100 loss: 1.6750635528564453
  batch 150 loss: 1.6527062726020814
  batch 200 loss: 1.6396594476699828
  batch 250 loss: 1.6736571264266968
  batch 300 loss: 1.6010468053817748
  batch 350 loss: 1.501500017642975
  batch 400 loss: 1.5201291036605835
  batch 450 loss: 1.4442450523376464
  batch 500 loss: 1.4844039869308472
  batch 550 loss: 1.4936736130714416
  batch 600 loss: 1.4187591218948363
  batch 650 loss: 1.433404176235199
  batch 700 loss: 1.435024118423462
  batch 750 loss: 1.4030091428756715
  batch 800 loss: 1.3320738863945008
  batch 850 loss: 1.3383205771446227
  batch 900 loss: 1.3598978734016418
LOSS train 1.35990 valid 1.34121, valid PER 41.29%
EPOCH 3:
  batch 50 loss: 1.3070442485809326
  batch 100 loss: 1.3070340168476104
  batch 150 loss: 1.2972053074836731
  batch 200 loss: 1.278072214126587
  batch 250 loss: 1.2749558115005493
  batch 300 loss: 1.2545434248447418
  batch 350 loss: 1.281752188205719
  batch 400 loss: 1.2565814411640168
  batch 450 loss: 1.2383784639835358
  batch 500 loss: 1.2189535045623778
  batch 550 loss: 1.247591495513916
  batch 600 loss: 1.2373026895523072
  batch 650 loss: 1.1709958875179292
  batch 700 loss: 1.2071031093597413
  batch 750 loss: 1.2637461268901824
  batch 800 loss: 1.2143079292774202
  batch 850 loss: 1.209556016921997
  batch 900 loss: 1.147002899646759
LOSS train 1.14700 valid 1.22191, valid PER 37.57%
EPOCH 4:
  batch 50 loss: 1.131588681936264
  batch 100 loss: 1.1554187059402465
  batch 150 loss: 1.1195900046825409
  batch 200 loss: 1.1407865917682647
  batch 250 loss: 1.1349772655963897
  batch 300 loss: 1.1438007962703705
  batch 350 loss: 1.0663277006149292
  batch 400 loss: 1.1084195637702943
  batch 450 loss: 1.1013559091091156
  batch 500 loss: 1.0908241832256318
  batch 550 loss: 1.1007174587249755
  batch 600 loss: 1.1362548053264618
  batch 650 loss: 1.1176331782341002
  batch 700 loss: 1.0878338074684144
  batch 750 loss: 1.0699760520458221
  batch 800 loss: 1.0374869894981384
  batch 850 loss: 1.0889904177188874
  batch 900 loss: 1.1250517547130585
LOSS train 1.12505 valid 1.10080, valid PER 33.79%
EPOCH 5:
  batch 50 loss: 1.0230989277362823
  batch 100 loss: 1.0275575006008149
  batch 150 loss: 1.0801155483722686
  batch 200 loss: 1.0085387456417083
  batch 250 loss: 1.0153113830089568
  batch 300 loss: 1.0202052795886993
  batch 350 loss: 1.0159522557258607
  batch 400 loss: 1.0290791308879852
  batch 450 loss: 1.0175213313102722
  batch 500 loss: 1.0325568306446076
  batch 550 loss: 1.0001974380016327
  batch 600 loss: 1.0601084482669831
  batch 650 loss: 1.0225468587875366
  batch 700 loss: 1.0622426462173462
  batch 750 loss: 0.974466222524643
  batch 800 loss: 1.0652728283405304
  batch 850 loss: 1.0362024915218353
  batch 900 loss: 1.0246374011039734
LOSS train 1.02464 valid 1.06118, valid PER 33.36%
EPOCH 6:
  batch 50 loss: 1.021391223669052
  batch 100 loss: 0.9490118873119354
  batch 150 loss: 0.9477064239978791
  batch 200 loss: 0.9888927781581879
  batch 250 loss: 0.9925706136226654
  batch 300 loss: 0.9709028589725495
  batch 350 loss: 0.960440354347229
  batch 400 loss: 0.9419785809516906
  batch 450 loss: 1.0132435941696167
  batch 500 loss: 0.9736192214488983
  batch 550 loss: 0.9754207456111907
  batch 600 loss: 0.9419222819805145
  batch 650 loss: 0.9586090850830078
  batch 700 loss: 0.9595737838745118
  batch 750 loss: 0.9474832928180694
  batch 800 loss: 0.9617119121551514
  batch 850 loss: 0.9413625824451447
  batch 900 loss: 0.9609994840621948
LOSS train 0.96100 valid 1.04837, valid PER 32.37%
EPOCH 7:
  batch 50 loss: 0.9099509143829345
  batch 100 loss: 0.9234055280685425
  batch 150 loss: 0.9155176723003388
  batch 200 loss: 0.8839990985393524
  batch 250 loss: 0.8926435983181
  batch 300 loss: 0.898784966468811
  batch 350 loss: 0.9157126319408416
  batch 400 loss: 0.896358460187912
  batch 450 loss: 0.9093202733993531
  batch 500 loss: 0.896527384519577
  batch 550 loss: 0.9065768957138062
  batch 600 loss: 0.8965364623069764
  batch 650 loss: 0.9015247523784637
  batch 700 loss: 0.9248862099647522
  batch 750 loss: 0.9075230383872985
  batch 800 loss: 0.8888142251968384
  batch 850 loss: 0.895898026227951
  batch 900 loss: 0.9478767156600952
LOSS train 0.94788 valid 1.00994, valid PER 31.47%
EPOCH 8:
  batch 50 loss: 0.8687863802909851
  batch 100 loss: 0.8657828903198242
  batch 150 loss: 0.8598633229732513
  batch 200 loss: 0.83774209856987
  batch 250 loss: 0.8652610599994659
  batch 300 loss: 0.81371022939682
  batch 350 loss: 0.8876752412319183
  batch 400 loss: 0.849007009267807
  batch 450 loss: 0.8733139884471893
  batch 500 loss: 0.898288118839264
  batch 550 loss: 0.8211004388332367
  batch 600 loss: 0.8774159026145935
  batch 650 loss: 0.9119538915157318
  batch 700 loss: 0.8507733845710754
  batch 750 loss: 0.8609078550338745
  batch 800 loss: 0.8756533122062683
  batch 850 loss: 0.8571682798862458
  batch 900 loss: 0.8677547150850295
LOSS train 0.86775 valid 0.96616, valid PER 29.70%
EPOCH 9:
  batch 50 loss: 0.7747047483921051
  batch 100 loss: 0.8131915044784546
  batch 150 loss: 0.8410022222995758
  batch 200 loss: 0.790066603422165
  batch 250 loss: 0.8186068379878998
  batch 300 loss: 0.8195328652858734
  batch 350 loss: 0.8374922490119934
  batch 400 loss: 0.8243521595001221
  batch 450 loss: 0.8345695209503173
  batch 500 loss: 0.8071104109287262
  batch 550 loss: 0.8495092809200286
  batch 600 loss: 0.8640366089344025
  batch 650 loss: 0.8363178062438965
  batch 700 loss: 0.8005919098854065
  batch 750 loss: 0.8468776977062226
  batch 800 loss: 0.8344533014297485
  batch 850 loss: 0.8409564816951751
  batch 900 loss: 0.8162390124797821
LOSS train 0.81624 valid 0.96591, valid PER 29.51%
EPOCH 10:
  batch 50 loss: 0.7271613544225692
  batch 100 loss: 0.7579674708843231
  batch 150 loss: 0.7941782915592194
  batch 200 loss: 0.800758159160614
  batch 250 loss: 0.8003811001777649
  batch 300 loss: 0.7486146223545075
  batch 350 loss: 0.7929080367088318
  batch 400 loss: 0.7457023221254349
  batch 450 loss: 0.7423665463924408
  batch 500 loss: 0.7871781897544861
  batch 550 loss: 0.801996854543686
  batch 600 loss: 0.7679274654388428
  batch 650 loss: 0.7711812728643417
  batch 700 loss: 0.7850728631019592
  batch 750 loss: 0.7687463128566742
  batch 800 loss: 0.7900657176971435
  batch 850 loss: 0.7996409964561463
  batch 900 loss: 0.803644562959671
LOSS train 0.80364 valid 0.96082, valid PER 30.35%
EPOCH 11:
  batch 50 loss: 0.7076195359230042
  batch 100 loss: 0.6767574381828309
  batch 150 loss: 0.7103736078739167
  batch 200 loss: 0.7444517010450363
  batch 250 loss: 0.7699262762069702
  batch 300 loss: 0.7309671854972839
  batch 350 loss: 0.7296440857648849
  batch 400 loss: 0.828038455247879
  batch 450 loss: 0.7805303359031677
  batch 500 loss: 0.7573299527168273
  batch 550 loss: 0.757081573009491
  batch 600 loss: 0.7532545554637909
  batch 650 loss: 0.813336365222931
  batch 700 loss: 0.7300235277414322
  batch 750 loss: 0.7400750827789306
  batch 800 loss: 0.7788406121730804
  batch 850 loss: 0.7804640328884125
  batch 900 loss: 0.7906376099586487
LOSS train 0.79064 valid 0.95370, valid PER 28.80%
EPOCH 12:
  batch 50 loss: 0.699095705151558
  batch 100 loss: 0.6852184396982193
  batch 150 loss: 0.6737455379962921
  batch 200 loss: 0.6868963408470153
  batch 250 loss: 0.7077855545282364
  batch 300 loss: 0.6979210638999939
  batch 350 loss: 0.6972232306003571
  batch 400 loss: 0.7229734772443771
  batch 450 loss: 0.7368410193920135
  batch 500 loss: 0.7452010631561279
  batch 550 loss: 0.7036145251989364
  batch 600 loss: 0.7196946668624878
  batch 650 loss: 0.7711215102672577
  batch 700 loss: 0.760448921918869
  batch 750 loss: 0.7237131291627884
  batch 800 loss: 0.747816281914711
  batch 850 loss: 0.7677293419837952
  batch 900 loss: 0.753983405828476
LOSS train 0.75398 valid 0.92249, valid PER 28.22%
EPOCH 13:
  batch 50 loss: 0.6619829750061035
  batch 100 loss: 0.664485068321228
  batch 150 loss: 0.6512912756204605
  batch 200 loss: 0.6681592971086502
  batch 250 loss: 0.6765388482809067
  batch 300 loss: 0.6646406024694442
  batch 350 loss: 0.6787567412853241
  batch 400 loss: 0.6844022530317306
  batch 450 loss: 0.685156187415123
  batch 500 loss: 0.6679578149318695
  batch 550 loss: 0.7058293855190277
  batch 600 loss: 0.6755754923820496
  batch 650 loss: 0.7029648840427398
  batch 700 loss: 0.707680560350418
  batch 750 loss: 0.6658134078979492
  batch 800 loss: 0.6627740174531936
  batch 850 loss: 0.6994144541025161
  batch 900 loss: 0.7059869927167892
LOSS train 0.70599 valid 0.94008, valid PER 27.82%
EPOCH 14:
  batch 50 loss: 0.6062531369924545
  batch 100 loss: 0.6292035835981369
  batch 150 loss: 0.6452648508548736
  batch 200 loss: 0.6553905093669892
  batch 250 loss: 0.6781568539142608
  batch 300 loss: 0.6738383984565735
  batch 350 loss: 0.6278306722640992
  batch 400 loss: 0.6397579622268676
  batch 450 loss: 0.6374110168218613
  batch 500 loss: 0.6491977393627166
  batch 550 loss: 0.6480244541168213
  batch 600 loss: 0.6390189856290818
  batch 650 loss: 0.6533194571733475
  batch 700 loss: 0.6830008995532989
  batch 750 loss: 0.6512688535451889
  batch 800 loss: 0.6670755928754807
  batch 850 loss: 0.7004776334762574
  batch 900 loss: 0.7200270414352417
LOSS train 0.72003 valid 0.94883, valid PER 28.12%
EPOCH 15:
  batch 50 loss: 0.5969407373666763
  batch 100 loss: 0.5881504529714584
  batch 150 loss: 0.6054847800731659
  batch 200 loss: 0.6336226433515548
  batch 250 loss: 0.6284300696849823
  batch 300 loss: 0.6081027257442474
  batch 350 loss: 0.6157426863908768
  batch 400 loss: 0.6273882514238358
  batch 450 loss: 0.6196017581224441
  batch 500 loss: 0.6176033741235734
  batch 550 loss: 0.6694114643335343
  batch 600 loss: 0.6655813449621201
  batch 650 loss: 0.6638893789052963
  batch 700 loss: 0.6728601312637329
  batch 750 loss: 0.694828884601593
  batch 800 loss: 0.6645698153972626
  batch 850 loss: 0.6411805212497711
  batch 900 loss: 0.6638534247875214
LOSS train 0.66385 valid 0.97244, valid PER 28.60%
EPOCH 16:
  batch 50 loss: 0.5984735727310181
  batch 100 loss: 0.5961525839567184
  batch 150 loss: 0.5943193185329437
  batch 200 loss: 0.5871743148565293
  batch 250 loss: 0.6029915988445282
  batch 300 loss: 0.60216237783432
  batch 350 loss: 0.6297346329689026
  batch 400 loss: 0.6414202904701233
  batch 450 loss: 0.6261327576637268
  batch 500 loss: 0.5911248338222503
  batch 550 loss: 0.617775496840477
  batch 600 loss: 0.6183072459697724
  batch 650 loss: 0.6230039936304093
  batch 700 loss: 0.5958206325769424
  batch 750 loss: 0.5975633752346039
  batch 800 loss: 0.6287581449747086
  batch 850 loss: 0.6040554946660995
  batch 900 loss: 0.6279664671421051
LOSS train 0.62797 valid 0.94177, valid PER 27.41%
EPOCH 17:
  batch 50 loss: 0.5542369133234024
  batch 100 loss: 0.5564213919639588
  batch 150 loss: 0.5487966167926789
  batch 200 loss: 0.5505294662714004
  batch 250 loss: 0.5757830947637558
  batch 300 loss: 0.5714638286828995
  batch 350 loss: 0.5513393604755401
  batch 400 loss: 0.5943267911672592
  batch 450 loss: 0.5941678416728974
  batch 500 loss: 0.5805537474155426
  batch 550 loss: 0.5852600753307342
  batch 600 loss: 0.6084630638360977
  batch 650 loss: 0.5739838445186615
  batch 700 loss: 0.5936727279424667
  batch 750 loss: 0.578174998164177
  batch 800 loss: 0.5770181727409363
  batch 850 loss: 0.6177616751194
  batch 900 loss: 0.585048748254776
LOSS train 0.58505 valid 0.95989, valid PER 27.16%
EPOCH 18:
  batch 50 loss: 0.5260335916280746
  batch 100 loss: 0.5301907497644425
  batch 150 loss: 0.56829820394516
  batch 200 loss: 0.5574953305721283
  batch 250 loss: 0.5502777338027954
  batch 300 loss: 0.5293016475439072
  batch 350 loss: 0.5627209335565567
  batch 400 loss: 0.548363978266716
  batch 450 loss: 0.5842426264286041
  batch 500 loss: 0.6049086475372314
  batch 550 loss: 0.5973767590522766
  batch 600 loss: 0.6040549367666245
  batch 650 loss: 0.5671222251653671
  batch 700 loss: 0.6099565672874451
  batch 750 loss: 0.5637770581245423
  batch 800 loss: 0.57721260368824
  batch 850 loss: 0.5829635685682297
  batch 900 loss: 0.57369753241539
LOSS train 0.57370 valid 0.96669, valid PER 27.70%
EPOCH 19:
  batch 50 loss: 0.5093004316091537
  batch 100 loss: 0.48124584019184113
  batch 150 loss: 0.4974686920642853
  batch 200 loss: 0.5021146726608277
  batch 250 loss: 0.5274831032752991
  batch 300 loss: 0.5243674337863922
  batch 350 loss: 0.49224297881126405
  batch 400 loss: 0.5167923659086228
  batch 450 loss: 0.5680827939510346
  batch 500 loss: 0.5700170493125916
  batch 550 loss: 0.5441620373725891
  batch 600 loss: 0.5347905749082565
  batch 650 loss: 0.5853552943468094
  batch 700 loss: 0.5412548965215683
  batch 750 loss: 0.529199612736702
  batch 800 loss: 0.5630378621816635
  batch 850 loss: 0.5530003196001053
  batch 900 loss: 0.5497371435165406
LOSS train 0.54974 valid 0.98204, valid PER 27.52%
EPOCH 20:
  batch 50 loss: 0.477203711271286
  batch 100 loss: 0.4916782492399216
  batch 150 loss: 0.45547845125198366
  batch 200 loss: 0.46328521430492403
  batch 250 loss: 0.4874617493152618
  batch 300 loss: 0.5191462540626526
  batch 350 loss: 0.5160284203290939
  batch 400 loss: 0.5276491832733154
  batch 450 loss: 0.5579626721143722
  batch 500 loss: 0.5169603371620178
  batch 550 loss: 0.5822452586889267
  batch 600 loss: 0.5281765806674957
  batch 650 loss: 0.5577362239360809
  batch 700 loss: 0.5705199790000915
  batch 750 loss: 0.5634171479940414
  batch 800 loss: 0.5790377104282379
  batch 850 loss: 0.5653689432144166
  batch 900 loss: 0.557930262684822
LOSS train 0.55793 valid 1.01087, valid PER 28.12%
train_loss
[1.7826184821128845, 1.3598978734016418, 1.147002899646759, 1.1250517547130585, 1.0246374011039734, 0.9609994840621948, 0.9478767156600952, 0.8677547150850295, 0.8162390124797821, 0.803644562959671, 0.7906376099586487, 0.753983405828476, 0.7059869927167892, 0.7200270414352417, 0.6638534247875214, 0.6279664671421051, 0.585048748254776, 0.57369753241539, 0.5497371435165406, 0.557930262684822]
valid_loss
[1.7477688789367676, 1.341208577156067, 1.2219066619873047, 1.1007951498031616, 1.0611826181411743, 1.04837167263031, 1.0099399089813232, 0.9661645889282227, 0.9659124612808228, 0.9608240127563477, 0.9536972641944885, 0.9224897027015686, 0.9400762915611267, 0.948830783367157, 0.9724448323249817, 0.9417709708213806, 0.9598869681358337, 0.9666932225227356, 0.9820386171340942, 1.0108698606491089]
valid_per
[66.66444474070124, 41.28782828956139, 37.568324223436875, 33.788828156245835, 33.36221837088388, 32.36901746433809, 31.469137448340224, 29.702706305825888, 29.509398746833753, 30.3492867617651, 28.79616051193174, 28.222903612851617, 27.81629116117851, 28.11625116651113, 28.602852952939607, 27.409678709505396, 27.156379149446742, 27.702972936941737, 27.522996933742167, 28.12291694440741]
Training finished in 6.0 minutes.
Model saved to checkpoints/20231208_091915/model_12
Loading model from checkpoints/20231208_091915/model_12
SUB: 15.89%, DEL: 11.61%, INS: 2.57%, COR: 72.50%, PER: 30.07%
