Namespace(seed=123, train_json='/rds/user/yo279/hpc-work/MLMI2/fbanks/train_fbank.json', val_json='/rds/user/yo279/hpc-work/MLMI2/fbanks/dev_fbank.json', test_json='/rds/user/yo279/hpc-work/MLMI2/fbanks/test_fbank.json', batch_size=4, num_layers=2, fbank_dims=23, model_dims=128, concat=1, lr=0.5, vocab='vocab_39.txt', report_interval=50, num_epochs=20, dropout_rate=0.5, clip_max_norm=1)
Total number of model parameters is 562216
EPOCH 1:
  batch 50 loss: 4.168295168876648
  batch 100 loss: 3.295912480354309
  batch 150 loss: 3.240770721435547
  batch 200 loss: 3.1339090347290037
  batch 250 loss: 2.977527370452881
  batch 300 loss: 2.7700962352752687
  batch 350 loss: 2.659389057159424
  batch 400 loss: 2.54543025970459
  batch 450 loss: 2.463611755371094
  batch 500 loss: 2.323884196281433
  batch 550 loss: 2.2641353964805604
  batch 600 loss: 2.193670370578766
  batch 650 loss: 2.1106464219093324
  batch 700 loss: 2.0852904319763184
  batch 750 loss: 1.9923379778862
  batch 800 loss: 1.9535919451713561
  batch 850 loss: 1.896722526550293
  batch 900 loss: 1.8641639828681946
LOSS train 1.86416 valid 1.73871, valid PER 66.68%
EPOCH 2:
  batch 50 loss: 1.781184093952179
  batch 100 loss: 1.7033306384086608
  batch 150 loss: 1.714101493358612
  batch 200 loss: 1.6833090496063232
  batch 250 loss: 1.6822596287727356
  batch 300 loss: 1.6189100790023803
  batch 350 loss: 1.5268135619163514
  batch 400 loss: 1.5361771011352539
  batch 450 loss: 1.4622540330886842
  batch 500 loss: 1.4927357339859009
  batch 550 loss: 1.5001991033554076
  batch 600 loss: 1.4271250939369202
  batch 650 loss: 1.4449748039245605
  batch 700 loss: 1.396756525039673
  batch 750 loss: 1.4052605986595155
  batch 800 loss: 1.3067351293563843
  batch 850 loss: 1.3134743690490722
  batch 900 loss: 1.324013979434967
LOSS train 1.32401 valid 1.25687, valid PER 39.86%
EPOCH 3:
  batch 50 loss: 1.2995889115333557
  batch 100 loss: 1.2661173677444457
  batch 150 loss: 1.2709366130828856
  batch 200 loss: 1.2409807968139648
  batch 250 loss: 1.2219788932800293
  batch 300 loss: 1.2237583744525908
  batch 350 loss: 1.2672051453590394
  batch 400 loss: 1.2375458657741547
  batch 450 loss: 1.2137458801269532
  batch 500 loss: 1.2038057446479797
  batch 550 loss: 1.216006281375885
  batch 600 loss: 1.1757161974906922
  batch 650 loss: 1.1491873133182526
  batch 700 loss: 1.1835068893432616
  batch 750 loss: 1.2257682907581329
  batch 800 loss: 1.1577745962142945
  batch 850 loss: 1.1957574343681336
  batch 900 loss: 1.1084712159633636
LOSS train 1.10847 valid 1.14805, valid PER 34.60%
EPOCH 4:
  batch 50 loss: 1.0993206369876862
  batch 100 loss: 1.136006042957306
  batch 150 loss: 1.0862543296813965
  batch 200 loss: 1.1078261458873748
  batch 250 loss: 1.1148930490016937
  batch 300 loss: 1.112097294330597
  batch 350 loss: 1.0460294365882874
  batch 400 loss: 1.104252245426178
  batch 450 loss: 1.0823579323291779
  batch 500 loss: 1.0623526751995087
  batch 550 loss: 1.0873148143291473
  batch 600 loss: 1.1055235326290132
  batch 650 loss: 1.088456301689148
  batch 700 loss: 1.0525184381008148
  batch 750 loss: 1.0478062105178834
  batch 800 loss: 1.0255522787570954
  batch 850 loss: 1.0341866946220397
  batch 900 loss: 1.083209285736084
LOSS train 1.08321 valid 0.99707, valid PER 31.06%
EPOCH 5:
  batch 50 loss: 1.0148033022880554
  batch 100 loss: 0.9875506448745728
  batch 150 loss: 1.047912437915802
  batch 200 loss: 0.9735179173946381
  batch 250 loss: 0.9816528069972992
  batch 300 loss: 1.014217803478241
  batch 350 loss: 1.020408465862274
  batch 400 loss: 1.023437181711197
  batch 450 loss: 1.0082347917556762
  batch 500 loss: 1.024761792421341
  batch 550 loss: 0.9550851857662201
  batch 600 loss: 1.0242445755004883
  batch 650 loss: 1.005838496685028
  batch 700 loss: 1.0371899008750916
  batch 750 loss: 0.9635370945930481
  batch 800 loss: 1.0001539182662964
  batch 850 loss: 1.0016961264610291
  batch 900 loss: 0.9936605095863342
LOSS train 0.99366 valid 0.95871, valid PER 29.73%
EPOCH 6:
  batch 50 loss: 0.984546365737915
  batch 100 loss: 0.9210375308990478
  batch 150 loss: 0.9291866624355316
  batch 200 loss: 0.9258417952060699
  batch 250 loss: 0.9947282183170318
  batch 300 loss: 0.9758000564575195
  batch 350 loss: 0.9433556926250458
  batch 400 loss: 0.9439373517036438
  batch 450 loss: 0.9697176396846772
  batch 500 loss: 0.955812439918518
  batch 550 loss: 0.9613558983802796
  batch 600 loss: 0.9328219282627106
  batch 650 loss: 0.9316979813575744
  batch 700 loss: 0.9504434394836426
  batch 750 loss: 0.9391255927085876
  batch 800 loss: 0.9166977477073669
  batch 850 loss: 0.9345468723773956
  batch 900 loss: 0.9351671326160431
LOSS train 0.93517 valid 0.96271, valid PER 30.00%
EPOCH 7:
  batch 50 loss: 0.9144116103649139
  batch 100 loss: 0.9414808785915375
  batch 150 loss: 0.8999798071384429
  batch 200 loss: 0.8951771503686905
  batch 250 loss: 0.885062325000763
  batch 300 loss: 0.8717558801174163
  batch 350 loss: 0.8928217089176178
  batch 400 loss: 0.9044209837913513
  batch 450 loss: 0.8971330541372299
  batch 500 loss: 0.9091432547569275
  batch 550 loss: 0.8844567680358887
  batch 600 loss: 0.9095353424549103
  batch 650 loss: 0.9041429805755615
  batch 700 loss: 0.9190718400478363
  batch 750 loss: 0.8833219051361084
  batch 800 loss: 0.884134657382965
  batch 850 loss: 0.9231984913349152
  batch 900 loss: 0.9479065907001495
LOSS train 0.94791 valid 0.91809, valid PER 28.88%
EPOCH 8:
  batch 50 loss: 0.8684105098247528
  batch 100 loss: 0.8540693044662475
  batch 150 loss: 0.8515059316158294
  batch 200 loss: 0.8656170642375947
  batch 250 loss: 0.9010975074768066
  batch 300 loss: 0.8162447130680084
  batch 350 loss: 0.8992128574848175
  batch 400 loss: 0.8413825964927674
  batch 450 loss: 0.8640625929832458
  batch 500 loss: 0.9013872802257538
  batch 550 loss: 0.8442705392837524
  batch 600 loss: 0.8867343211174011
  batch 650 loss: 0.8931477224826813
  batch 700 loss: 0.8510326337814331
  batch 750 loss: 0.8642685782909393
  batch 800 loss: 0.8737175714969635
  batch 850 loss: 0.8674327123165131
  batch 900 loss: 0.8794697272777557
LOSS train 0.87947 valid 0.89583, valid PER 27.76%
EPOCH 9:
  batch 50 loss: 0.8067402446269989
  batch 100 loss: 0.838675320148468
  batch 150 loss: 0.8235076403617859
  batch 200 loss: 0.8105120372772217
  batch 250 loss: 0.8184075140953064
  batch 300 loss: 0.8430183470249176
  batch 350 loss: 0.8459162175655365
  batch 400 loss: 0.826445288658142
  batch 450 loss: 0.8366628336906433
  batch 500 loss: 0.7969677066802978
  batch 550 loss: 0.8372240316867828
  batch 600 loss: 0.852509994506836
  batch 650 loss: 0.8210376000404358
  batch 700 loss: 0.8080264103412628
  batch 750 loss: 0.8189847362041474
  batch 800 loss: 0.8498603916168213
  batch 850 loss: 0.8575945484638214
  batch 900 loss: 0.7964328622817993
LOSS train 0.79643 valid 0.85484, valid PER 26.68%
EPOCH 10:
  batch 50 loss: 0.752600936293602
  batch 100 loss: 0.7862366718053818
  batch 150 loss: 0.7898702669143677
  batch 200 loss: 0.7884455782175064
  batch 250 loss: 0.8055232167243958
  batch 300 loss: 0.7625556588172913
  batch 350 loss: 0.8063905775547028
  batch 400 loss: 0.7604864907264709
  batch 450 loss: 0.7796697735786438
  batch 500 loss: 0.8310553097724914
  batch 550 loss: 0.8325014305114746
  batch 600 loss: 0.8103634119033813
  batch 650 loss: 0.7926882755756378
  batch 700 loss: 0.8260145008563995
  batch 750 loss: 0.8060000538825989
  batch 800 loss: 0.8192233872413636
  batch 850 loss: 0.8093988108634949
  batch 900 loss: 0.8254156887531281
LOSS train 0.82542 valid 0.86450, valid PER 27.11%
EPOCH 11:
  batch 50 loss: 0.757628892660141
  batch 100 loss: 0.7454562532901764
  batch 150 loss: 0.765534348487854
  batch 200 loss: 0.7922873830795288
  batch 250 loss: 0.7871323102712631
  batch 300 loss: 0.7491446268558503
  batch 350 loss: 0.7570680606365204
  batch 400 loss: 0.7751455998420715
  batch 450 loss: 0.7667728674411773
  batch 500 loss: 0.7575310504436493
  batch 550 loss: 0.7828213846683503
  batch 600 loss: 0.7429658967256546
  batch 650 loss: 0.8302024936676026
  batch 700 loss: 0.7636038172245025
  batch 750 loss: 0.7698453456163407
  batch 800 loss: 0.7880449169874191
  batch 850 loss: 0.7870476031303406
  batch 900 loss: 0.7965187156200408
LOSS train 0.79652 valid 0.82671, valid PER 25.82%
EPOCH 12:
  batch 50 loss: 0.7272329384088516
  batch 100 loss: 0.7305832183361054
  batch 150 loss: 0.6939478850364685
  batch 200 loss: 0.721819064617157
  batch 250 loss: 0.7537217819690705
  batch 300 loss: 0.7188059312105178
  batch 350 loss: 0.7475990426540374
  batch 400 loss: 0.7548309457302094
  batch 450 loss: 0.7690127658843994
  batch 500 loss: 0.7729972755908966
  batch 550 loss: 0.7109010696411133
  batch 600 loss: 0.7302395582199097
  batch 650 loss: 0.7649117302894592
  batch 700 loss: 0.7774339854717255
  batch 750 loss: 0.7336414575576782
  batch 800 loss: 0.7162625515460967
  batch 850 loss: 0.773459432721138
  batch 900 loss: 0.7539422088861465
LOSS train 0.75394 valid 0.81678, valid PER 25.59%
EPOCH 13:
  batch 50 loss: 0.7499403524398803
  batch 100 loss: 0.7378374963998795
  batch 150 loss: 0.7326482671499253
  batch 200 loss: 0.7662611842155457
  batch 250 loss: 0.74240083694458
  batch 300 loss: 0.7206437653303146
  batch 350 loss: 0.7169553291797638
  batch 400 loss: 0.7372567868232727
  batch 450 loss: 0.7495858049392701
  batch 500 loss: 0.7217084097862244
  batch 550 loss: 0.7828125810623169
  batch 600 loss: 0.7316065514087677
  batch 650 loss: 0.764538277387619
  batch 700 loss: 0.7506673210859298
  batch 750 loss: 0.7013555812835693
  batch 800 loss: 0.719971222281456
  batch 850 loss: 0.809780552983284
  batch 900 loss: 0.7707375717163086
LOSS train 0.77074 valid 0.81558, valid PER 25.56%
EPOCH 14:
  batch 50 loss: 0.6830785357952118
  batch 100 loss: 0.7022446471452714
  batch 150 loss: 0.6994720804691315
  batch 200 loss: 0.7042797863483429
  batch 250 loss: 0.7067626881599426
  batch 300 loss: 0.7225653028488159
  batch 350 loss: 0.6822849613428116
  batch 400 loss: 0.7303366303443909
  batch 450 loss: 0.7204211938381195
  batch 500 loss: 0.713892958164215
  batch 550 loss: 0.7296798396110534
  batch 600 loss: 0.6956197386980056
  batch 650 loss: 0.7318408739566803
  batch 700 loss: 0.7592095303535461
  batch 750 loss: 0.7235313642024994
  batch 800 loss: 0.6893781805038453
  batch 850 loss: 0.7414483404159546
  batch 900 loss: 0.7377217793464661
LOSS train 0.73772 valid 0.80697, valid PER 25.26%
EPOCH 15:
  batch 50 loss: 0.6796084672212601
  batch 100 loss: 0.6914121174812317
  batch 150 loss: 0.670103530883789
  batch 200 loss: 0.702700886130333
  batch 250 loss: 0.694458327293396
  batch 300 loss: 0.6804615777730941
  batch 350 loss: 0.6854562580585479
  batch 400 loss: 0.6602027505636215
  batch 450 loss: 0.6994558823108673
  batch 500 loss: 0.6631480783224106
  batch 550 loss: 0.7089276129007339
  batch 600 loss: 0.7262545883655548
  batch 650 loss: 0.7396083426475525
  batch 700 loss: 0.7501759713888169
  batch 750 loss: 0.7378262782096863
  batch 800 loss: 0.7112532097101212
  batch 850 loss: 0.6961619567871093
  batch 900 loss: 0.7023461627960205
LOSS train 0.70235 valid 0.81760, valid PER 25.04%
EPOCH 16:
  batch 50 loss: 0.7481391775608063
  batch 100 loss: 0.6728461933135986
  batch 150 loss: 0.6793963605165482
  batch 200 loss: 0.6877941405773162
  batch 250 loss: 0.6880861151218415
  batch 300 loss: 0.6767486292123794
  batch 350 loss: 0.6759612888097764
  batch 400 loss: 0.6926491302251816
  batch 450 loss: 0.6913115519285202
  batch 500 loss: 0.6547058379650116
  batch 550 loss: 0.6732703876495362
  batch 600 loss: 0.666948630809784
  batch 650 loss: 0.6906513112783432
  batch 700 loss: 0.6465996301174164
  batch 750 loss: 0.6768044877052307
  batch 800 loss: 0.6900657558441162
  batch 850 loss: 0.6894533222913742
  batch 900 loss: 0.7087743562459946
LOSS train 0.70877 valid 0.80827, valid PER 24.66%
EPOCH 17:
  batch 50 loss: 0.6559713172912598
  batch 100 loss: 0.6495674389600754
  batch 150 loss: 0.6375928068161011
  batch 200 loss: 0.6491972017288208
  batch 250 loss: 0.6623326659202575
  batch 300 loss: 0.7000968343019486
  batch 350 loss: 0.6704228758811951
  batch 400 loss: 0.6951662576198578
  batch 450 loss: 0.6794738870859146
  batch 500 loss: 0.6449730396270752
  batch 550 loss: 0.6650334125757218
  batch 600 loss: 0.6889118713140487
  batch 650 loss: 0.6594045889377594
  batch 700 loss: 0.6651625233888626
  batch 750 loss: 0.6817719101905823
  batch 800 loss: 0.6926087868213654
  batch 850 loss: 0.6908510440587997
  batch 900 loss: 0.6828054612874985
LOSS train 0.68281 valid 0.78857, valid PER 23.75%
EPOCH 18:
  batch 50 loss: 0.6524162799119949
  batch 100 loss: 0.6463426822423934
  batch 150 loss: 0.6611925739049912
  batch 200 loss: 0.654277338385582
  batch 250 loss: 0.6792126107215881
  batch 300 loss: 0.6715291601419449
  batch 350 loss: 0.6951906687021255
  batch 400 loss: 0.6479744136333465
  batch 450 loss: 0.6770374464988709
  batch 500 loss: 0.6538572716712951
  batch 550 loss: 0.6778765553236008
  batch 600 loss: 0.6271645194292068
  batch 650 loss: 0.6421845561265945
  batch 700 loss: 0.6740168440341949
  batch 750 loss: 0.6340124887228012
  batch 800 loss: 0.6202957230806351
  batch 850 loss: 0.6342163753509521
  batch 900 loss: 0.6656782752275467
LOSS train 0.66568 valid 0.78879, valid PER 24.49%
EPOCH 19:
  batch 50 loss: 0.6206041049957275
  batch 100 loss: 0.6192588210105896
  batch 150 loss: 0.6146551686525344
  batch 200 loss: 0.6241788679361343
  batch 250 loss: 0.6316704344749451
  batch 300 loss: 0.6233087652921676
  batch 350 loss: 0.6177410155534744
  batch 400 loss: 0.6238579314947128
  batch 450 loss: 0.6435081458091736
  batch 500 loss: 0.6635526651144028
  batch 550 loss: 0.6186619824171067
  batch 600 loss: 0.6284912997484207
  batch 650 loss: 0.6804656767845154
  batch 700 loss: 0.6169990158081055
  batch 750 loss: 0.6180856430530548
  batch 800 loss: 0.6473345828056335
  batch 850 loss: 0.6428657382726669
  batch 900 loss: 0.6417976647615433
LOSS train 0.64180 valid 0.77403, valid PER 23.77%
EPOCH 20:
  batch 50 loss: 0.5928912389278412
  batch 100 loss: 0.5883210402727127
  batch 150 loss: 0.5934149050712585
  batch 200 loss: 0.5895895045995713
  batch 250 loss: 0.5802442288398743
  batch 300 loss: 0.6111855739355088
  batch 350 loss: 0.5968943172693253
  batch 400 loss: 0.608622795343399
  batch 450 loss: 0.609895356297493
  batch 500 loss: 0.596720729470253
  batch 550 loss: 0.6559389752149581
  batch 600 loss: 0.595307914018631
  batch 650 loss: 0.611846918463707
  batch 700 loss: 0.6026368051767349
  batch 750 loss: 0.5908037054538727
  batch 800 loss: 0.647914519906044
  batch 850 loss: 0.6296678531169891
  batch 900 loss: 0.7341880267858505
LOSS train 0.73419 valid 0.85062, valid PER 25.37%
train_loss
[1.8641639828681946, 1.324013979434967, 1.1084712159633636, 1.083209285736084, 0.9936605095863342, 0.9351671326160431, 0.9479065907001495, 0.8794697272777557, 0.7964328622817993, 0.8254156887531281, 0.7965187156200408, 0.7539422088861465, 0.7707375717163086, 0.7377217793464661, 0.7023461627960205, 0.7087743562459946, 0.6828054612874985, 0.6656782752275467, 0.6417976647615433, 0.7341880267858505]
valid_loss
[1.7387120723724365, 1.2568727731704712, 1.1480464935302734, 0.9970709681510925, 0.9587149024009705, 0.9627060294151306, 0.9180901646614075, 0.8958306908607483, 0.8548414707183838, 0.8645047545433044, 0.8267065286636353, 0.8167815804481506, 0.8155842423439026, 0.8069695234298706, 0.8176029920578003, 0.8082689642906189, 0.7885708212852478, 0.788788378238678, 0.7740304470062256, 0.8506177067756653]
valid_per
[66.67777629649379, 39.86135181975737, 34.59538728169577, 31.06252499666711, 29.72936941741101, 30.00266631115851, 28.87614984668711, 27.762964938008267, 26.683108918810827, 27.109718704172774, 25.82322357019064, 25.589921343820826, 25.563258232235704, 25.2566324490068, 25.036661778429544, 24.656712438341554, 23.750166644447408, 24.49006799093454, 23.77016397813625, 25.36995067324357]
Training finished in 4.0 minutes.
Model saved to checkpoints/20231208_000043/model_19
Loading model from checkpoints/20231208_000043/model_19
SUB: 15.29%, DEL: 7.80%, INS: 2.11%, COR: 76.91%, PER: 25.21%
