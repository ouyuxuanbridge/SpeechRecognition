Namespace(seed=123, train_json='/rds/user/yo279/hpc-work/MLMI2/fbanks/train_fbank.json', val_json='/rds/user/yo279/hpc-work/MLMI2/fbanks/dev_fbank.json', test_json='/rds/user/yo279/hpc-work/MLMI2/fbanks/test_fbank.json', batch_size=4, num_layers=2, fbank_dims=23, model_dims=128, concat=1, lr=0.5, vocab='vocab_39.txt', report_interval=50, num_epochs=20, dropout_rate=0.3, clip_max_norm=1)
Total number of model parameters is 562216
EPOCH 1:
  batch 50 loss: 4.171486134529114
  batch 100 loss: 3.2947706317901613
  batch 150 loss: 3.2378955411911012
  batch 200 loss: 3.1278454732894896
  batch 250 loss: 2.962936940193176
  batch 300 loss: 2.7570849800109865
  batch 350 loss: 2.645959315299988
  batch 400 loss: 2.5312407970428468
  batch 450 loss: 2.447659816741943
  batch 500 loss: 2.3129127407073975
  batch 550 loss: 2.2510057926177978
  batch 600 loss: 2.174941825866699
  batch 650 loss: 2.0883435559272767
  batch 700 loss: 2.068834319114685
  batch 750 loss: 1.9675287699699402
  batch 800 loss: 1.9352030682563781
  batch 850 loss: 1.8703683733940124
  batch 900 loss: 1.840601885318756
LOSS train 1.84060 valid 1.72966, valid PER 66.48%
EPOCH 2:
  batch 50 loss: 1.7593357920646668
  batch 100 loss: 1.6726851415634156
  batch 150 loss: 1.6892377710342408
  batch 200 loss: 1.6540823554992676
  batch 250 loss: 1.650479793548584
  batch 300 loss: 1.5758650517463684
  batch 350 loss: 1.5018509387969972
  batch 400 loss: 1.5003674626350403
  batch 450 loss: 1.4273594164848327
  batch 500 loss: 1.4587958192825317
  batch 550 loss: 1.4584349751472474
  batch 600 loss: 1.385121591091156
  batch 650 loss: 1.4031685566902161
  batch 700 loss: 1.3458411121368408
  batch 750 loss: 1.3395431232452393
  batch 800 loss: 1.2698527896404266
  batch 850 loss: 1.289832079410553
  batch 900 loss: 1.3069496130943299
LOSS train 1.30695 valid 1.24626, valid PER 39.38%
EPOCH 3:
  batch 50 loss: 1.2544296789169311
  batch 100 loss: 1.2417474853992463
  batch 150 loss: 1.2196091270446778
  batch 200 loss: 1.209546275138855
  batch 250 loss: 1.1858965134620667
  batch 300 loss: 1.1904060971736907
  batch 350 loss: 1.2218977653980254
  batch 400 loss: 1.197262727022171
  batch 450 loss: 1.1773862373828887
  batch 500 loss: 1.155409996509552
  batch 550 loss: 1.1611617875099183
  batch 600 loss: 1.1363581597805024
  batch 650 loss: 1.0990302157402039
  batch 700 loss: 1.1466238594055176
  batch 750 loss: 1.171130188703537
  batch 800 loss: 1.1085411465168
  batch 850 loss: 1.1399980318546294
  batch 900 loss: 1.0786587238311767
LOSS train 1.07866 valid 1.11882, valid PER 34.10%
EPOCH 4:
  batch 50 loss: 1.0531628024578095
  batch 100 loss: 1.1000204861164093
  batch 150 loss: 1.0306695294380188
  batch 200 loss: 1.0824180030822754
  batch 250 loss: 1.0749840700626374
  batch 300 loss: 1.0786450350284575
  batch 350 loss: 1.0023207890987396
  batch 400 loss: 1.0632706832885743
  batch 450 loss: 1.0490945267677307
  batch 500 loss: 1.023919813632965
  batch 550 loss: 1.0591967451572417
  batch 600 loss: 1.0687575626373291
  batch 650 loss: 1.0367759680747985
  batch 700 loss: 1.0121777045726776
  batch 750 loss: 1.0193054378032684
  batch 800 loss: 0.9735952877998352
  batch 850 loss: 1.001721887588501
  batch 900 loss: 1.0456358313560485
LOSS train 1.04564 valid 1.00717, valid PER 30.98%
EPOCH 5:
  batch 50 loss: 0.96762526512146
  batch 100 loss: 0.9441705191135407
  batch 150 loss: 0.9890643215179443
  batch 200 loss: 0.9400230622291565
  batch 250 loss: 0.9521589243412018
  batch 300 loss: 0.9642180442810059
  batch 350 loss: 0.9551992726325989
  batch 400 loss: 0.9714641761779785
  batch 450 loss: 0.9558017897605896
  batch 500 loss: 0.9838200402259827
  batch 550 loss: 0.9286725664138794
  batch 600 loss: 0.9901297438144684
  batch 650 loss: 0.9639613091945648
  batch 700 loss: 0.9963609993457794
  batch 750 loss: 0.9110149383544922
  batch 800 loss: 0.9676873981952667
  batch 850 loss: 0.9550834834575653
  batch 900 loss: 0.9552578628063202
LOSS train 0.95526 valid 0.95043, valid PER 29.54%
EPOCH 6:
  batch 50 loss: 0.9614307034015656
  batch 100 loss: 0.885733219385147
  batch 150 loss: 0.8859422063827515
  batch 200 loss: 0.9128742754459381
  batch 250 loss: 0.9680993556976318
  batch 300 loss: 0.9174140119552612
  batch 350 loss: 0.914091876745224
  batch 400 loss: 0.8697533822059631
  batch 450 loss: 0.9274891102313996
  batch 500 loss: 0.9090534353256225
  batch 550 loss: 0.9229776179790496
  batch 600 loss: 0.8880458831787109
  batch 650 loss: 0.8956358766555786
  batch 700 loss: 0.9171088397502899
  batch 750 loss: 0.8766216862201691
  batch 800 loss: 0.8801053702831269
  batch 850 loss: 0.882906185388565
  batch 900 loss: 0.8896371388435363
LOSS train 0.88964 valid 0.92264, valid PER 28.14%
EPOCH 7:
  batch 50 loss: 0.8600700736045838
  batch 100 loss: 0.8837976706027985
  batch 150 loss: 0.8360144507884979
  batch 200 loss: 0.8828978705406189
  batch 250 loss: 0.8624284875392914
  batch 300 loss: 0.8132476413249969
  batch 350 loss: 0.8537912714481354
  batch 400 loss: 0.8397294294834137
  batch 450 loss: 0.8398654198646546
  batch 500 loss: 0.8407437598705292
  batch 550 loss: 0.8311711525917054
  batch 600 loss: 0.8625908374786377
  batch 650 loss: 0.8559069085121155
  batch 700 loss: 0.8609505295753479
  batch 750 loss: 0.8238542127609253
  batch 800 loss: 0.8340599560737609
  batch 850 loss: 0.8561483657360077
  batch 900 loss: 0.9008692812919616
LOSS train 0.90087 valid 0.89203, valid PER 28.38%
EPOCH 8:
  batch 50 loss: 0.8013640880584717
  batch 100 loss: 0.7925456273555755
  batch 150 loss: 0.7808763754367828
  batch 200 loss: 0.7921715676784515
  batch 250 loss: 0.8052169144153595
  batch 300 loss: 0.7667490136623383
  batch 350 loss: 0.8318879461288452
  batch 400 loss: 0.7838658881187439
  batch 450 loss: 0.7944311630725861
  batch 500 loss: 0.8213245677947998
  batch 550 loss: 0.7765132343769073
  batch 600 loss: 0.8114202845096589
  batch 650 loss: 0.8380181562900543
  batch 700 loss: 0.7986058604717254
  batch 750 loss: 0.7961426222324371
  batch 800 loss: 0.7978979420661926
  batch 850 loss: 0.7870236146450043
  batch 900 loss: 0.8248420923948288
LOSS train 0.82484 valid 0.86979, valid PER 27.47%
EPOCH 9:
  batch 50 loss: 0.7255507016181946
  batch 100 loss: 0.7628927844762802
  batch 150 loss: 0.7754316818714142
  batch 200 loss: 0.7346573114395142
  batch 250 loss: 0.7877544152736664
  batch 300 loss: 0.7747882103919983
  batch 350 loss: 0.8063038861751557
  batch 400 loss: 0.7639166808128357
  batch 450 loss: 0.7677950739860535
  batch 500 loss: 0.743822471499443
  batch 550 loss: 0.8342427480220794
  batch 600 loss: 0.8246851480007171
  batch 650 loss: 0.7852682662010193
  batch 700 loss: 0.7731944876909256
  batch 750 loss: 0.7728749585151672
  batch 800 loss: 0.798069566488266
  batch 850 loss: 0.8172230529785156
  batch 900 loss: 0.7613968551158905
LOSS train 0.76140 valid 0.86635, valid PER 26.88%
EPOCH 10:
  batch 50 loss: 0.717525160908699
  batch 100 loss: 0.7172443294525146
  batch 150 loss: 0.7407618260383606
  batch 200 loss: 0.7699016797542572
  batch 250 loss: 0.749494731426239
  batch 300 loss: 0.7022755056619644
  batch 350 loss: 0.762941621541977
  batch 400 loss: 0.704898898601532
  batch 450 loss: 0.7221910399198532
  batch 500 loss: 0.7505175673961639
  batch 550 loss: 0.7500537598133087
  batch 600 loss: 0.763319514989853
  batch 650 loss: 0.7507654702663422
  batch 700 loss: 0.7525106149911881
  batch 750 loss: 0.7465323948860169
  batch 800 loss: 0.7558390325307847
  batch 850 loss: 0.7552426433563233
  batch 900 loss: 0.7574777817726135
LOSS train 0.75748 valid 0.83666, valid PER 26.80%
EPOCH 11:
  batch 50 loss: 0.6915810328722
  batch 100 loss: 0.6724288427829742
  batch 150 loss: 0.6880117160081863
  batch 200 loss: 0.7851441895961762
  batch 250 loss: 0.7770841747522355
  batch 300 loss: 0.7165688133239746
  batch 350 loss: 0.7249179983139038
  batch 400 loss: 0.7561913669109345
  batch 450 loss: 0.7664528560638427
  batch 500 loss: 0.7090959966182708
  batch 550 loss: 0.7332903057336807
  batch 600 loss: 0.7128408092260361
  batch 650 loss: 0.7733752471208573
  batch 700 loss: 0.7072085952758789
  batch 750 loss: 0.7309883916378022
  batch 800 loss: 0.7354999768733979
  batch 850 loss: 0.7878186285495759
  batch 900 loss: 0.784664763212204
LOSS train 0.78466 valid 0.83159, valid PER 25.96%
EPOCH 12:
  batch 50 loss: 0.6927354460954667
  batch 100 loss: 0.6773574525117874
  batch 150 loss: 0.6433097970485687
  batch 200 loss: 0.6768364405632019
  batch 250 loss: 0.6971814727783203
  batch 300 loss: 0.667007520198822
  batch 350 loss: 0.6772553616762161
  batch 400 loss: 0.6964217054843903
  batch 450 loss: 0.7032864606380462
  batch 500 loss: 0.7170395213365555
  batch 550 loss: 0.6723098289966584
  batch 600 loss: 0.6889766657352447
  batch 650 loss: 0.7248443895578385
  batch 700 loss: 0.7368530917167664
  batch 750 loss: 0.7077439767122269
  batch 800 loss: 0.6944080483913422
  batch 850 loss: 0.7341956555843353
  batch 900 loss: 0.7243593776226044
LOSS train 0.72436 valid 0.83968, valid PER 26.22%
EPOCH 13:
  batch 50 loss: 0.6718710231781005
  batch 100 loss: 0.6767370486259461
  batch 150 loss: 0.659377436041832
  batch 200 loss: 0.6592168593406678
  batch 250 loss: 0.6570158195495606
  batch 300 loss: 0.6716263651847839
  batch 350 loss: 0.6686953073740005
  batch 400 loss: 0.6632990795373916
  batch 450 loss: 0.6823847544193268
  batch 500 loss: 0.6376715683937073
  batch 550 loss: 0.6941730439662933
  batch 600 loss: 0.6816334933042526
  batch 650 loss: 0.7102142149209976
  batch 700 loss: 0.679064136147499
  batch 750 loss: 0.6407574874162674
  batch 800 loss: 0.7001955336332322
  batch 850 loss: 0.7185588836669922
  batch 900 loss: 0.697600184082985
LOSS train 0.69760 valid 0.82018, valid PER 25.66%
EPOCH 14:
  batch 50 loss: 0.6299404436349869
  batch 100 loss: 0.654644227027893
  batch 150 loss: 0.6325142109394073
  batch 200 loss: 0.6436490178108215
  batch 250 loss: 0.6341166335344315
  batch 300 loss: 0.67762850522995
  batch 350 loss: 0.6330278974771499
  batch 400 loss: 0.6435475444793701
  batch 450 loss: 0.6660610365867615
  batch 500 loss: 0.6923267406225204
  batch 550 loss: 0.6920865952968598
  batch 600 loss: 0.6484524536132813
  batch 650 loss: 0.676850026845932
  batch 700 loss: 0.6990481531620025
  batch 750 loss: 0.6809733158349991
  batch 800 loss: 0.6195218187570571
  batch 850 loss: 0.6585727214813233
  batch 900 loss: 0.679086468219757
LOSS train 0.67909 valid 0.80459, valid PER 25.00%
EPOCH 15:
  batch 50 loss: 0.6204420953989029
  batch 100 loss: 0.6057250100374222
  batch 150 loss: 0.6216094624996186
  batch 200 loss: 0.6440537828207016
  batch 250 loss: 0.6504098874330521
  batch 300 loss: 0.6191217964887619
  batch 350 loss: 0.6072131913900375
  batch 400 loss: 0.6222175896167755
  batch 450 loss: 0.6307504343986511
  batch 500 loss: 0.6240385437011718
  batch 550 loss: 0.6659928870201111
  batch 600 loss: 0.6473792231082917
  batch 650 loss: 0.6663229668140411
  batch 700 loss: 0.6700630724430084
  batch 750 loss: 0.650387813448906
  batch 800 loss: 0.6330889040231704
  batch 850 loss: 0.6471319705247879
  batch 900 loss: 0.6830241030454636
LOSS train 0.68302 valid 0.82972, valid PER 25.52%
EPOCH 16:
  batch 50 loss: 0.6210289043188095
  batch 100 loss: 0.5924010181427002
  batch 150 loss: 0.6005676299333572
  batch 200 loss: 0.610638969540596
  batch 250 loss: 0.6335449796915055
  batch 300 loss: 0.6155732184648514
  batch 350 loss: 0.6232491648197174
  batch 400 loss: 0.6135992962121963
  batch 450 loss: 0.6266718727350234
  batch 500 loss: 0.617568451166153
  batch 550 loss: 0.5978819805383683
  batch 600 loss: 0.6014057832956314
  batch 650 loss: 0.6930179589986801
  batch 700 loss: 0.6315629971027374
  batch 750 loss: 0.6455665266513825
  batch 800 loss: 0.6549940836429596
  batch 850 loss: 0.639269413948059
  batch 900 loss: 0.6410273170471191
LOSS train 0.64103 valid 0.79493, valid PER 23.96%
EPOCH 17:
  batch 50 loss: 0.5916924840211868
  batch 100 loss: 0.6274602514505386
  batch 150 loss: 0.5901132887601852
  batch 200 loss: 0.6085662120580673
  batch 250 loss: 0.6262728643417358
  batch 300 loss: 0.6225508832931519
  batch 350 loss: 0.6067255109548568
  batch 400 loss: 0.6634645140171052
  batch 450 loss: 0.6253123563528061
  batch 500 loss: 0.6057421606779099
  batch 550 loss: 0.6220656430721283
  batch 600 loss: 0.6267283642292023
  batch 650 loss: 0.6039395141601562
  batch 700 loss: 0.5856452465057373
  batch 750 loss: 0.6018346327543259
  batch 800 loss: 0.6294285678863525
  batch 850 loss: 0.6420479023456573
  batch 900 loss: 0.6300061219930648
LOSS train 0.63001 valid 0.81460, valid PER 23.91%
EPOCH 18:
  batch 50 loss: 0.6188235402107238
  batch 100 loss: 0.6415171593427658
  batch 150 loss: 0.6314952576160431
  batch 200 loss: 0.6139494025707245
  batch 250 loss: 0.6053282648324967
  batch 300 loss: 0.5866227322816848
  batch 350 loss: 0.6192193764448166
  batch 400 loss: 0.5907108449935913
  batch 450 loss: 0.6329323762655258
  batch 500 loss: 0.6505494797229767
  batch 550 loss: 0.6814546906948089
  batch 600 loss: 0.6222184693813324
  batch 650 loss: 0.6136981463432312
  batch 700 loss: 0.6474404543638229
  batch 750 loss: 0.5909542721509934
  batch 800 loss: 0.595984628200531
  batch 850 loss: 0.5962674850225449
  batch 900 loss: 0.639590368270874
LOSS train 0.63959 valid 0.80997, valid PER 24.78%
EPOCH 19:
  batch 50 loss: 0.560027865767479
  batch 100 loss: 0.6164397418498992
  batch 150 loss: 0.5869410389661789
  batch 200 loss: 0.5916576725244522
  batch 250 loss: 0.6054020947217942
  batch 300 loss: 0.5963448822498322
  batch 350 loss: 0.5980848067998886
  batch 400 loss: 0.6155661779642105
  batch 450 loss: 0.6309965425729751
  batch 500 loss: 0.631427321434021
  batch 550 loss: 0.6001717561483383
  batch 600 loss: 0.593574909567833
  batch 650 loss: 0.6465548574924469
  batch 700 loss: 0.5801656991243362
  batch 750 loss: 0.5775972646474838
  batch 800 loss: 0.6213791745901108
  batch 850 loss: 0.6528561645746231
  batch 900 loss: 0.6274358654022216
LOSS train 0.62744 valid 0.78594, valid PER 23.89%
EPOCH 20:
  batch 50 loss: 0.5603898984193801
  batch 100 loss: 0.5980978465080261
  batch 150 loss: 0.6244216376543045
  batch 200 loss: 0.6355445516109467
  batch 250 loss: 0.5957556492090226
  batch 300 loss: 0.6200619184970856
  batch 350 loss: 0.6035942822694779
  batch 400 loss: 0.6450025671720505
  batch 450 loss: 0.6053307557106018
  batch 500 loss: 0.578775960803032
  batch 550 loss: 0.619328818321228
  batch 600 loss: 0.5685874176025391
  batch 650 loss: 0.6392110168933869
  batch 700 loss: 0.6471350926160813
  batch 750 loss: 0.587397797703743
  batch 800 loss: 0.6183497714996338
  batch 850 loss: 0.6309795266389847
  batch 900 loss: 0.6179967564344406
LOSS train 0.61800 valid 0.85130, valid PER 25.39%
train_loss
[1.840601885318756, 1.3069496130943299, 1.0786587238311767, 1.0456358313560485, 0.9552578628063202, 0.8896371388435363, 0.9008692812919616, 0.8248420923948288, 0.7613968551158905, 0.7574777817726135, 0.784664763212204, 0.7243593776226044, 0.697600184082985, 0.679086468219757, 0.6830241030454636, 0.6410273170471191, 0.6300061219930648, 0.639590368270874, 0.6274358654022216, 0.6179967564344406]
valid_loss
[1.7296643257141113, 1.246255874633789, 1.1188181638717651, 1.0071674585342407, 0.9504296183586121, 0.9226440191268921, 0.892033040523529, 0.8697905540466309, 0.8663508892059326, 0.8366644382476807, 0.8315871357917786, 0.839677631855011, 0.8201799392700195, 0.8045909404754639, 0.8297163844108582, 0.7949262261390686, 0.8145952224731445, 0.8099728226661682, 0.7859393954277039, 0.8513041734695435]
valid_per
[66.47780295960538, 39.38141581122517, 34.09545393947474, 30.982535661911747, 29.536061858418876, 28.136248500199972, 28.382882282362353, 27.469670710571926, 26.876416477802962, 26.80309292094387, 25.95653912811625, 26.22317024396747, 25.66324490067991, 25.003332888948137, 25.523263564858016, 23.963471537128385, 23.91014531395814, 24.776696440474602, 23.890147980269298, 25.38994800693241]
Training finished in 4.0 minutes.
Model saved to checkpoints/20231207_235547/model_19
Loading model from checkpoints/20231207_235547/model_19
SUB: 15.52%, DEL: 8.30%, INS: 2.35%, COR: 76.19%, PER: 26.16%
