Namespace(seed=123, train_json='/rds/user/yo279/hpc-work/MLMI2/fbanks/train_fbank.json', val_json='/rds/user/yo279/hpc-work/MLMI2/fbanks/dev_fbank.json', test_json='/rds/user/yo279/hpc-work/MLMI2/fbanks/test_fbank.json', batch_size=4, num_layers=1, fbank_dims=23, model_dims=128, concat=1, lr=0.005, vocab='vocab_39.txt', report_interval=50, num_epochs=20, dropout_rate=0.5, clip_max_norm=1)
Total number of model parameters is 166952
EPOCH 1:
  batch 50 loss: 18.806246166229247
  batch 100 loss: 4.563045148849487
  batch 150 loss: 3.6673193693161013
  batch 200 loss: 3.6059737634658813
  batch 250 loss: 3.560184507369995
  batch 300 loss: 3.513691816329956
  batch 350 loss: 3.474863348007202
  batch 400 loss: 3.4488046073913576
  batch 450 loss: 3.418705530166626
  batch 500 loss: 3.394729881286621
  batch 550 loss: 3.3713285541534423
  batch 600 loss: 3.3518118143081663
  batch 650 loss: 3.3332839250564574
  batch 700 loss: 3.340035767555237
  batch 750 loss: 3.3187516164779662
  batch 800 loss: 3.323306794166565
  batch 850 loss: 3.315624237060547
  batch 900 loss: 3.298761258125305
LOSS train 3.29876 valid 3.31925, valid PER 100.00%
EPOCH 2:
  batch 50 loss: 3.292419695854187
  batch 100 loss: 3.2899812412261964
  batch 150 loss: 3.286065411567688
  batch 200 loss: 3.2821756315231325
  batch 250 loss: 3.2779809284210204
  batch 300 loss: 3.277633171081543
  batch 350 loss: 3.2669779109954833
  batch 400 loss: 3.263196530342102
  batch 450 loss: 3.2704758930206297
  batch 500 loss: 3.2585530138015746
  batch 550 loss: 3.2688426780700683
  batch 600 loss: 3.260690202713013
  batch 650 loss: 3.2611795902252196
  batch 700 loss: 3.2500977516174316
  batch 750 loss: 3.2620617866516115
  batch 800 loss: 3.2493229341506957
  batch 850 loss: 3.2528367853164672
  batch 900 loss: 3.253239951133728
LOSS train 3.25324 valid 3.26289, valid PER 100.00%
EPOCH 3:
  batch 50 loss: 3.247810378074646
  batch 100 loss: 3.2460186767578123
  batch 150 loss: 3.2466767263412475
  batch 200 loss: 3.2447256517410277
  batch 250 loss: 3.2329963397979737
  batch 300 loss: 3.247258381843567
  batch 350 loss: 3.2469670295715334
  batch 400 loss: 3.2402537870407104
  batch 450 loss: 3.243160548210144
  batch 500 loss: 3.2340132808685302
  batch 550 loss: 3.2403956127166746
  batch 600 loss: 3.223814330101013
  batch 650 loss: 3.2153233242034913
  batch 700 loss: 3.212268133163452
  batch 750 loss: 3.242411198616028
  batch 800 loss: 3.222730555534363
  batch 850 loss: 3.2237088298797607
  batch 900 loss: 3.2202441024780275
LOSS train 3.22024 valid 3.23524, valid PER 100.00%
EPOCH 4:
  batch 50 loss: 3.222602787017822
  batch 100 loss: 3.2172106647491456
  batch 150 loss: 3.2081570386886598
  batch 200 loss: 3.2281932163238527
  batch 250 loss: 3.226380958557129
  batch 300 loss: 3.221276044845581
  batch 350 loss: 3.2080940628051757
  batch 400 loss: 3.215759768486023
  batch 450 loss: 3.2251841259002685
  batch 500 loss: 3.191193380355835
  batch 550 loss: 3.220063486099243
  batch 600 loss: 3.216655797958374
  batch 650 loss: 3.2143118381500244
  batch 700 loss: 3.2076342821121218
  batch 750 loss: 3.195440354347229
  batch 800 loss: 3.193562560081482
  batch 850 loss: 3.1914090013504026
  batch 900 loss: 3.2059165811538697
LOSS train 3.20592 valid 3.20869, valid PER 100.00%
EPOCH 5:
  batch 50 loss: 3.186790957450867
  batch 100 loss: 3.1837183332443235
  batch 150 loss: 3.183487334251404
  batch 200 loss: 3.186205520629883
  batch 250 loss: 3.1924363422393798
  batch 300 loss: 3.1868505573272703
  batch 350 loss: 3.1981366586685183
  batch 400 loss: 3.1898094987869263
  batch 450 loss: 3.1861369895935057
  batch 500 loss: 3.1976999759674074
  batch 550 loss: 3.178372383117676
  batch 600 loss: 3.1743000411987303
  batch 650 loss: 3.1737916707992553
  batch 700 loss: 3.1803582811355593
  batch 750 loss: 3.1744336318969726
  batch 800 loss: 3.1903458881378173
  batch 850 loss: 3.190676698684692
  batch 900 loss: 3.183266077041626
LOSS train 3.18327 valid 3.17903, valid PER 100.00%
EPOCH 6:
  batch 50 loss: 3.160825462341309
  batch 100 loss: 3.185689716339111
  batch 150 loss: 3.1580950355529787
  batch 200 loss: 3.1681650400161745
  batch 250 loss: 3.1696082735061646
  batch 300 loss: 3.140989022254944
  batch 350 loss: 3.1625948190689086
  batch 400 loss: 3.1509921455383303
  batch 450 loss: 3.1571305322647096
  batch 500 loss: 3.145051064491272
  batch 550 loss: 3.1568677186965943
  batch 600 loss: 3.1461611700057985
  batch 650 loss: 3.1489227390289307
  batch 700 loss: 3.1558080101013184
  batch 750 loss: 3.149816870689392
  batch 800 loss: 3.1348564529418947
  batch 850 loss: 3.143821368217468
  batch 900 loss: 3.145541481971741
LOSS train 3.14554 valid 3.14302, valid PER 100.00%
EPOCH 7:
  batch 50 loss: 3.129561333656311
  batch 100 loss: 3.1400648736953736
  batch 150 loss: 3.1314728021621705
  batch 200 loss: 3.135131015777588
  batch 250 loss: 3.1263435745239256
  batch 300 loss: 3.1143232870101927
  batch 350 loss: 3.1186258935928346
  batch 400 loss: 3.115396747589111
  batch 450 loss: 3.1126893663406374
  batch 500 loss: 3.111132917404175
  batch 550 loss: 3.1170793342590333
  batch 600 loss: 3.117185101509094
  batch 650 loss: 3.1007053327560423
  batch 700 loss: 3.099406352043152
  batch 750 loss: 3.093994641304016
  batch 800 loss: 3.0994488191604614
  batch 850 loss: 3.0960439252853393
  batch 900 loss: 3.1147126531600953
LOSS train 3.11471 valid 3.09544, valid PER 100.00%
EPOCH 8:
  batch 50 loss: 3.087955002784729
  batch 100 loss: 3.0908821296691893
  batch 150 loss: 3.071835603713989
  batch 200 loss: 3.0822512245178224
  batch 250 loss: 3.0694363737106323
  batch 300 loss: 3.071961612701416
  batch 350 loss: 3.0632190132141113
  batch 400 loss: 3.0650721406936645
  batch 450 loss: 3.0664754629135134
  batch 500 loss: 3.0614755296707155
  batch 550 loss: 3.060350832939148
  batch 600 loss: 3.0539037227630614
  batch 650 loss: 3.056792726516724
  batch 700 loss: 3.039427514076233
  batch 750 loss: 3.0378462934494017
  batch 800 loss: 3.031212363243103
  batch 850 loss: 3.0451986026763915
  batch 900 loss: 3.032395987510681
LOSS train 3.03240 valid 3.03198, valid PER 98.17%
EPOCH 9:
  batch 50 loss: 3.0165977621078492
  batch 100 loss: 3.028510584831238
  batch 150 loss: 3.011304383277893
  batch 200 loss: 3.0071284055709837
  batch 250 loss: 3.0054440689086914
  batch 300 loss: 3.0130548238754273
  batch 350 loss: 3.007301592826843
  batch 400 loss: 3.00321457862854
  batch 450 loss: 2.99795823097229
  batch 500 loss: 2.978681735992432
  batch 550 loss: 2.96613254070282
  batch 600 loss: 2.9756248569488526
  batch 650 loss: 2.9646741008758544
  batch 700 loss: 2.996479244232178
  batch 750 loss: 2.978067717552185
  batch 800 loss: 2.9733019351959227
  batch 850 loss: 2.9621929025650022
  batch 900 loss: 2.9679943561553954
LOSS train 2.96799 valid 2.96038, valid PER 90.04%
EPOCH 10:
  batch 50 loss: 2.9549332094192504
  batch 100 loss: 2.952276930809021
  batch 150 loss: 2.954649205207825
  batch 200 loss: 2.9519301652908325
  batch 250 loss: 2.9378747653961184
  batch 300 loss: 2.940177488327026
  batch 350 loss: 2.928382701873779
  batch 400 loss: 2.9378837823867796
  batch 450 loss: 2.894678692817688
  batch 500 loss: 2.9305913066864013
  batch 550 loss: 2.9246882724761964
  batch 600 loss: 2.912369661331177
  batch 650 loss: 2.901422128677368
  batch 700 loss: 2.895589008331299
  batch 750 loss: 2.885895791053772
  batch 800 loss: 2.901769337654114
  batch 850 loss: 2.8751841449737547
  batch 900 loss: 2.876438660621643
LOSS train 2.87644 valid 2.89111, valid PER 86.74%
EPOCH 11:
  batch 50 loss: 2.887308645248413
  batch 100 loss: 2.873532829284668
  batch 150 loss: 2.8642266035079955
  batch 200 loss: 2.8723076820373534
  batch 250 loss: 2.8734310722351073
  batch 300 loss: 2.869909644126892
  batch 350 loss: 2.8544477462768554
  batch 400 loss: 2.864807105064392
  batch 450 loss: 2.847551383972168
  batch 500 loss: 2.8412925386428833
  batch 550 loss: 2.826571283340454
  batch 600 loss: 2.8454591464996337
  batch 650 loss: 2.8534672117233275
  batch 700 loss: 2.8351005601882933
  batch 750 loss: 2.838826365470886
  batch 800 loss: 2.8379475402832033
  batch 850 loss: 2.8445511293411254
  batch 900 loss: 2.8393201780319215
LOSS train 2.83932 valid 2.82985, valid PER 85.14%
EPOCH 12:
  batch 50 loss: 2.8371210384368895
  batch 100 loss: 2.805066022872925
  batch 150 loss: 2.817264633178711
  batch 200 loss: 2.7984022235870363
  batch 250 loss: 2.832763366699219
  batch 300 loss: 2.8023238611221313
  batch 350 loss: 2.810642809867859
  batch 400 loss: 2.8155863380432127
  batch 450 loss: 2.7963576459884645
  batch 500 loss: 2.789780864715576
  batch 550 loss: 2.7569121313095093
  batch 600 loss: 2.775691452026367
  batch 650 loss: 2.781586799621582
  batch 700 loss: 2.7732918548583982
  batch 750 loss: 2.7696526765823366
  batch 800 loss: 2.7570045042037963
  batch 850 loss: 2.779456601142883
  batch 900 loss: 2.7789652442932127
LOSS train 2.77897 valid 2.77152, valid PER 84.84%
EPOCH 13:
  batch 50 loss: 2.7627621507644653
  batch 100 loss: 2.772877025604248
  batch 150 loss: 2.7456782722473143
  batch 200 loss: 2.7525116109848025
  batch 250 loss: 2.7406842041015627
  batch 300 loss: 2.7417001008987425
  batch 350 loss: 2.752201237678528
  batch 400 loss: 2.765225009918213
  batch 450 loss: 2.7393188524246215
  batch 500 loss: 2.7138342332839964
  batch 550 loss: 2.71227801322937
  batch 600 loss: 2.7381808376312256
  batch 650 loss: 2.7072777938842774
  batch 700 loss: 2.7263580322265626
  batch 750 loss: 2.7278417682647706
  batch 800 loss: 2.7055271816253663
  batch 850 loss: 2.713160877227783
  batch 900 loss: 2.7369665622711183
LOSS train 2.73697 valid 2.71820, valid PER 84.20%
EPOCH 14:
  batch 50 loss: 2.687935919761658
  batch 100 loss: 2.7234925413131714
  batch 150 loss: 2.6880840492248534
  batch 200 loss: 2.7251773357391356
  batch 250 loss: 2.6823594522476197
  batch 300 loss: 2.696601586341858
  batch 350 loss: 2.687077407836914
  batch 400 loss: 2.6824077224731444
  batch 450 loss: 2.6815743589401246
  batch 500 loss: 2.6926613807678224
  batch 550 loss: 2.678571677207947
  batch 600 loss: 2.681619234085083
  batch 650 loss: 2.696220541000366
  batch 700 loss: 2.6803157186508177
  batch 750 loss: 2.6560202264785766
  batch 800 loss: 2.6344301176071165
  batch 850 loss: 2.6792305850982667
  batch 900 loss: 2.6712745141983034
LOSS train 2.67127 valid 2.66919, valid PER 83.52%
EPOCH 15:
  batch 50 loss: 2.6665842437744143
  batch 100 loss: 2.652242422103882
  batch 150 loss: 2.642006974220276
  batch 200 loss: 2.6795546579360963
  batch 250 loss: 2.63429536819458
  batch 300 loss: 2.6416883993148805
  batch 350 loss: 2.63110933303833
  batch 400 loss: 2.6257423877716066
  batch 450 loss: 2.642573881149292
  batch 500 loss: 2.620198621749878
  batch 550 loss: 2.6550516748428343
  batch 600 loss: 2.6440262746810914
  batch 650 loss: 2.6280169439315797
  batch 700 loss: 2.633559217453003
  batch 750 loss: 2.612142162322998
  batch 800 loss: 2.6147092533111573
  batch 850 loss: 2.6221823787689207
  batch 900 loss: 2.6183494186401366
LOSS train 2.61835 valid 2.62328, valid PER 82.84%
EPOCH 16:
  batch 50 loss: 2.607470164299011
  batch 100 loss: 2.6082068490982055
  batch 150 loss: 2.597480731010437
  batch 200 loss: 2.611843452453613
  batch 250 loss: 2.6212614583969116
  batch 300 loss: 2.6008240842819212
  batch 350 loss: 2.6199287939071656
  batch 400 loss: 2.588080577850342
  batch 450 loss: 2.6010651206970214
  batch 500 loss: 2.57860999584198
  batch 550 loss: 2.6185348224639893
  batch 600 loss: 2.570335001945496
  batch 650 loss: 2.591893401145935
  batch 700 loss: 2.5607853078842164
  batch 750 loss: 2.568554563522339
  batch 800 loss: 2.5778404569625852
  batch 850 loss: 2.5649526643753053
  batch 900 loss: 2.5681379079818725
LOSS train 2.56814 valid 2.58095, valid PER 82.24%
EPOCH 17:
  batch 50 loss: 2.5486573076248167
  batch 100 loss: 2.5705678510665892
  batch 150 loss: 2.545132346153259
  batch 200 loss: 2.563067374229431
  batch 250 loss: 2.5637546491622927
  batch 300 loss: 2.5623527574539184
  batch 350 loss: 2.525612506866455
  batch 400 loss: 2.565377655029297
  batch 450 loss: 2.563721470832825
  batch 500 loss: 2.543184494972229
  batch 550 loss: 2.5603122758865355
  batch 600 loss: 2.5734892225265504
  batch 650 loss: 2.5384608840942384
  batch 700 loss: 2.546539077758789
  batch 750 loss: 2.5280987930297854
  batch 800 loss: 2.549215569496155
  batch 850 loss: 2.4970182085037234
  batch 900 loss: 2.5321106052398683
LOSS train 2.53211 valid 2.53934, valid PER 81.92%
EPOCH 18:
  batch 50 loss: 2.5279621076583862
  batch 100 loss: 2.5205981588363646
  batch 150 loss: 2.5282955837249754
  batch 200 loss: 2.5146859741210936
  batch 250 loss: 2.519901518821716
  batch 300 loss: 2.51147891998291
  batch 350 loss: 2.524570631980896
  batch 400 loss: 2.5312649488449095
  batch 450 loss: 2.5136067867279053
  batch 500 loss: 2.506230340003967
  batch 550 loss: 2.500190477371216
  batch 600 loss: 2.4940544748306275
  batch 650 loss: 2.4729649686813353
  batch 700 loss: 2.528792767524719
  batch 750 loss: 2.4741830825805664
  batch 800 loss: 2.5020251512527465
  batch 850 loss: 2.471687502861023
  batch 900 loss: 2.5065931844711304
LOSS train 2.50659 valid 2.49970, valid PER 81.60%
EPOCH 19:
  batch 50 loss: 2.4580590295791627
  batch 100 loss: 2.4617438983917235
  batch 150 loss: 2.4802667713165283
  batch 200 loss: 2.469115762710571
  batch 250 loss: 2.490479602813721
  batch 300 loss: 2.4906578063964844
  batch 350 loss: 2.4595087432861327
  batch 400 loss: 2.4959791326522827
  batch 450 loss: 2.472261080741882
  batch 500 loss: 2.4899908399581907
  batch 550 loss: 2.4749167013168334
  batch 600 loss: 2.4451087284088135
  batch 650 loss: 2.4879916858673097
  batch 700 loss: 2.4599420833587646
  batch 750 loss: 2.4496955251693726
  batch 800 loss: 2.4544880533218385
  batch 850 loss: 2.4636230993270876
  batch 900 loss: 2.43108108997345
LOSS train 2.43108 valid 2.46199, valid PER 81.18%
EPOCH 20:
  batch 50 loss: 2.454009838104248
  batch 100 loss: 2.4628763580322266
  batch 150 loss: 2.4596741724014284
  batch 200 loss: 2.4399966382980347
  batch 250 loss: 2.44558394908905
  batch 300 loss: 2.436842565536499
  batch 350 loss: 2.4263271570205687
  batch 400 loss: 2.41757794380188
  batch 450 loss: 2.4059154272079466
  batch 500 loss: 2.4141907119750976
  batch 550 loss: 2.4584264039993284
  batch 600 loss: 2.3905730724334715
  batch 650 loss: 2.4307343673706057
  batch 700 loss: 2.428616886138916
  batch 750 loss: 2.406140546798706
  batch 800 loss: 2.425231599807739
  batch 850 loss: 2.414739098548889
  batch 900 loss: 2.4236307764053344
LOSS train 2.42363 valid 2.42452, valid PER 80.84%
train_loss
[3.298761258125305, 3.253239951133728, 3.2202441024780275, 3.2059165811538697, 3.183266077041626, 3.145541481971741, 3.1147126531600953, 3.032395987510681, 2.9679943561553954, 2.876438660621643, 2.8393201780319215, 2.7789652442932127, 2.7369665622711183, 2.6712745141983034, 2.6183494186401366, 2.5681379079818725, 2.5321106052398683, 2.5065931844711304, 2.43108108997345, 2.4236307764053344]
valid_loss
[3.3192455768585205, 3.262887477874756, 3.2352399826049805, 3.2086856365203857, 3.179028034210205, 3.143022298812866, 3.095442056655884, 3.031982898712158, 2.9603793621063232, 2.891113519668579, 2.8298540115356445, 2.7715158462524414, 2.718196153640747, 2.669187307357788, 2.6232779026031494, 2.5809521675109863, 2.5393412113189697, 2.499702215194702, 2.4619932174682617, 2.424516201019287]
valid_per
[100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 98.17357685641915, 90.04132782295694, 86.7351019864018, 85.13531529129449, 84.83535528596187, 84.20210638581523, 83.51553126249833, 82.842287694974, 82.24236768430876, 81.91574456739102, 81.59578722836956, 81.17584322090387, 80.83588854819358]
Training finished in 4.0 minutes.
Model saved to checkpoints/20231208_015004/model_20
Loading model from checkpoints/20231208_015004/model_20
SUB: 2.99%, DEL: 77.70%, INS: 0.00%, COR: 19.31%, PER: 80.69%
