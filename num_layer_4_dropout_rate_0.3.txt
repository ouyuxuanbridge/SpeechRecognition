Namespace(seed=123, train_json='/rds/user/yo279/hpc-work/MLMI2/fbanks/train_fbank.json', val_json='/rds/user/yo279/hpc-work/MLMI2/fbanks/dev_fbank.json', test_json='/rds/user/yo279/hpc-work/MLMI2/fbanks/test_fbank.json', batch_size=4, num_layers=4, fbank_dims=23, model_dims=128, concat=1, lr=0.5, vocab='vocab_39.txt', report_interval=50, num_epochs=20, dropout_rate=0.3, clip_max_norm=1)
Total number of model parameters is 1352744
EPOCH 1:
  batch 50 loss: 4.184674968719483
  batch 100 loss: 3.3280353593826293
  batch 150 loss: 3.3082739782333372
  batch 200 loss: 3.278555908203125
  batch 250 loss: 3.2747042179107666
  batch 300 loss: 3.228685336112976
  batch 350 loss: 3.2046130990982054
  batch 400 loss: 3.1499972105026246
  batch 450 loss: 3.07443754196167
  batch 500 loss: 2.927214756011963
  batch 550 loss: 2.829353585243225
  batch 600 loss: 2.770527033805847
  batch 650 loss: 2.688255615234375
  batch 700 loss: 2.6614930534362795
  batch 750 loss: 2.56014790058136
  batch 800 loss: 2.5090924453735353
  batch 850 loss: 2.4319943523406984
  batch 900 loss: 2.3574393701553347
LOSS train 2.35744 valid 2.32776, valid PER 72.22%
EPOCH 2:
  batch 50 loss: 2.2430293202400207
  batch 100 loss: 2.160404498577118
  batch 150 loss: 2.0704631710052492
  batch 200 loss: 2.0440005087852477
  batch 250 loss: 1.9988482475280762
  batch 300 loss: 1.9035416388511657
  batch 350 loss: 1.836343114376068
  batch 400 loss: 1.8007691168785096
  batch 450 loss: 1.750966386795044
  batch 500 loss: 1.7346394729614258
  batch 550 loss: 1.7252535104751587
  batch 600 loss: 1.634919021129608
  batch 650 loss: 1.6704848337173461
  batch 700 loss: 1.572158420085907
  batch 750 loss: 1.5817863821983338
  batch 800 loss: 1.4968858098983764
  batch 850 loss: 1.4759812784194946
  batch 900 loss: 1.494681749343872
LOSS train 1.49468 valid 1.38387, valid PER 43.87%
EPOCH 3:
  batch 50 loss: 1.4338782286643983
  batch 100 loss: 1.3920303225517272
  batch 150 loss: 1.3984462237358093
  batch 200 loss: 1.3642998337745667
  batch 250 loss: 1.3290864396095277
  batch 300 loss: 1.338636074066162
  batch 350 loss: 1.3434902811050415
  batch 400 loss: 1.3519457185268402
  batch 450 loss: 1.2982547521591186
  batch 500 loss: 1.2918330001831055
  batch 550 loss: 1.2699867045879365
  batch 600 loss: 1.2544808888435364
  batch 650 loss: 1.1955245208740235
  batch 700 loss: 1.2648491227626801
  batch 750 loss: 1.2641658890247345
  batch 800 loss: 1.1976361644268037
  batch 850 loss: 1.2167429387569428
  batch 900 loss: 1.1585066151618957
LOSS train 1.15851 valid 1.18347, valid PER 36.79%
EPOCH 4:
  batch 50 loss: 1.1495808160305023
  batch 100 loss: 1.1291733515262603
  batch 150 loss: 1.1096463668346406
  batch 200 loss: 1.1370502841472625
  batch 250 loss: 1.1291131341457368
  batch 300 loss: 1.164398808479309
  batch 350 loss: 1.0699665105342866
  batch 400 loss: 1.113888887166977
  batch 450 loss: 1.0982303249835967
  batch 500 loss: 1.0882425582408906
  batch 550 loss: 1.0972259473800658
  batch 600 loss: 1.1265007209777833
  batch 650 loss: 1.1078360378742218
  batch 700 loss: 1.0479143512248994
  batch 750 loss: 1.0345474016666412
  batch 800 loss: 1.0217781710624694
  batch 850 loss: 1.0645744955539704
  batch 900 loss: 1.0885540401935578
LOSS train 1.08855 valid 1.01248, valid PER 31.54%
EPOCH 5:
  batch 50 loss: 0.9918850576877594
  batch 100 loss: 0.9862936532497406
  batch 150 loss: 1.0465616142749787
  batch 200 loss: 0.9701973092556
  batch 250 loss: 0.9998207950592041
  batch 300 loss: 0.9873638999462128
  batch 350 loss: 0.988121612071991
  batch 400 loss: 1.006109721660614
  batch 450 loss: 0.9967758595943451
  batch 500 loss: 1.0185552430152893
  batch 550 loss: 0.9307154643535615
  batch 600 loss: 1.0224566793441772
  batch 650 loss: 0.980243923664093
  batch 700 loss: 1.0011451637744904
  batch 750 loss: 0.9394275605678558
  batch 800 loss: 0.9729405152797699
  batch 850 loss: 0.9527719283103943
  batch 900 loss: 0.9617560291290284
LOSS train 0.96176 valid 0.93372, valid PER 29.36%
EPOCH 6:
  batch 50 loss: 0.9589046323299408
  batch 100 loss: 0.8987789154052734
  batch 150 loss: 0.8859014737606049
  batch 200 loss: 0.9023160433769226
  batch 250 loss: 0.9337044370174408
  batch 300 loss: 0.9206253182888031
  batch 350 loss: 0.8991789126396179
  batch 400 loss: 0.9020476162433624
  batch 450 loss: 0.9277981865406036
  batch 500 loss: 0.9252203297615051
  batch 550 loss: 0.915090594291687
  batch 600 loss: 0.8868696129322052
  batch 650 loss: 0.8996621942520142
  batch 700 loss: 0.909692007303238
  batch 750 loss: 0.8718115985393524
  batch 800 loss: 0.8830129766464233
  batch 850 loss: 0.8712206697463989
  batch 900 loss: 0.9033986580371857
LOSS train 0.90340 valid 0.90620, valid PER 27.91%
EPOCH 7:
  batch 50 loss: 0.8462968695163727
  batch 100 loss: 0.8772492289543152
  batch 150 loss: 0.8655605220794678
  batch 200 loss: 0.8454944336414337
  batch 250 loss: 0.8335181587934494
  batch 300 loss: 0.8138855385780335
  batch 350 loss: 0.8400709116458893
  batch 400 loss: 0.8189087498188019
  batch 450 loss: 0.8553902685642243
  batch 500 loss: 0.8411701583862304
  batch 550 loss: 0.8244649577140808
  batch 600 loss: 0.8438967406749726
  batch 650 loss: 0.8335913228988647
  batch 700 loss: 0.878809678554535
  batch 750 loss: 0.8269401669502259
  batch 800 loss: 0.825087159872055
  batch 850 loss: 0.841559669971466
  batch 900 loss: 0.8692268860340119
LOSS train 0.86923 valid 0.85053, valid PER 26.91%
EPOCH 8:
  batch 50 loss: 0.7866232359409332
  batch 100 loss: 0.7886537790298462
  batch 150 loss: 0.7779489880800248
  batch 200 loss: 0.7706432390213013
  batch 250 loss: 0.7803527581691742
  batch 300 loss: 0.7637927716970444
  batch 350 loss: 0.807437230348587
  batch 400 loss: 0.7588648670911788
  batch 450 loss: 0.7962695217132568
  batch 500 loss: 0.8115875160694123
  batch 550 loss: 0.7362411463260651
  batch 600 loss: 0.8042385792732238
  batch 650 loss: 0.8250930881500245
  batch 700 loss: 0.769448539018631
  batch 750 loss: 0.7758855378627777
  batch 800 loss: 0.7953199458122253
  batch 850 loss: 0.7760296428203582
  batch 900 loss: 0.7806950587034226
LOSS train 0.78070 valid 0.83642, valid PER 26.45%
EPOCH 9:
  batch 50 loss: 0.7044569611549377
  batch 100 loss: 0.7458448541164399
  batch 150 loss: 0.7471565878391266
  batch 200 loss: 0.7259216636419297
  batch 250 loss: 0.7685495316982269
  batch 300 loss: 0.7697496604919434
  batch 350 loss: 0.7677297002077103
  batch 400 loss: 0.7342367798089982
  batch 450 loss: 0.7327080690860748
  batch 500 loss: 0.7306993293762207
  batch 550 loss: 0.7446778392791749
  batch 600 loss: 0.7767397487163543
  batch 650 loss: 0.7422483479976654
  batch 700 loss: 0.7240641677379608
  batch 750 loss: 0.7344163411855698
  batch 800 loss: 0.7451643288135529
  batch 850 loss: 0.7761492133140564
  batch 900 loss: 0.7434937125444412
LOSS train 0.74349 valid 0.81492, valid PER 25.36%
EPOCH 10:
  batch 50 loss: 0.6752833783626556
  batch 100 loss: 0.6860384947061539
  batch 150 loss: 0.7034944987297058
  batch 200 loss: 0.7161659216880798
  batch 250 loss: 0.7267977058887481
  batch 300 loss: 0.6748898285627365
  batch 350 loss: 0.714892235994339
  batch 400 loss: 0.6966267263889313
  batch 450 loss: 0.6872728192806243
  batch 500 loss: 0.7170796746015549
  batch 550 loss: 0.7239102375507355
  batch 600 loss: 0.7306800937652588
  batch 650 loss: 0.7068154752254486
  batch 700 loss: 0.7066132140159607
  batch 750 loss: 0.6900207495689392
  batch 800 loss: 0.7377674859762192
  batch 850 loss: 0.7241961306333542
  batch 900 loss: 0.7235481345653534
LOSS train 0.72355 valid 0.81192, valid PER 25.79%
EPOCH 11:
  batch 50 loss: 0.6415361750125885
  batch 100 loss: 0.6198195242881774
  batch 150 loss: 0.647923042178154
  batch 200 loss: 0.6932223451137542
  batch 250 loss: 0.6783438593149185
  batch 300 loss: 0.6488408893346786
  batch 350 loss: 0.6646041154861451
  batch 400 loss: 0.6815885162353515
  batch 450 loss: 0.6874680584669113
  batch 500 loss: 0.6798580455780029
  batch 550 loss: 0.7020776295661926
  batch 600 loss: 0.682571314573288
  batch 650 loss: 0.745635312795639
  batch 700 loss: 0.6575321352481842
  batch 750 loss: 0.6669039541482925
  batch 800 loss: 0.71266168653965
  batch 850 loss: 0.7195589303970337
  batch 900 loss: 0.722564662694931
LOSS train 0.72256 valid 0.78497, valid PER 24.09%
EPOCH 12:
  batch 50 loss: 0.6372601008415222
  batch 100 loss: 0.6586589324474335
  batch 150 loss: 0.6014631819725037
  batch 200 loss: 0.6287175726890564
  batch 250 loss: 0.6427851539850234
  batch 300 loss: 0.6216052407026291
  batch 350 loss: 0.6287944328784942
  batch 400 loss: 0.6662339532375335
  batch 450 loss: 0.6569496458768844
  batch 500 loss: 0.6729851853847504
  batch 550 loss: 0.6234334999322891
  batch 600 loss: 0.6574021524190903
  batch 650 loss: 0.6877086305618286
  batch 700 loss: 0.6885246938467026
  batch 750 loss: 0.6359885567426682
  batch 800 loss: 0.6251917511224747
  batch 850 loss: 0.7003547477722168
  batch 900 loss: 0.6821972751617431
LOSS train 0.68220 valid 0.75939, valid PER 23.66%
EPOCH 13:
  batch 50 loss: 0.591312403678894
  batch 100 loss: 0.6144879615306854
  batch 150 loss: 0.6098194515705109
  batch 200 loss: 0.6364821010828018
  batch 250 loss: 0.6005278301239013
  batch 300 loss: 0.6243050599098205
  batch 350 loss: 0.5954934340715409
  batch 400 loss: 0.5994858580827713
  batch 450 loss: 0.6168417477607727
  batch 500 loss: 0.6042939007282258
  batch 550 loss: 0.6370566785335541
  batch 600 loss: 0.61119249522686
  batch 650 loss: 0.6380871498584747
  batch 700 loss: 0.6389261388778686
  batch 750 loss: 0.5915339666604996
  batch 800 loss: 0.5987842428684235
  batch 850 loss: 0.656309163570404
  batch 900 loss: 0.6601091653108597
LOSS train 0.66011 valid 0.77224, valid PER 23.91%
EPOCH 14:
  batch 50 loss: 0.5996019768714905
  batch 100 loss: 0.6363282799720764
  batch 150 loss: 0.5973220098018647
  batch 200 loss: 0.5916566842794418
  batch 250 loss: 0.5975814712047577
  batch 300 loss: 0.6212990963459015
  batch 350 loss: 0.5714149433374405
  batch 400 loss: 0.5973543310165406
  batch 450 loss: 0.6172675675153733
  batch 500 loss: 0.6350323969125747
  batch 550 loss: 0.644593134522438
  batch 600 loss: 0.6010949808359146
  batch 650 loss: 0.6199462348222733
  batch 700 loss: 0.6355785870552063
  batch 750 loss: 0.5905732578039169
  batch 800 loss: 0.5744949972629547
  batch 850 loss: 0.6099284482002258
  batch 900 loss: 0.6176692634820938
LOSS train 0.61767 valid 0.73417, valid PER 22.96%
EPOCH 15:
  batch 50 loss: 0.544880433678627
  batch 100 loss: 0.5748425996303559
  batch 150 loss: 0.5523151737451554
  batch 200 loss: 0.5919456106424331
  batch 250 loss: 0.6017928087711334
  batch 300 loss: 0.5538497751951218
  batch 350 loss: 0.552829195857048
  batch 400 loss: 0.5483659088611603
  batch 450 loss: 0.5640733641386032
  batch 500 loss: 0.5412662613391876
  batch 550 loss: 0.579349507689476
  batch 600 loss: 0.591987356543541
  batch 650 loss: 0.6022203361988068
  batch 700 loss: 0.5958413475751877
  batch 750 loss: 0.5744441944360733
  batch 800 loss: 0.5743175208568573
  batch 850 loss: 0.5618585139513016
  batch 900 loss: 0.5672238051891327
LOSS train 0.56722 valid 0.75239, valid PER 22.46%
EPOCH 16:
  batch 50 loss: 0.5709373587369919
  batch 100 loss: 0.5213934826850891
  batch 150 loss: 0.5255916583538055
  batch 200 loss: 0.5204842501878738
  batch 250 loss: 0.5589064586162568
  batch 300 loss: 0.5466185522079468
  batch 350 loss: 0.5518676298856735
  batch 400 loss: 0.5537112033367158
  batch 450 loss: 0.5631009310483932
  batch 500 loss: 0.521359885931015
  batch 550 loss: 0.5317198950052261
  batch 600 loss: 0.5285454946756363
  batch 650 loss: 0.551475270986557
  batch 700 loss: 0.5380018055438995
  batch 750 loss: 0.5480499106645584
  batch 800 loss: 0.5653007173538208
  batch 850 loss: 0.5451671040058136
  batch 900 loss: 0.5587703907489776
LOSS train 0.55877 valid 0.71244, valid PER 21.91%
EPOCH 17:
  batch 50 loss: 0.5128068733215332
  batch 100 loss: 0.5313638436794281
  batch 150 loss: 0.4978916800022125
  batch 200 loss: 0.5189674866199493
  batch 250 loss: 0.5344836020469665
  batch 300 loss: 0.5114299726486206
  batch 350 loss: 0.5056491583585739
  batch 400 loss: 0.5317992693185807
  batch 450 loss: 0.526568438410759
  batch 500 loss: 0.5428222239017486
  batch 550 loss: 0.553758961558342
  batch 600 loss: 0.5820674002170563
  batch 650 loss: 0.5540138202905655
  batch 700 loss: 0.5504354602098465
  batch 750 loss: 0.5412061554193497
  batch 800 loss: 0.5251247972249985
  batch 850 loss: 0.5514886045455932
  batch 900 loss: 0.5200752282142639
LOSS train 0.52008 valid 0.72683, valid PER 22.08%
EPOCH 18:
  batch 50 loss: 0.5142627012729645
  batch 100 loss: 0.5041515964269638
  batch 150 loss: 0.5198953872919083
  batch 200 loss: 0.5085794365406037
  batch 250 loss: 0.5252718532085419
  batch 300 loss: 0.5006249314546585
  batch 350 loss: 0.5001562428474426
  batch 400 loss: 0.5014647513628006
  batch 450 loss: 0.5207952100038529
  batch 500 loss: 0.5153152352571487
  batch 550 loss: 0.5371696794033051
  batch 600 loss: 0.4967078506946564
  batch 650 loss: 0.5166142964363098
  batch 700 loss: 0.5363750094175339
  batch 750 loss: 0.5142169803380966
  batch 800 loss: 0.4894206666946411
  batch 850 loss: 0.5036171567440033
  batch 900 loss: 0.5232963997125626
LOSS train 0.52330 valid 0.70168, valid PER 21.42%
EPOCH 19:
  batch 50 loss: 0.44627136528491973
  batch 100 loss: 0.4429998028278351
  batch 150 loss: 0.4579235589504242
  batch 200 loss: 0.4744550448656082
  batch 250 loss: 0.48947063624858855
  batch 300 loss: 0.5133116048574448
  batch 350 loss: 0.46411227345466616
  batch 400 loss: 0.48783436834812166
  batch 450 loss: 0.5177411210536956
  batch 500 loss: 0.5192114162445068
  batch 550 loss: 0.48987335085868833
  batch 600 loss: 0.48932420611381533
  batch 650 loss: 0.5386279201507569
  batch 700 loss: 0.46925522446632384
  batch 750 loss: 0.48456123054027556
  batch 800 loss: 0.5180008935928345
  batch 850 loss: 0.5150990998744964
  batch 900 loss: 0.5186914175748825
LOSS train 0.51869 valid 0.73299, valid PER 22.03%
EPOCH 20:
  batch 50 loss: 0.4689793762564659
  batch 100 loss: 0.46419333696365356
  batch 150 loss: 0.4395681548118591
  batch 200 loss: 0.490862517952919
  batch 250 loss: 0.45687198638916016
  batch 300 loss: 0.46877775847911834
  batch 350 loss: 0.44543180376291275
  batch 400 loss: 0.47712752878665926
  batch 450 loss: 0.47886775076389315
  batch 500 loss: 0.4481644785404205
  batch 550 loss: 0.4969900107383728
  batch 600 loss: 0.45037244617938993
  batch 650 loss: 0.47215617299079893
  batch 700 loss: 0.4863311183452606
  batch 750 loss: 0.4578036141395569
  batch 800 loss: 0.5057572847604752
  batch 850 loss: 0.49418443858623506
  batch 900 loss: 0.4873260948061943
LOSS train 0.48733 valid 0.77496, valid PER 22.50%
train_loss
[2.3574393701553347, 1.494681749343872, 1.1585066151618957, 1.0885540401935578, 0.9617560291290284, 0.9033986580371857, 0.8692268860340119, 0.7806950587034226, 0.7434937125444412, 0.7235481345653534, 0.722564662694931, 0.6821972751617431, 0.6601091653108597, 0.6176692634820938, 0.5672238051891327, 0.5587703907489776, 0.5200752282142639, 0.5232963997125626, 0.5186914175748825, 0.4873260948061943]
valid_loss
[2.32776141166687, 1.383870005607605, 1.1834681034088135, 1.0124841928482056, 0.933722198009491, 0.9062039852142334, 0.8505262732505798, 0.8364183902740479, 0.814917266368866, 0.811915934085846, 0.7849667072296143, 0.7593896389007568, 0.7722378969192505, 0.7341693639755249, 0.752385139465332, 0.7124448418617249, 0.7268297672271729, 0.7016812562942505, 0.732988178730011, 0.7749574184417725]
valid_per
[72.21703772830288, 43.86748433542194, 36.788428209572054, 31.535795227303026, 29.362751633115586, 27.909612051726434, 26.90974536728436, 26.44980669244101, 25.363284895347288, 25.789894680709242, 24.090121317157713, 23.656845753899482, 23.91014531395814, 22.95693907479003, 22.46367151046527, 21.91041194507399, 22.077056392481005, 21.423810158645516, 22.03039594720704, 22.503666177842955]
Training finished in 7.0 minutes.
Model saved to checkpoints/20231207_233951/model_18
Loading model from checkpoints/20231207_233951/model_18
SUB: 14.01%, DEL: 7.06%, INS: 2.25%, COR: 78.92%, PER: 23.33%
