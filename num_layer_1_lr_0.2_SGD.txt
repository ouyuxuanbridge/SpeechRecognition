Namespace(seed=123, train_json='/rds/user/yo279/hpc-work/MLMI2/fbanks/train_fbank.json', val_json='/rds/user/yo279/hpc-work/MLMI2/fbanks/dev_fbank.json', test_json='/rds/user/yo279/hpc-work/MLMI2/fbanks/test_fbank.json', batch_size=4, num_layers=1, fbank_dims=23, model_dims=128, concat=1, lr=0.2, vocab='vocab_39.txt', report_interval=50, num_epochs=20, dropout_rate=0.5, clip_max_norm=1)
Total number of model parameters is 166952
EPOCH 1:
  batch 50 loss: 4.322671766281128
  batch 100 loss: 3.281926736831665
  batch 150 loss: 3.2281560468673707
  batch 200 loss: 3.170402960777283
  batch 250 loss: 3.0773828411102295
  batch 300 loss: 2.9321776151657106
  batch 350 loss: 2.8116761589050294
  batch 400 loss: 2.7307292127609255
  batch 450 loss: 2.6637742376327513
  batch 500 loss: 2.5493325328826906
  batch 550 loss: 2.4840116262435914
  batch 600 loss: 2.432348952293396
  batch 650 loss: 2.3532735824584963
  batch 700 loss: 2.3392877197265625
  batch 750 loss: 2.2833226490020753
  batch 800 loss: 2.2499492812156676
  batch 850 loss: 2.219666447639465
  batch 900 loss: 2.179884412288666
LOSS train 2.17988 valid 2.12671, valid PER 80.70%
EPOCH 2:
  batch 50 loss: 2.145778558254242
  batch 100 loss: 2.073728926181793
  batch 150 loss: 2.034166431427002
  batch 200 loss: 2.0298707771301268
  batch 250 loss: 2.02460321187973
  batch 300 loss: 1.9888739418983459
  batch 350 loss: 1.90036776304245
  batch 400 loss: 1.9163428688049315
  batch 450 loss: 1.8702282238006591
  batch 500 loss: 1.8582914996147155
  batch 550 loss: 1.859677004814148
  batch 600 loss: 1.7930259919166565
  batch 650 loss: 1.84797856092453
  batch 700 loss: 1.7931934022903442
  batch 750 loss: 1.7854388880729675
  batch 800 loss: 1.7192203545570373
  batch 850 loss: 1.720119731426239
  batch 900 loss: 1.7267352843284607
LOSS train 1.72674 valid 1.69280, valid PER 67.09%
EPOCH 3:
  batch 50 loss: 1.6967299842834473
  batch 100 loss: 1.650236668586731
  batch 150 loss: 1.65824844121933
  batch 200 loss: 1.6498102974891662
  batch 250 loss: 1.6053014230728149
  batch 300 loss: 1.5966021704673767
  batch 350 loss: 1.642491750717163
  batch 400 loss: 1.6214238142967223
  batch 450 loss: 1.5753969573974609
  batch 500 loss: 1.571289939880371
  batch 550 loss: 1.5741098022460938
  batch 600 loss: 1.5261646699905396
  batch 650 loss: 1.508556876182556
  batch 700 loss: 1.5251554656028747
  batch 750 loss: 1.5823089861869812
  batch 800 loss: 1.4909540939331054
  batch 850 loss: 1.514642472267151
  batch 900 loss: 1.4587984418869018
LOSS train 1.45880 valid 1.52346, valid PER 54.37%
EPOCH 4:
  batch 50 loss: 1.4702768397331238
  batch 100 loss: 1.4761505556106567
  batch 150 loss: 1.4236919093132019
  batch 200 loss: 1.4671483731269837
  batch 250 loss: 1.4628882265090943
  batch 300 loss: 1.461614511013031
  batch 350 loss: 1.3601337790489196
  batch 400 loss: 1.4299762988090514
  batch 450 loss: 1.407622058391571
  batch 500 loss: 1.3845091581344604
  batch 550 loss: 1.4108004236221314
  batch 600 loss: 1.4092334246635436
  batch 650 loss: 1.40020525932312
  batch 700 loss: 1.3695052766799927
  batch 750 loss: 1.3443441438674926
  batch 800 loss: 1.3331642210483552
  batch 850 loss: 1.3603749680519104
  batch 900 loss: 1.4029073846340179
LOSS train 1.40291 valid 1.37203, valid PER 50.28%
EPOCH 5:
  batch 50 loss: 1.3296550631523132
  batch 100 loss: 1.3277511858940125
  batch 150 loss: 1.3570250606536864
  batch 200 loss: 1.2993085277080536
  batch 250 loss: 1.2847337424755096
  batch 300 loss: 1.3155888390541077
  batch 350 loss: 1.3150837552547454
  batch 400 loss: 1.3056238889694214
  batch 450 loss: 1.2822420072555543
  batch 500 loss: 1.290285097360611
  batch 550 loss: 1.2366458034515382
  batch 600 loss: 1.3131803107261657
  batch 650 loss: 1.2692844998836517
  batch 700 loss: 1.316027204990387
  batch 750 loss: 1.2404906785488128
  batch 800 loss: 1.2859906721115113
  batch 850 loss: 1.2770031440258025
  batch 900 loss: 1.2750213754177093
LOSS train 1.27502 valid 1.26906, valid PER 43.29%
EPOCH 6:
  batch 50 loss: 1.2756491827964782
  batch 100 loss: 1.2047909414768219
  batch 150 loss: 1.2228316128253938
  batch 200 loss: 1.2191628456115722
  batch 250 loss: 1.2454731631278992
  batch 300 loss: 1.1981842231750488
  batch 350 loss: 1.2286496460437775
  batch 400 loss: 1.2063826251029968
  batch 450 loss: 1.2322864270210265
  batch 500 loss: 1.2115116333961486
  batch 550 loss: 1.2451828742027282
  batch 600 loss: 1.1918407022953033
  batch 650 loss: 1.209099875688553
  batch 700 loss: 1.201643397808075
  batch 750 loss: 1.1793082499504088
  batch 800 loss: 1.1691164398193359
  batch 850 loss: 1.1615342032909393
  batch 900 loss: 1.1989820277690888
LOSS train 1.19898 valid 1.24694, valid PER 43.43%
EPOCH 7:
  batch 50 loss: 1.1757864141464234
  batch 100 loss: 1.174010419845581
  batch 150 loss: 1.1721647322177886
  batch 200 loss: 1.1440229749679565
  batch 250 loss: 1.1455593585968018
  batch 300 loss: 1.127171484231949
  batch 350 loss: 1.1241882944107056
  batch 400 loss: 1.1472679126262664
  batch 450 loss: 1.144382815361023
  batch 500 loss: 1.1244834077358246
  batch 550 loss: 1.135568596124649
  batch 600 loss: 1.1426041209697724
  batch 650 loss: 1.1325882697105407
  batch 700 loss: 1.1519121289253236
  batch 750 loss: 1.1219504952430726
  batch 800 loss: 1.1076301586627961
  batch 850 loss: 1.1522933948040008
  batch 900 loss: 1.1667489910125732
LOSS train 1.16675 valid 1.16719, valid PER 39.95%
EPOCH 8:
  batch 50 loss: 1.105302939414978
  batch 100 loss: 1.0804173266887664
  batch 150 loss: 1.0899245679378509
  batch 200 loss: 1.0667307043075562
  batch 250 loss: 1.096697233915329
  batch 300 loss: 1.0313199591636657
  batch 350 loss: 1.1187274527549744
  batch 400 loss: 1.0871634006500244
  batch 450 loss: 1.0899520766735078
  batch 500 loss: 1.122886655330658
  batch 550 loss: 1.0532067513465881
  batch 600 loss: 1.116316056251526
  batch 650 loss: 1.1237773656845094
  batch 700 loss: 1.0595157587528228
  batch 750 loss: 1.0823280489444733
  batch 800 loss: 1.106951186656952
  batch 850 loss: 1.0845689713954925
  batch 900 loss: 1.0894800114631653
LOSS train 1.08948 valid 1.13871, valid PER 36.03%
EPOCH 9:
  batch 50 loss: 0.9998004710674286
  batch 100 loss: 1.06035915017128
  batch 150 loss: 1.0508463299274444
  batch 200 loss: 1.0234776246547699
  batch 250 loss: 1.0673011755943298
  batch 300 loss: 1.063419519662857
  batch 350 loss: 1.0774840533733367
  batch 400 loss: 1.0523704969882965
  batch 450 loss: 1.0403727352619172
  batch 500 loss: 1.036641993522644
  batch 550 loss: 1.0494292724132537
  batch 600 loss: 1.0679810297489167
  batch 650 loss: 1.027731215953827
  batch 700 loss: 1.0322584974765778
  batch 750 loss: 1.0309319150447847
  batch 800 loss: 1.0630335795879364
  batch 850 loss: 1.061193367242813
  batch 900 loss: 1.0128855407238007
LOSS train 1.01289 valid 1.10414, valid PER 35.18%
EPOCH 10:
  batch 50 loss: 0.9812711024284363
  batch 100 loss: 0.9934441781044007
  batch 150 loss: 1.0292069625854492
  batch 200 loss: 1.016356806755066
  batch 250 loss: 1.038763655424118
  batch 300 loss: 0.9907258307933807
  batch 350 loss: 1.0025073027610778
  batch 400 loss: 0.9729948258399963
  batch 450 loss: 0.9648616826534271
  batch 500 loss: 1.0131632566452027
  batch 550 loss: 1.0346670269966125
  batch 600 loss: 1.0067324912548066
  batch 650 loss: 0.9816195440292358
  batch 700 loss: 1.0086255872249603
  batch 750 loss: 0.9968966245651245
  batch 800 loss: 1.0012479960918426
  batch 850 loss: 1.0266911721229552
  batch 900 loss: 1.0142720425128937
LOSS train 1.01427 valid 1.08373, valid PER 35.40%
EPOCH 11:
  batch 50 loss: 0.9522739505767822
  batch 100 loss: 0.9181442868709564
  batch 150 loss: 0.9761340880393982
  batch 200 loss: 1.0059793782234192
  batch 250 loss: 0.9862556803226471
  batch 300 loss: 0.9551709699630737
  batch 350 loss: 0.9691418850421906
  batch 400 loss: 0.9887410998344421
  batch 450 loss: 0.9860072910785675
  batch 500 loss: 0.9684942030906677
  batch 550 loss: 0.9691460227966309
  batch 600 loss: 0.9620658111572266
  batch 650 loss: 1.029014276266098
  batch 700 loss: 0.9357350206375122
  batch 750 loss: 0.9516242790222168
  batch 800 loss: 0.9893124890327454
  batch 850 loss: 0.9870605862140656
  batch 900 loss: 0.9928170037269592
LOSS train 0.99282 valid 1.08132, valid PER 34.63%
EPOCH 12:
  batch 50 loss: 0.9591073250770569
  batch 100 loss: 0.9398939597606659
  batch 150 loss: 0.9213958525657654
  batch 200 loss: 0.9199898660182952
  batch 250 loss: 0.9558533918857575
  batch 300 loss: 0.9439419496059418
  batch 350 loss: 0.9206762218475342
  batch 400 loss: 0.9578048503398895
  batch 450 loss: 0.9493506228923798
  batch 500 loss: 0.9520647382736206
  batch 550 loss: 0.8934551584720611
  batch 600 loss: 0.9196073567867279
  batch 650 loss: 0.959231150150299
  batch 700 loss: 0.9600561094284058
  batch 750 loss: 0.9152371561527253
  batch 800 loss: 0.9205974841117859
  batch 850 loss: 0.9823150527477265
  batch 900 loss: 0.9757712495326996
LOSS train 0.97577 valid 1.05073, valid PER 33.04%
EPOCH 13:
  batch 50 loss: 0.8885874724388123
  batch 100 loss: 0.907105667591095
  batch 150 loss: 0.8849591088294982
  batch 200 loss: 0.9208296775817871
  batch 250 loss: 0.9105953240394592
  batch 300 loss: 0.8887686240673065
  batch 350 loss: 0.9150083220005035
  batch 400 loss: 0.9442834043502808
  batch 450 loss: 0.9148972713947296
  batch 500 loss: 0.8951536476612091
  batch 550 loss: 0.9177419543266296
  batch 600 loss: 0.9088203573226928
  batch 650 loss: 0.9352019786834717
  batch 700 loss: 0.922259259223938
  batch 750 loss: 0.8808260321617126
  batch 800 loss: 0.895152838230133
  batch 850 loss: 0.9498967587947845
  batch 900 loss: 0.9395291638374329
LOSS train 0.93953 valid 1.03310, valid PER 32.51%
EPOCH 14:
  batch 50 loss: 0.8769128596782685
  batch 100 loss: 0.8889936518669128
  batch 150 loss: 0.8810834193229675
  batch 200 loss: 0.8748838019371032
  batch 250 loss: 0.8686059951782227
  batch 300 loss: 0.9200893938541412
  batch 350 loss: 0.8828669703006744
  batch 400 loss: 0.8672298228740692
  batch 450 loss: 0.8941940712928772
  batch 500 loss: 0.9024213719367981
  batch 550 loss: 0.9162517404556274
  batch 600 loss: 0.8759722578525543
  batch 650 loss: 0.8938706171512604
  batch 700 loss: 0.935673919916153
  batch 750 loss: 0.8628369891643524
  batch 800 loss: 0.8576668572425842
  batch 850 loss: 0.9002700352668762
  batch 900 loss: 0.8935857629776001
LOSS train 0.89359 valid 1.02771, valid PER 32.49%
EPOCH 15:
  batch 50 loss: 0.8608627843856812
  batch 100 loss: 0.8520645332336426
  batch 150 loss: 0.8496200144290924
  batch 200 loss: 0.8823910534381867
  batch 250 loss: 0.8770118224620819
  batch 300 loss: 0.8622947072982788
  batch 350 loss: 0.8576062631607055
  batch 400 loss: 0.862436398267746
  batch 450 loss: 0.8465381407737732
  batch 500 loss: 0.8281551241874695
  batch 550 loss: 0.8513567471504211
  batch 600 loss: 0.8881393706798554
  batch 650 loss: 0.9077705240249634
  batch 700 loss: 0.8941760003566742
  batch 750 loss: 0.8705845975875854
  batch 800 loss: 0.8595118343830108
  batch 850 loss: 0.8497794985771179
  batch 900 loss: 0.8793302869796753
LOSS train 0.87933 valid 1.03543, valid PER 31.85%
EPOCH 16:
  batch 50 loss: 0.8566809666156768
  batch 100 loss: 0.7973217332363128
  batch 150 loss: 0.8267515552043915
  batch 200 loss: 0.8317074191570282
  batch 250 loss: 0.8567121124267578
  batch 300 loss: 0.8353814303874969
  batch 350 loss: 0.8573996043205261
  batch 400 loss: 0.853187415599823
  batch 450 loss: 0.8727002167701721
  batch 500 loss: 0.8370043790340423
  batch 550 loss: 0.8475682699680328
  batch 600 loss: 0.8347836780548096
  batch 650 loss: 0.8530822050571442
  batch 700 loss: 0.8264293479919433
  batch 750 loss: 0.8644874930381775
  batch 800 loss: 0.8503486490249634
  batch 850 loss: 0.8382282197475434
  batch 900 loss: 0.837267290353775
LOSS train 0.83727 valid 1.02460, valid PER 31.20%
EPOCH 17:
  batch 50 loss: 0.8217101931571961
  batch 100 loss: 0.8268512296676636
  batch 150 loss: 0.807717307806015
  batch 200 loss: 0.8047549951076508
  batch 250 loss: 0.8275075900554657
  batch 300 loss: 0.8242715549468994
  batch 350 loss: 0.800804134607315
  batch 400 loss: 0.8555623304843902
  batch 450 loss: 0.832306694984436
  batch 500 loss: 0.8189719939231872
  batch 550 loss: 0.8247192478179932
  batch 600 loss: 0.8598177266120911
  batch 650 loss: 0.8257291805744171
  batch 700 loss: 0.813879644870758
  batch 750 loss: 0.8068516743183136
  batch 800 loss: 0.8181290155649186
  batch 850 loss: 0.8219578969478607
  batch 900 loss: 0.8129645407199859
LOSS train 0.81296 valid 1.01772, valid PER 31.30%
EPOCH 18:
  batch 50 loss: 0.7973133301734925
  batch 100 loss: 0.7912241518497467
  batch 150 loss: 0.822344069480896
  batch 200 loss: 0.8047971248626709
  batch 250 loss: 0.8217247986793518
  batch 300 loss: 0.7843711745738983
  batch 350 loss: 0.8141242492198945
  batch 400 loss: 0.7903525924682617
  batch 450 loss: 0.8225277054309845
  batch 500 loss: 0.8145726799964905
  batch 550 loss: 0.8173819100856781
  batch 600 loss: 0.7774097895622254
  batch 650 loss: 0.7890102887153625
  batch 700 loss: 0.8170741510391235
  batch 750 loss: 0.8052054965496063
  batch 800 loss: 0.8063952529430389
  batch 850 loss: 0.8011281418800354
  batch 900 loss: 0.8313964128494262
LOSS train 0.83140 valid 1.01812, valid PER 31.41%
EPOCH 19:
  batch 50 loss: 0.7399025440216065
  batch 100 loss: 0.7415721786022186
  batch 150 loss: 0.7632135689258576
  batch 200 loss: 0.7698307847976684
  batch 250 loss: 0.7951792395114898
  batch 300 loss: 0.7758619868755341
  batch 350 loss: 0.7826498210430145
  batch 400 loss: 0.7971175491809845
  batch 450 loss: 0.8096836936473847
  batch 500 loss: 0.7909169435501099
  batch 550 loss: 0.7745761477947235
  batch 600 loss: 0.787454960346222
  batch 650 loss: 0.8405220127105713
  batch 700 loss: 0.7899827444553376
  batch 750 loss: 0.7674556708335877
  batch 800 loss: 0.7914829862117767
  batch 850 loss: 0.8012038969993591
  batch 900 loss: 0.7987826204299927
LOSS train 0.79878 valid 1.01714, valid PER 31.14%
EPOCH 20:
  batch 50 loss: 0.7504169631004334
  batch 100 loss: 0.7451079356670379
  batch 150 loss: 0.7408413791656494
  batch 200 loss: 0.7527684485912323
  batch 250 loss: 0.7369627565145492
  batch 300 loss: 0.782655308842659
  batch 350 loss: 0.7482754355669021
  batch 400 loss: 0.7637768298387527
  batch 450 loss: 0.7589947056770324
  batch 500 loss: 0.7469616723060608
  batch 550 loss: 0.807281197309494
  batch 600 loss: 0.749244577884674
  batch 650 loss: 0.7873481988906861
  batch 700 loss: 0.7899958372116089
  batch 750 loss: 0.7417851066589356
  batch 800 loss: 0.8113348639011383
  batch 850 loss: 0.801138356924057
  batch 900 loss: 0.7837813633680344
LOSS train 0.78378 valid 1.01715, valid PER 30.43%
train_loss
[2.179884412288666, 1.7267352843284607, 1.4587984418869018, 1.4029073846340179, 1.2750213754177093, 1.1989820277690888, 1.1667489910125732, 1.0894800114631653, 1.0128855407238007, 1.0142720425128937, 0.9928170037269592, 0.9757712495326996, 0.9395291638374329, 0.8935857629776001, 0.8793302869796753, 0.837267290353775, 0.8129645407199859, 0.8313964128494262, 0.7987826204299927, 0.7837813633680344]
valid_loss
[2.1267080307006836, 1.6928030252456665, 1.5234614610671997, 1.3720277547836304, 1.2690571546554565, 1.2469419240951538, 1.1671875715255737, 1.1387114524841309, 1.104141116142273, 1.0837262868881226, 1.081315279006958, 1.05073082447052, 1.033095121383667, 1.0277098417282104, 1.0354269742965698, 1.0246037244796753, 1.017720341682434, 1.01811945438385, 1.0171416997909546, 1.01715087890625]
valid_per
[80.70257299026797, 67.09105452606319, 54.36608452206373, 50.27996267164379, 43.28756165844554, 43.434208772163714, 39.948006932409015, 36.02852952939608, 35.17530995867218, 35.395280629249434, 34.628716171177174, 33.04226103186242, 32.50899880015998, 32.48900146647114, 31.84908678842821, 31.202506332489, 31.295827223036927, 31.409145447273694, 31.135848553526195, 30.429276096520464]
Training finished in 3.0 minutes.
Model saved to checkpoints/20231208_020142/model_19
Loading model from checkpoints/20231208_020142/model_19
SUB: 15.46%, DEL: 15.03%, INS: 1.95%, COR: 69.51%, PER: 32.43%
