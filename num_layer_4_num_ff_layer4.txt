Namespace(seed=123, train_json='/rds/user/yo279/hpc-work/MLMI2/fbanks/train_fbank.json', val_json='/rds/user/yo279/hpc-work/MLMI2/fbanks/dev_fbank.json', test_json='/rds/user/yo279/hpc-work/MLMI2/fbanks/test_fbank.json', batch_size=4, num_layers=4, fbank_dims=23, model_dims=128, concat=1, lr=0.5, vocab='vocab_39.txt', report_interval=50, num_epochs=20, dropout_rate=0.5, clip_max_norm=1, num_ff_layers=4)
Total number of model parameters is 1357664
EPOCH 1:
  batch 50 loss: 4.852089171409607
  batch 100 loss: 3.411813669204712
  batch 150 loss: 3.3985928106307983
  batch 200 loss: 3.38712251663208
  batch 250 loss: 3.390059804916382
  batch 300 loss: 3.356548318862915
  batch 350 loss: 3.3446583127975464
  batch 400 loss: 3.3399972677230836
  batch 450 loss: 3.340351691246033
  batch 500 loss: 3.3294787216186523
  batch 550 loss: 3.320876188278198
  batch 600 loss: 3.3192085790634156
  batch 650 loss: 3.3139941120147705
  batch 700 loss: 3.3285575580596922
  batch 750 loss: 3.3234644079208375
  batch 800 loss: 3.332443928718567
  batch 850 loss: 3.332667140960693
  batch 900 loss: 3.3058057022094727
LOSS train 3.30581 valid 3.33196, valid PER 100.00%
EPOCH 2:
  batch 50 loss: 3.309878945350647
  batch 100 loss: 3.302909827232361
  batch 150 loss: 3.302756500244141
  batch 200 loss: 3.3043414545059204
  batch 250 loss: 3.2817504024505615
  batch 300 loss: 3.270172414779663
  batch 350 loss: 3.2604600620269775
  batch 400 loss: 3.2560498046875
  batch 450 loss: 3.2200854444503784
  batch 500 loss: 3.1527978658676146
  batch 550 loss: 3.131484317779541
  batch 600 loss: 3.0155483150482176
  batch 650 loss: 3.0180259799957274
  batch 700 loss: 2.964925923347473
  batch 750 loss: 2.938271174430847
  batch 800 loss: 2.889453024864197
  batch 850 loss: 2.880702075958252
  batch 900 loss: 2.8778284883499143
LOSS train 2.87783 valid 2.82490, valid PER 80.76%
EPOCH 3:
  batch 50 loss: 2.845698204040527
  batch 100 loss: 2.7948523235321043
  batch 150 loss: 2.8031348419189452
  batch 200 loss: 2.819318914413452
  batch 250 loss: 2.7540531063079836
  batch 300 loss: 2.7629962730407716
  batch 350 loss: 2.842860264778137
  batch 400 loss: 2.7527243852615357
  batch 450 loss: 2.791960291862488
  batch 500 loss: 2.743209385871887
  batch 550 loss: 2.73785240650177
  batch 600 loss: 2.6837741327285767
  batch 650 loss: 2.6287644052505494
  batch 700 loss: 2.637858657836914
  batch 750 loss: 2.652662920951843
  batch 800 loss: 2.644423975944519
  batch 850 loss: 2.6136544227600096
  batch 900 loss: 2.590536503791809
LOSS train 2.59054 valid 2.51151, valid PER 77.14%
EPOCH 4:
  batch 50 loss: 2.577181348800659
  batch 100 loss: 2.5499469423294068
  batch 150 loss: 2.470088772773743
  batch 200 loss: 2.509669427871704
  batch 250 loss: 2.4827145528793335
  batch 300 loss: 2.5249848413467406
  batch 350 loss: 2.3951039028167727
  batch 400 loss: 2.50589156627655
  batch 450 loss: 2.4339640140533447
  batch 500 loss: 2.4072486686706545
  batch 550 loss: 2.3688214159011842
  batch 600 loss: 2.3849361753463745
  batch 650 loss: 2.44471296787262
  batch 700 loss: 2.360637731552124
  batch 750 loss: 2.3174297428131103
  batch 800 loss: 2.266923429965973
  batch 850 loss: 2.2387319827079772
  batch 900 loss: 2.342249240875244
LOSS train 2.34225 valid 2.16530, valid PER 74.52%
EPOCH 5:
  batch 50 loss: 2.2123423504829405
  batch 100 loss: 2.182790923118591
  batch 150 loss: 2.1803956031799316
  batch 200 loss: 2.1620231604576112
  batch 250 loss: 2.1540188932418824
  batch 300 loss: 2.1797821712493897
  batch 350 loss: 2.192207112312317
  batch 400 loss: 2.2721317267417906
  batch 450 loss: 2.1013898468017578
  batch 500 loss: 2.1122509837150574
  batch 550 loss: 2.052533197402954
  batch 600 loss: 2.0891810560226443
  batch 650 loss: 2.0062890791893007
  batch 700 loss: 2.0460166192054747
  batch 750 loss: 1.9967014122009277
  batch 800 loss: 2.0234527349472047
  batch 850 loss: 2.0056939244270326
  batch 900 loss: 1.9745092678070069
LOSS train 1.97451 valid 1.85616, valid PER 55.43%
EPOCH 6:
  batch 50 loss: 2.069252643585205
  batch 100 loss: 1.8755838584899902
  batch 150 loss: 1.9109803175926208
  batch 200 loss: 1.819019033908844
  batch 250 loss: 1.889275574684143
  batch 300 loss: 1.7860144591331482
  batch 350 loss: 1.8125237536430359
  batch 400 loss: 1.7815144205093383
  batch 450 loss: 1.820895357131958
  batch 500 loss: 1.778566482067108
  batch 550 loss: 1.7488039231300354
  batch 600 loss: 1.7446145939826965
  batch 650 loss: 1.6937033367156982
  batch 700 loss: 1.7189934635162354
  batch 750 loss: 1.6932940936088563
  batch 800 loss: 1.6877744746208192
  batch 850 loss: 1.7069274353981019
  batch 900 loss: 1.6841126847267152
LOSS train 1.68411 valid 1.60254, valid PER 44.60%
EPOCH 7:
  batch 50 loss: 1.6540698862075807
  batch 100 loss: 1.636165726184845
  batch 150 loss: 1.6839430594444276
  batch 200 loss: 1.6251331901550292
  batch 250 loss: 1.684365451335907
  batch 300 loss: 1.573376817703247
  batch 350 loss: 1.6178561806678773
  batch 400 loss: 1.5601604056358338
  batch 450 loss: 1.6018801951408386
  batch 500 loss: 1.639274342060089
  batch 550 loss: 1.5928543043136596
  batch 600 loss: 1.5579514050483703
  batch 650 loss: 1.5043330788612366
  batch 700 loss: 1.5285144758224487
  batch 750 loss: 1.5170667910575866
  batch 800 loss: 1.5000272822380065
  batch 850 loss: 1.5374658703804016
  batch 900 loss: 1.5371621775627136
LOSS train 1.53716 valid 1.42755, valid PER 39.20%
EPOCH 8:
  batch 50 loss: 1.4857951402664185
  batch 100 loss: 1.5025085234642028
  batch 150 loss: 1.484345633983612
  batch 200 loss: 1.3945919561386109
  batch 250 loss: 1.452976622581482
  batch 300 loss: 1.4451862740516663
  batch 350 loss: 1.5220392084121703
  batch 400 loss: 1.4851646947860717
  batch 450 loss: 1.5371072721481323
  batch 500 loss: 1.4645128750801086
  batch 550 loss: 1.3640216279029846
  batch 600 loss: 1.456345489025116
  batch 650 loss: 1.4363244795799255
  batch 700 loss: 1.3751309263706206
  batch 750 loss: 1.3434128856658936
  batch 800 loss: 1.3655884051322937
  batch 850 loss: 1.389255907535553
  batch 900 loss: 1.399469691514969
LOSS train 1.39947 valid 1.34334, valid PER 37.59%
EPOCH 9:
  batch 50 loss: 1.2915929353237152
  batch 100 loss: 1.4105289936065675
  batch 150 loss: 1.3275832104682923
  batch 200 loss: 1.2841090846061707
  batch 250 loss: 1.3344280707836151
  batch 300 loss: 1.3439228987693788
  batch 350 loss: 1.36111656665802
  batch 400 loss: 1.311846694946289
  batch 450 loss: 1.3242620420455933
  batch 500 loss: 1.2664347088336945
  batch 550 loss: 1.5902164769172669
  batch 600 loss: 1.598111071586609
  batch 650 loss: 1.46589501619339
  batch 700 loss: 1.4026332485675812
  batch 750 loss: 1.451821038722992
  batch 800 loss: 1.4253453540802001
  batch 850 loss: 1.4698155236244201
  batch 900 loss: 1.3211095988750459
LOSS train 1.32111 valid 1.38423, valid PER 38.46%
EPOCH 10:
  batch 50 loss: 1.3282605004310608
  batch 100 loss: 1.3086021900177003
  batch 150 loss: 1.3971654427051545
  batch 200 loss: 1.386808683872223
  batch 250 loss: 1.3458549928665162
  batch 300 loss: 1.2650116646289826
  batch 350 loss: 1.3203435492515565
  batch 400 loss: 1.3268839478492738
  batch 450 loss: 1.254487657546997
  batch 500 loss: 1.3060882520675658
  batch 550 loss: 1.3103586184978484
  batch 600 loss: 1.2978530287742616
  batch 650 loss: 1.3504188966751098
  batch 700 loss: 1.2998197281360626
  batch 750 loss: 1.2458636796474456
  batch 800 loss: 1.2764387810230255
  batch 850 loss: 1.4133035326004029
  batch 900 loss: 1.467570035457611
LOSS train 1.46757 valid 1.43175, valid PER 41.69%
EPOCH 11:
  batch 50 loss: 1.3426642966270448
  batch 100 loss: 1.4904987454414367
  batch 150 loss: 1.3237842404842377
  batch 200 loss: 1.3599737429618834
  batch 250 loss: 1.3795530319213867
  batch 300 loss: 1.2815877377986908
  batch 350 loss: 1.2643465304374695
  batch 400 loss: 1.4375623440742493
  batch 450 loss: 1.3958416676521301
  batch 500 loss: 1.3168838286399842
  batch 550 loss: 1.288339592218399
  batch 600 loss: 1.313406935930252
  batch 650 loss: 1.378097871541977
  batch 700 loss: 1.203510752916336
  batch 750 loss: 1.2702426433563232
  batch 800 loss: 1.3171215915679932
  batch 850 loss: 1.3104902338981628
  batch 900 loss: 1.289863736629486
LOSS train 1.28986 valid 1.29933, valid PER 37.24%
EPOCH 12:
  batch 50 loss: 1.269173938035965
  batch 100 loss: 1.2387926423549651
  batch 150 loss: 1.2245019459724427
  batch 200 loss: 1.2300826239585876
  batch 250 loss: 1.3607836604118346
  batch 300 loss: 1.2900920021533966
  batch 350 loss: 1.2974352550506592
  batch 400 loss: 1.2650225710868837
  batch 450 loss: 1.2978362488746642
  batch 500 loss: 1.3373726606369019
  batch 550 loss: 1.2160126018524169
  batch 600 loss: 1.3277192521095276
  batch 650 loss: 1.3055808162689209
  batch 700 loss: 1.333027811050415
  batch 750 loss: 1.1929388320446015
  batch 800 loss: 1.208138438463211
  batch 850 loss: 1.3504226994514466
  batch 900 loss: 1.338831079006195
LOSS train 1.33883 valid 1.23999, valid PER 35.41%
EPOCH 13:
  batch 50 loss: 1.2223977649211883
  batch 100 loss: 1.2626083624362945
  batch 150 loss: 1.146094092130661
  batch 200 loss: 1.1961586940288544
  batch 250 loss: 1.2312090337276458
  batch 300 loss: 1.1857149147987365
  batch 350 loss: 1.2614907217025757
  batch 400 loss: 1.2677042829990386
  batch 450 loss: 1.2372458636760713
  batch 500 loss: 1.2495961785316467
  batch 550 loss: 1.210540270805359
  batch 600 loss: 1.199327391386032
  batch 650 loss: 1.258083461523056
  batch 700 loss: 1.2064797353744507
  batch 750 loss: 1.1626218199729919
  batch 800 loss: 1.1928317987918853
  batch 850 loss: 1.292170788049698
  batch 900 loss: 1.2668622994422913
LOSS train 1.26686 valid 1.22037, valid PER 34.90%
EPOCH 14:
  batch 50 loss: 1.184424809217453
  batch 100 loss: 1.1728884732723237
  batch 150 loss: 1.1680349373817445
  batch 200 loss: 1.2049056661128998
  batch 250 loss: 1.1301384210586547
  batch 300 loss: 1.1834940838813781
  batch 350 loss: 1.265617380142212
  batch 400 loss: 1.2314370465278626
  batch 450 loss: 1.2702441644668578
  batch 500 loss: 1.211882302761078
  batch 550 loss: 1.204165232181549
  batch 600 loss: 1.36782533288002
  batch 650 loss: 1.3742025876045227
  batch 700 loss: 1.3076646411418915
  batch 750 loss: 1.390785344839096
  batch 800 loss: 1.369685368537903
  batch 850 loss: 1.3335858488082886
  batch 900 loss: 1.2469837808609008
LOSS train 1.24698 valid 1.24106, valid PER 36.03%
EPOCH 15:
  batch 50 loss: 1.3567782473564147
  batch 100 loss: 1.3257258474826812
  batch 150 loss: 1.2139059472084046
  batch 200 loss: 1.3534043562412261
  batch 250 loss: 1.39719731092453
  batch 300 loss: 1.362836444377899
  batch 350 loss: 1.3431508338451386
  batch 400 loss: 1.2563135933876037
  batch 450 loss: 1.2560204577445984
  batch 500 loss: 1.3143040370941161
  batch 550 loss: 1.3850453543663024
  batch 600 loss: 1.3552974164485931
  batch 650 loss: 1.429569582939148
  batch 700 loss: 1.3481445634365081
  batch 750 loss: 1.277197380065918
  batch 800 loss: 1.2289386677742005
  batch 850 loss: 1.3304216623306275
  batch 900 loss: 1.5725934720039367
LOSS train 1.57259 valid 1.47156, valid PER 42.15%
EPOCH 16:
  batch 50 loss: 1.6779889702796935
  batch 100 loss: 1.4718951344490052
  batch 150 loss: 1.4834583830833434
  batch 200 loss: 1.3179967248439788
  batch 250 loss: 1.37810542345047
  batch 300 loss: 1.4068086576461791
  batch 350 loss: 1.4669277739524842
  batch 400 loss: 1.3307393670082093
  batch 450 loss: 1.3270150542259216
  batch 500 loss: 1.2915572810173035
  batch 550 loss: 1.3353581273555755
  batch 600 loss: 1.2814058256149292
  batch 650 loss: 1.2584985613822937
  batch 700 loss: 1.261192160844803
  batch 750 loss: 1.2763350796699524
  batch 800 loss: 1.3521667790412903
  batch 850 loss: 1.249792503118515
  batch 900 loss: 1.3359729075431823
LOSS train 1.33597 valid 1.43273, valid PER 37.87%
EPOCH 17:
  batch 50 loss: 1.2968165755271912
  batch 100 loss: 1.3474579739570618
  batch 150 loss: 1.2536670207977294
  batch 200 loss: 1.2781398749351502
  batch 250 loss: 1.3079046177864075
  batch 300 loss: 1.3182632076740264
  batch 350 loss: 1.2188980352878571
  batch 400 loss: 1.2573091208934783
  batch 450 loss: 1.2265652990341187
  batch 500 loss: 1.1818233859539031
  batch 550 loss: 1.1889931225776673
  batch 600 loss: 1.2540826284885407
  batch 650 loss: 1.2554915368556976
  batch 700 loss: 1.35206120967865
  batch 750 loss: 1.3391611456871033
  batch 800 loss: 1.3288656735420228
  batch 850 loss: 1.2873739206790924
  batch 900 loss: 1.2533134496212006
LOSS train 1.25331 valid 1.30338, valid PER 37.55%
EPOCH 18:
  batch 50 loss: 1.290017992258072
  batch 100 loss: 1.3593097114562989
  batch 150 loss: 1.2769478857517242
  batch 200 loss: 1.2393429362773896
  batch 250 loss: 1.2048959374427795
  batch 300 loss: 1.2122407484054565
  batch 350 loss: 1.5026840472221374
  batch 400 loss: 1.3407837235927582
  batch 450 loss: 1.2629518473148347
  batch 500 loss: 1.434284279346466
  batch 550 loss: 1.2528964483737945
  batch 600 loss: 1.2463406372070311
  batch 650 loss: 1.7304277157783508
  batch 700 loss: 1.7512181997299194
  batch 750 loss: 1.4945229291915894
  batch 800 loss: 1.4933088541030883
  batch 850 loss: 1.4174124991893768
  batch 900 loss: 1.331438283920288
LOSS train 1.33144 valid 1.35063, valid PER 39.44%
EPOCH 19:
  batch 50 loss: 1.2211270976066588
  batch 100 loss: 1.3991825008392333
  batch 150 loss: 1.4213457798957825
  batch 200 loss: 1.3635593247413635
  batch 250 loss: 1.2911691892147064
  batch 300 loss: 1.2940448474884034
  batch 350 loss: 1.3041849637031555
  batch 400 loss: 1.2486423909664155
  batch 450 loss: 1.2163188588619231
  batch 500 loss: 1.233773286342621
  batch 550 loss: 1.275094827413559
  batch 600 loss: 1.2313684141635894
  batch 650 loss: 1.336622920036316
  batch 700 loss: 1.2481015050411224
  batch 750 loss: 1.1981061255931855
  batch 800 loss: 1.6459277486801147
  batch 850 loss: 1.5982866382598877
  batch 900 loss: 1.6120073843002318
LOSS train 1.61201 valid 1.72082, valid PER 46.43%
EPOCH 20:
  batch 50 loss: 1.4783072781562805
  batch 100 loss: 1.35592631816864
  batch 150 loss: 1.5454417204856872
  batch 200 loss: 1.4897004437446595
  batch 250 loss: 1.4226725816726684
  batch 300 loss: nan
  batch 350 loss: nan
  batch 400 loss: nan
  batch 450 loss: nan
  batch 500 loss: nan
  batch 550 loss: nan
  batch 600 loss: nan
  batch 650 loss: nan
  batch 700 loss: nan
  batch 750 loss: nan
  batch 800 loss: nan
  batch 850 loss: nan
  batch 900 loss: nan
LOSS train nan valid nan, valid PER 100.00%
train_loss
[3.3058057022094727, 2.8778284883499143, 2.590536503791809, 2.342249240875244, 1.9745092678070069, 1.6841126847267152, 1.5371621775627136, 1.399469691514969, 1.3211095988750459, 1.467570035457611, 1.289863736629486, 1.338831079006195, 1.2668622994422913, 1.2469837808609008, 1.5725934720039367, 1.3359729075431823, 1.2533134496212006, 1.331438283920288, 1.6120073843002318, nan]
valid_loss
[3.331958293914795, 2.8248958587646484, 2.5115065574645996, 2.1652963161468506, 1.8561581373214722, 1.6025381088256836, 1.427549958229065, 1.34334135055542, 1.384226679801941, 1.4317519664764404, 1.2993268966674805, 1.2399885654449463, 1.2203658819198608, 1.2410588264465332, 1.4715559482574463, 1.4327327013015747, 1.3033778667449951, 1.3506345748901367, 1.7208184003829956, nan]
valid_per
[100.0, 80.75589921343821, 77.1363818157579, 74.51673110251966, 55.432608985468605, 44.6007199040128, 39.2014398080256, 37.594987335022, 38.46153846153847, 41.6944407412345, 37.24170110651913, 35.408612185041996, 34.902013064924674, 36.02852952939608, 42.15437941607786, 37.8682842287695, 37.55499266764431, 39.44140781229169, 46.43380882548993, 100.0]
Training finished in 7.0 minutes.
Model saved to checkpoints/20231208_131934/model_13
Loading model from checkpoints/20231208_131934/model_13
SUB: 22.47%, DEL: 11.26%, INS: 2.21%, COR: 66.27%, PER: 35.94%
