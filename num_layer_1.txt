Namespace(seed=123, train_json='/rds/user/yo279/hpc-work/MLMI2/fbanks/train_fbank.json', val_json='/rds/user/yo279/hpc-work/MLMI2/fbanks/dev_fbank.json', test_json='/rds/user/yo279/hpc-work/MLMI2/fbanks/test_fbank.json', batch_size=4, num_layers=1, fbank_dims=23, model_dims=128, concat=1, lr=0.5, vocab='vocab_39.txt', report_interval=50, num_epochs=20, dropout_rate=0.5, clip_max_norm=1)
Total number of model parameters is 166952
EPOCH 1:
  batch 50 loss: 4.078928780555725
  batch 100 loss: 3.1571432161331177
  batch 150 loss: 3.02589638710022
  batch 200 loss: 2.907837414741516
  batch 250 loss: 2.832811484336853
  batch 300 loss: 2.7009062147140503
  batch 350 loss: 2.5174450159072874
  batch 400 loss: 2.427889494895935
  batch 450 loss: 2.390459246635437
  batch 500 loss: 2.258040487766266
  batch 550 loss: 2.1373790097236633
  batch 600 loss: 2.0801124691963198
  batch 650 loss: 1.9672044849395751
  batch 700 loss: 1.9773194456100465
  batch 750 loss: 1.9003113389015198
  batch 800 loss: 1.8803345847129822
  batch 850 loss: 1.8083264303207398
  batch 900 loss: 1.8000157213211059
LOSS train 1.80002 valid 1.80027, valid PER 68.31%
EPOCH 2:
  batch 50 loss: 1.7461361408233642
  batch 100 loss: 1.6996016025543212
  batch 150 loss: 1.6521826004981994
  batch 200 loss: 1.6697696352005005
  batch 250 loss: 1.6790146660804748
  batch 300 loss: 1.6259354972839355
  batch 350 loss: 1.5373647475242616
  batch 400 loss: 1.5470023727416993
  batch 450 loss: 1.4850608015060425
  batch 500 loss: 1.5268820428848267
  batch 550 loss: 1.5324129414558412
  batch 600 loss: 1.459086935520172
  batch 650 loss: 1.4708670663833618
  batch 700 loss: 1.4600662326812743
  batch 750 loss: 1.4309776377677919
  batch 800 loss: 1.3584832525253296
  batch 850 loss: 1.3853207921981812
  batch 900 loss: 1.4001509284973144
LOSS train 1.40015 valid 1.36777, valid PER 42.05%
EPOCH 3:
  batch 50 loss: 1.335355064868927
  batch 100 loss: 1.3166246962547303
  batch 150 loss: 1.3400622630119323
  batch 200 loss: 1.302341365814209
  batch 250 loss: 1.3057787454128265
  batch 300 loss: 1.2746892714500426
  batch 350 loss: 1.3122436094284058
  batch 400 loss: 1.3011831116676331
  batch 450 loss: 1.253146104812622
  batch 500 loss: 1.2427975857257842
  batch 550 loss: 1.2661859595775604
  batch 600 loss: 1.2410074651241303
  batch 650 loss: 1.1903074741363526
  batch 700 loss: 1.23639097571373
  batch 750 loss: 1.2909431099891662
  batch 800 loss: 1.1983576357364654
  batch 850 loss: 1.2412506985664367
  batch 900 loss: 1.1646179115772248
LOSS train 1.16462 valid 1.23198, valid PER 37.14%
EPOCH 4:
  batch 50 loss: 1.1531485056877135
  batch 100 loss: 1.1926070439815522
  batch 150 loss: 1.1552437245845795
  batch 200 loss: 1.172966729402542
  batch 250 loss: 1.1801854193210601
  batch 300 loss: 1.19753551363945
  batch 350 loss: 1.1073894417285919
  batch 400 loss: 1.1649235129356383
  batch 450 loss: 1.1455443811416626
  batch 500 loss: 1.131409202814102
  batch 550 loss: 1.155888376235962
  batch 600 loss: 1.175143312215805
  batch 650 loss: 1.1405511486530304
  batch 700 loss: 1.1155306100845337
  batch 750 loss: 1.107320386171341
  batch 800 loss: 1.1111479163169862
  batch 850 loss: 1.1415220201015472
  batch 900 loss: 1.145662716627121
LOSS train 1.14566 valid 1.17930, valid PER 36.55%
EPOCH 5:
  batch 50 loss: 1.0526750075817108
  batch 100 loss: 1.071811089515686
  batch 150 loss: 1.1158796262741089
  batch 200 loss: 1.031274744272232
  batch 250 loss: 1.046193492412567
  batch 300 loss: 1.0742626988887787
  batch 350 loss: 1.0795404279232026
  batch 400 loss: 1.073547921180725
  batch 450 loss: 1.0945503425598144
  batch 500 loss: 1.0697249341011048
  batch 550 loss: 1.023714430332184
  batch 600 loss: 1.1050165712833404
  batch 650 loss: 1.0418322694301605
  batch 700 loss: 1.0880313873291017
  batch 750 loss: 1.033298250436783
  batch 800 loss: 1.0529052364826201
  batch 850 loss: 1.0511733829975127
  batch 900 loss: 1.08156134724617
LOSS train 1.08156 valid 1.09486, valid PER 34.07%
EPOCH 6:
  batch 50 loss: 1.041234359741211
  batch 100 loss: 1.0075744080543518
  batch 150 loss: 0.9774200010299683
  batch 200 loss: 1.013598598241806
  batch 250 loss: 1.0475275564193725
  batch 300 loss: 1.0028083097934724
  batch 350 loss: 1.000753458738327
  batch 400 loss: 1.0194376599788666
  batch 450 loss: 1.0544097197055817
  batch 500 loss: 1.0220077264308929
  batch 550 loss: 1.035830498933792
  batch 600 loss: 0.9834941077232361
  batch 650 loss: 0.9970926070213317
  batch 700 loss: 1.0104390513896941
  batch 750 loss: 0.9935519587993622
  batch 800 loss: 0.9941175174713135
  batch 850 loss: 0.9808765435218811
  batch 900 loss: 1.000772225856781
LOSS train 1.00077 valid 1.07305, valid PER 33.67%
EPOCH 7:
  batch 50 loss: 0.9688959884643554
  batch 100 loss: 0.9742759954929352
  batch 150 loss: 0.9730541515350342
  batch 200 loss: 0.9503716444969177
  batch 250 loss: 0.9462680971622467
  batch 300 loss: 0.9497736370563508
  batch 350 loss: 0.957768383026123
  batch 400 loss: 1.0061142456531524
  batch 450 loss: 0.9996844947338104
  batch 500 loss: 0.9729706454277038
  batch 550 loss: 0.9528610372543335
  batch 600 loss: 0.9523961508274078
  batch 650 loss: 0.9893720090389252
  batch 700 loss: 0.9877304589748382
  batch 750 loss: 0.9781407010555268
  batch 800 loss: 0.9774384093284607
  batch 850 loss: 0.9604511451721192
  batch 900 loss: 1.0289361047744752
LOSS train 1.02894 valid 1.04010, valid PER 33.05%
EPOCH 8:
  batch 50 loss: 0.9228782689571381
  batch 100 loss: 0.9218980300426484
  batch 150 loss: 0.9299257719516754
  batch 200 loss: 0.90312908411026
  batch 250 loss: 0.9327744376659394
  batch 300 loss: 0.8834408521652222
  batch 350 loss: 0.9411579775810242
  batch 400 loss: 0.9105958592891693
  batch 450 loss: 0.9268015241622924
  batch 500 loss: 0.9689588356018066
  batch 550 loss: 0.8805585277080535
  batch 600 loss: 0.9265714323520661
  batch 650 loss: 0.9626624727249146
  batch 700 loss: 0.9136964344978332
  batch 750 loss: 0.9297209823131561
  batch 800 loss: 0.9251457500457764
  batch 850 loss: 0.919028742313385
  batch 900 loss: 0.9258223032951355
LOSS train 0.92582 valid 1.02419, valid PER 30.90%
EPOCH 9:
  batch 50 loss: 0.8386130881309509
  batch 100 loss: 0.8860707783699036
  batch 150 loss: 0.8973442697525025
  batch 200 loss: 0.8625238442420959
  batch 250 loss: 0.9025414597988128
  batch 300 loss: 0.8830954921245575
  batch 350 loss: 0.9110648262500763
  batch 400 loss: 0.8950471401214599
  batch 450 loss: 0.8981078064441681
  batch 500 loss: 0.8589798557758331
  batch 550 loss: 0.9197928297519684
  batch 600 loss: 0.9535148322582245
  batch 650 loss: 0.9200303876399993
  batch 700 loss: 0.8800368762016296
  batch 750 loss: 0.8875165724754334
  batch 800 loss: 0.912989581823349
  batch 850 loss: 0.92125652551651
  batch 900 loss: 0.8635931217670441
LOSS train 0.86359 valid 1.05761, valid PER 31.69%
EPOCH 10:
  batch 50 loss: 0.8298098409175872
  batch 100 loss: 0.8750194036960601
  batch 150 loss: 0.9706913805007935
  batch 200 loss: 0.9061953759193421
  batch 250 loss: 0.8978099822998047
  batch 300 loss: 0.8343100452423096
  batch 350 loss: 0.8917968010902405
  batch 400 loss: 0.858060530424118
  batch 450 loss: 0.8365367341041565
  batch 500 loss: 0.9022400295734405
  batch 550 loss: 0.9084506940841675
  batch 600 loss: 0.9016596364974976
  batch 650 loss: 0.9098196971416473
  batch 700 loss: 0.8985945105552673
  batch 750 loss: 0.8721035242080688
  batch 800 loss: 0.8859128499031067
  batch 850 loss: 0.9123711204528808
  batch 900 loss: 0.8750647783279419
LOSS train 0.87506 valid 1.00483, valid PER 31.90%
EPOCH 11:
  batch 50 loss: 0.8073631167411804
  batch 100 loss: 0.7849175322055817
  batch 150 loss: 0.8112511825561524
  batch 200 loss: 0.8560082960128784
  batch 250 loss: 0.8541319978237152
  batch 300 loss: 0.8144169485569
  batch 350 loss: 0.857241644859314
  batch 400 loss: 0.8821437537670136
  batch 450 loss: 0.8422883713245392
  batch 500 loss: 0.8323374688625336
  batch 550 loss: 0.8532572185993195
  batch 600 loss: 0.8309958636760711
  batch 650 loss: 0.8915321040153503
  batch 700 loss: 0.8291303324699402
  batch 750 loss: 0.8701611816883087
  batch 800 loss: 0.9176913130283356
  batch 850 loss: 0.923404085636139
  batch 900 loss: 0.8940431320667267
LOSS train 0.89404 valid 0.99565, valid PER 30.80%
EPOCH 12:
  batch 50 loss: 0.8324958097934723
  batch 100 loss: 0.8029695785045624
  batch 150 loss: 0.8160936856269836
  batch 200 loss: 0.8414301574230194
  batch 250 loss: 0.8366801035404205
  batch 300 loss: 0.8293534398078919
  batch 350 loss: 0.8194085097312928
  batch 400 loss: 0.8523425853252411
  batch 450 loss: 0.8514450550079345
  batch 500 loss: 0.8493095552921295
  batch 550 loss: 0.7704569375514985
  batch 600 loss: 0.7952001249790192
  batch 650 loss: 0.85777752161026
  batch 700 loss: 0.8598796844482421
  batch 750 loss: 0.8162034106254578
  batch 800 loss: 0.8194433772563934
  batch 850 loss: 0.8406333637237549
  batch 900 loss: 0.8793147778511048
LOSS train 0.87931 valid 0.97113, valid PER 30.10%
EPOCH 13:
  batch 50 loss: 0.8025980949401855
  batch 100 loss: 0.8104220271110535
  batch 150 loss: 0.7684708070755005
  batch 200 loss: 0.8012838661670685
  batch 250 loss: 0.7957060539722443
  batch 300 loss: 0.8078994226455688
  batch 350 loss: 0.8087070715427399
  batch 400 loss: 0.8097772145271301
  batch 450 loss: 0.8312335801124573
  batch 500 loss: 0.7997902739048004
  batch 550 loss: 0.8284611892700195
  batch 600 loss: 0.8170517599582672
  batch 650 loss: 0.8223946243524551
  batch 700 loss: 0.823383218050003
  batch 750 loss: 0.7787216329574584
  batch 800 loss: 0.8301616775989532
  batch 850 loss: 0.8608654499053955
  batch 900 loss: 0.8637798988819122
LOSS train 0.86378 valid 0.99472, valid PER 30.76%
EPOCH 14:
  batch 50 loss: 0.7957300627231598
  batch 100 loss: 0.7962584793567657
  batch 150 loss: 0.8029203367233276
  batch 200 loss: 0.7837496316432953
  batch 250 loss: 0.8114576637744904
  batch 300 loss: 0.8357602870464325
  batch 350 loss: 0.7696683073043823
  batch 400 loss: 0.7795760881900787
  batch 450 loss: 0.802565883398056
  batch 500 loss: 0.7922147119045257
  batch 550 loss: 0.8219600820541382
  batch 600 loss: 0.7967938470840454
  batch 650 loss: 0.8139158380031586
  batch 700 loss: 0.8123358857631683
  batch 750 loss: 0.7971849775314331
  batch 800 loss: 0.7883571600914001
  batch 850 loss: 0.8464317488670349
  batch 900 loss: 0.8277454972267151
LOSS train 0.82775 valid 1.03619, valid PER 31.55%
EPOCH 15:
  batch 50 loss: 0.7991026705503463
  batch 100 loss: 0.765213029384613
  batch 150 loss: 0.7903622579574585
  batch 200 loss: 0.8200011724233627
  batch 250 loss: 0.796098381280899
  batch 300 loss: 0.7860035449266434
  batch 350 loss: 0.7835760569572449
  batch 400 loss: 0.7791335880756378
  batch 450 loss: 0.7643576765060425
  batch 500 loss: 0.747452642917633
  batch 550 loss: 0.7910892546176911
  batch 600 loss: 0.8058530473709107
  batch 650 loss: 0.8114793539047241
  batch 700 loss: 0.8248085951805115
  batch 750 loss: 0.8059092330932617
  batch 800 loss: 0.8154887557029724
  batch 850 loss: 0.7691918611526489
  batch 900 loss: 0.8150466048717498
LOSS train 0.81505 valid 1.01003, valid PER 30.64%
EPOCH 16:
  batch 50 loss: 0.7697056567668915
  batch 100 loss: 0.7583149659633637
  batch 150 loss: 0.7726322770118713
  batch 200 loss: 0.7458835566043853
  batch 250 loss: 0.7840693771839142
  batch 300 loss: 0.7702980291843414
  batch 350 loss: 0.7753955435752868
  batch 400 loss: 0.781859667301178
  batch 450 loss: 0.7869922661781311
  batch 500 loss: 0.7198276436328888
  batch 550 loss: 0.7527521294355393
  batch 600 loss: 0.7558423113822937
  batch 650 loss: 0.7727050614356995
  batch 700 loss: 0.7399853992462159
  batch 750 loss: 0.749896788597107
  batch 800 loss: 0.7869238150119782
  batch 850 loss: 0.7534290605783462
  batch 900 loss: 0.7520830202102661
LOSS train 0.75208 valid 0.93774, valid PER 28.52%
EPOCH 17:
  batch 50 loss: 0.7188870841264725
  batch 100 loss: 0.7169371390342713
  batch 150 loss: 0.7195505982637406
  batch 200 loss: 0.7274825823307037
  batch 250 loss: 0.7555895245075226
  batch 300 loss: 0.743593230843544
  batch 350 loss: 0.7046132808923722
  batch 400 loss: 0.7773491215705871
  batch 450 loss: 0.772350857257843
  batch 500 loss: 0.7433628809452056
  batch 550 loss: 0.7727356159687042
  batch 600 loss: 0.7838226187229157
  batch 650 loss: 0.7563624334335327
  batch 700 loss: 0.8027396285533905
  batch 750 loss: 0.7408907824754715
  batch 800 loss: 0.7662216275930405
  batch 850 loss: 0.7858911460638046
  batch 900 loss: 0.7528485834598542
LOSS train 0.75285 valid 0.96446, valid PER 29.26%
EPOCH 18:
  batch 50 loss: 0.7149949496984482
  batch 100 loss: 0.7467050218582153
  batch 150 loss: 0.7317120307683944
  batch 200 loss: 0.7391747897863388
  batch 250 loss: 0.7623856866359711
  batch 300 loss: 0.7300077837705612
  batch 350 loss: 0.7505406296253204
  batch 400 loss: 0.7428159105777741
  batch 450 loss: 0.8075817823410034
  batch 500 loss: 0.7469830948114395
  batch 550 loss: 0.7564898073673249
  batch 600 loss: 0.7544945406913758
  batch 650 loss: 0.7462536150217056
  batch 700 loss: 0.7843079721927643
  batch 750 loss: 0.7418034672737122
  batch 800 loss: 0.7399572813510895
  batch 850 loss: 0.765421599149704
  batch 900 loss: 0.7743175029754639
LOSS train 0.77432 valid 0.95552, valid PER 29.40%
EPOCH 19:
  batch 50 loss: 0.6835941278934479
  batch 100 loss: 0.687827370762825
  batch 150 loss: 0.706929839849472
  batch 200 loss: 0.7303540849685669
  batch 250 loss: 0.7627183544635773
  batch 300 loss: 0.7363194906711579
  batch 350 loss: 0.7305480992794037
  batch 400 loss: 0.7277819740772248
  batch 450 loss: 0.7269946265220643
  batch 500 loss: 0.7592112624645233
  batch 550 loss: 0.7295803618431091
  batch 600 loss: 0.7338502728939056
  batch 650 loss: 0.7975021553039551
  batch 700 loss: 0.7145097148418427
  batch 750 loss: 0.7078829044103623
  batch 800 loss: 0.7347451674938202
  batch 850 loss: 0.7369064569473267
  batch 900 loss: 0.7426369261741638
LOSS train 0.74264 valid 0.97770, valid PER 29.78%
EPOCH 20:
  batch 50 loss: 0.6857762050628662
  batch 100 loss: 0.6992996823787689
  batch 150 loss: 0.6918070238828659
  batch 200 loss: 0.6805204498767853
  batch 250 loss: 0.7107688617706299
  batch 300 loss: 0.7085709768533707
  batch 350 loss: 0.6912057846784592
  batch 400 loss: 0.7132821273803711
  batch 450 loss: 0.7101038259267807
  batch 500 loss: 0.6918882668018341
  batch 550 loss: 0.7927059066295624
  batch 600 loss: 0.773023648262024
  batch 650 loss: 0.7636537849903107
  batch 700 loss: 0.7675106555223465
  batch 750 loss: 0.7212189698219299
  batch 800 loss: 0.7513635385036469
  batch 850 loss: 0.73831318795681
  batch 900 loss: 0.7172343957424164
LOSS train 0.71723 valid 0.96876, valid PER 29.60%
train_loss
[1.8000157213211059, 1.4001509284973144, 1.1646179115772248, 1.145662716627121, 1.08156134724617, 1.000772225856781, 1.0289361047744752, 0.9258223032951355, 0.8635931217670441, 0.8750647783279419, 0.8940431320667267, 0.8793147778511048, 0.8637798988819122, 0.8277454972267151, 0.8150466048717498, 0.7520830202102661, 0.7528485834598542, 0.7743175029754639, 0.7426369261741638, 0.7172343957424164]
valid_loss
[1.8002674579620361, 1.3677679300308228, 1.2319806814193726, 1.179301142692566, 1.0948618650436401, 1.0730531215667725, 1.040101408958435, 1.0241938829421997, 1.0576063394546509, 1.0048253536224365, 0.9956539869308472, 0.9711345434188843, 0.9947211146354675, 1.0361888408660889, 1.010032296180725, 0.9377370476722717, 0.9644569158554077, 0.9555196166038513, 0.9776954650878906, 0.9687628149986267]
valid_per
[68.31089188108253, 42.05439274763365, 37.14171443807493, 36.54846020530596, 34.068790827889615, 33.66884415411279, 33.0489268097587, 30.895880549260095, 31.689108118917474, 31.90241301159845, 30.80255965871217, 30.095987201706436, 30.75589921343821, 31.549126783095588, 30.64258098920144, 28.51619784028796, 29.256099186775096, 29.402746300493266, 29.78269564058126, 29.602719637381682]
Training finished in 5.0 minutes.
Model saved to checkpoints/20231207_231859/model_16
Loading model from checkpoints/20231207_231859/model_16
SUB: 16.97%, DEL: 10.96%, INS: 2.34%, COR: 72.07%, PER: 30.27%
