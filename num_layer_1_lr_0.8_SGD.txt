Namespace(seed=123, train_json='/rds/user/yo279/hpc-work/MLMI2/fbanks/train_fbank.json', val_json='/rds/user/yo279/hpc-work/MLMI2/fbanks/dev_fbank.json', test_json='/rds/user/yo279/hpc-work/MLMI2/fbanks/test_fbank.json', batch_size=4, num_layers=1, fbank_dims=23, model_dims=128, concat=1, lr=0.8, vocab='vocab_39.txt', report_interval=50, num_epochs=20, dropout_rate=0.5, clip_max_norm=1)
Total number of model parameters is 166952
EPOCH 1:
  batch 50 loss: 4.584357452392578
  batch 100 loss: 2.964671001434326
  batch 150 loss: 2.872403631210327
  batch 200 loss: 2.6540418672561645
  batch 250 loss: 2.5788005113601686
  batch 300 loss: 2.285557360649109
  batch 350 loss: 2.1834653711318968
  batch 400 loss: 2.144861350059509
  batch 450 loss: 2.066385045051575
  batch 500 loss: 1.9427112102508546
  batch 550 loss: 1.9021136212348937
  batch 600 loss: 1.8611576986312866
  batch 650 loss: 1.75233238697052
  batch 700 loss: 1.7505450439453125
  batch 750 loss: 1.7215023303031922
  batch 800 loss: 1.700779218673706
  batch 850 loss: 1.6657933926582336
  batch 900 loss: 1.6362880849838257
LOSS train 1.63629 valid 1.66159, valid PER 65.06%
EPOCH 2:
  batch 50 loss: 1.6207122898101807
  batch 100 loss: 1.5370155763626099
  batch 150 loss: 1.5607415175437926
  batch 200 loss: 1.5457341504096984
  batch 250 loss: 1.5799902057647706
  batch 300 loss: 1.5240252614021301
  batch 350 loss: 1.4204740262031554
  batch 400 loss: 1.47940034866333
  batch 450 loss: 1.404519453048706
  batch 500 loss: 1.4595092487335206
  batch 550 loss: 1.4519806480407715
  batch 600 loss: 1.403410677909851
  batch 650 loss: 1.4280972814559936
  batch 700 loss: 1.4085534071922303
  batch 750 loss: 1.397579083442688
  batch 800 loss: 1.3183655786514281
  batch 850 loss: 1.331760436296463
  batch 900 loss: 1.35434818983078
LOSS train 1.35435 valid 1.33183, valid PER 43.01%
EPOCH 3:
  batch 50 loss: 1.2983554553985597
  batch 100 loss: 1.281640112400055
  batch 150 loss: 1.268260908126831
  batch 200 loss: 1.2443847692012786
  batch 250 loss: 1.2529272198677064
  batch 300 loss: 1.2380165302753448
  batch 350 loss: 1.288293969631195
  batch 400 loss: 1.2800705695152284
  batch 450 loss: 1.2320965206623078
  batch 500 loss: 1.2198984277248384
  batch 550 loss: 1.2333187925815583
  batch 600 loss: 1.18935485124588
  batch 650 loss: 1.178033585548401
  batch 700 loss: 1.1925860226154328
  batch 750 loss: 1.2677553868293763
  batch 800 loss: 1.1739873898029327
  batch 850 loss: 1.2104454350471496
  batch 900 loss: 1.1474824631214142
LOSS train 1.14748 valid 1.26461, valid PER 37.51%
EPOCH 4:
  batch 50 loss: 1.1248157584667207
  batch 100 loss: 1.1618747317790985
  batch 150 loss: 1.1244649839401246
  batch 200 loss: 1.1644172763824463
  batch 250 loss: 1.129455795288086
  batch 300 loss: 1.1674882519245147
  batch 350 loss: 1.068456265926361
  batch 400 loss: 1.1195025873184203
  batch 450 loss: 1.1104866445064545
  batch 500 loss: 1.0897413468360901
  batch 550 loss: 1.1357505214214325
  batch 600 loss: 1.1556905317306518
  batch 650 loss: 1.148437521457672
  batch 700 loss: 1.0965386366844176
  batch 750 loss: 1.079820601940155
  batch 800 loss: 1.0423399579524995
  batch 850 loss: 1.094922387599945
  batch 900 loss: 1.159674698114395
LOSS train 1.15967 valid 1.13190, valid PER 35.26%
EPOCH 5:
  batch 50 loss: 1.042068202495575
  batch 100 loss: 1.0370719146728515
  batch 150 loss: 1.1003933131694794
  batch 200 loss: 1.0375370657444
  batch 250 loss: 1.026121941804886
  batch 300 loss: 1.0269199693202973
  batch 350 loss: 1.0280599999427795
  batch 400 loss: 1.0560560190677644
  batch 450 loss: 1.041178492307663
  batch 500 loss: 1.0401976799964905
  batch 550 loss: 0.995947711467743
  batch 600 loss: 1.0730918002128602
  batch 650 loss: 1.026925585269928
  batch 700 loss: 1.0638410246372223
  batch 750 loss: 1.0019616663455964
  batch 800 loss: 1.0556356811523437
  batch 850 loss: 1.0458523654937744
  batch 900 loss: 1.0413186192512511
LOSS train 1.04132 valid 1.07430, valid PER 33.07%
EPOCH 6:
  batch 50 loss: 1.0152744436264038
  batch 100 loss: 0.949992047548294
  batch 150 loss: 0.9573516643047333
  batch 200 loss: 0.9646149408817292
  batch 250 loss: 1.030664759874344
  batch 300 loss: 1.0058424639701844
  batch 350 loss: 0.9538900423049926
  batch 400 loss: 0.9818586802482605
  batch 450 loss: 1.0133194994926453
  batch 500 loss: 0.9807150828838348
  batch 550 loss: 1.0084046030044556
  batch 600 loss: 0.9748485541343689
  batch 650 loss: 0.992300671339035
  batch 700 loss: 0.9755767500400543
  batch 750 loss: 0.9649019575119019
  batch 800 loss: 0.9616908311843873
  batch 850 loss: 0.9497079741954804
  batch 900 loss: 0.9862994539737702
LOSS train 0.98630 valid 1.06971, valid PER 32.88%
EPOCH 7:
  batch 50 loss: 0.948747581243515
  batch 100 loss: 0.933139649629593
  batch 150 loss: 0.9300390911102295
  batch 200 loss: 0.9167189431190491
  batch 250 loss: 0.9218736505508422
  batch 300 loss: 0.9016577768325805
  batch 350 loss: 0.9414773297309875
  batch 400 loss: 0.9418083798885345
  batch 450 loss: 0.9309270739555359
  batch 500 loss: 0.9214325249195099
  batch 550 loss: 0.916988924741745
  batch 600 loss: 0.9498940563201904
  batch 650 loss: 0.9315078043937683
  batch 700 loss: 0.9421480214595794
  batch 750 loss: 0.9226215136051178
  batch 800 loss: 0.9299639463424683
  batch 850 loss: 0.9348070764541626
  batch 900 loss: 0.9775160491466522
LOSS train 0.97752 valid 1.02777, valid PER 32.86%
EPOCH 8:
  batch 50 loss: 0.8817931044101716
  batch 100 loss: 0.8664371955394745
  batch 150 loss: 0.8806293392181397
  batch 200 loss: 0.8689475309848785
  batch 250 loss: 0.8820737087726593
  batch 300 loss: 0.8434313356876373
  batch 350 loss: 0.9184489285945893
  batch 400 loss: 0.8716228246688843
  batch 450 loss: 0.8990049207210541
  batch 500 loss: 0.9121955764293671
  batch 550 loss: 0.8515534245967865
  batch 600 loss: 0.9151177096366883
  batch 650 loss: 0.9227939641475678
  batch 700 loss: 0.8890131711959839
  batch 750 loss: 0.872558935880661
  batch 800 loss: 0.9209158170223236
  batch 850 loss: 0.9347609138488769
  batch 900 loss: 0.9434805965423584
LOSS train 0.94348 valid 1.02124, valid PER 30.68%
EPOCH 9:
  batch 50 loss: 0.7952164387702942
  batch 100 loss: 0.8449086487293244
  batch 150 loss: 0.8421923363208771
  batch 200 loss: 0.8232517337799072
  batch 250 loss: 0.8500956690311432
  batch 300 loss: 0.8785106432437897
  batch 350 loss: 0.895110512971878
  batch 400 loss: 0.8436919474601745
  batch 450 loss: 0.8625141680240631
  batch 500 loss: 0.8528066337108612
  batch 550 loss: 0.8831116437911988
  batch 600 loss: 0.8813115000724793
  batch 650 loss: 0.8605013322830201
  batch 700 loss: 0.839594475030899
  batch 750 loss: 0.862438496351242
  batch 800 loss: 0.8728043401241302
  batch 850 loss: 0.8894733917713166
  batch 900 loss: 0.8661954748630524
LOSS train 0.86620 valid 1.00169, valid PER 30.70%
EPOCH 10:
  batch 50 loss: 0.7965256309509278
  batch 100 loss: 0.7957814812660218
  batch 150 loss: 0.8131832230091095
  batch 200 loss: 0.844190138578415
  batch 250 loss: 0.8361488580703735
  batch 300 loss: 0.8000321996212005
  batch 350 loss: 0.8417451715469361
  batch 400 loss: 0.8018874967098236
  batch 450 loss: 0.8148229563236237
  batch 500 loss: 0.877815330028534
  batch 550 loss: 0.882708661556244
  batch 600 loss: 0.837262568473816
  batch 650 loss: 0.8486276805400849
  batch 700 loss: 0.8611212515830994
  batch 750 loss: 0.8385284554958343
  batch 800 loss: 0.861320698261261
  batch 850 loss: 0.8757416033744811
  batch 900 loss: 0.851287350654602
LOSS train 0.85129 valid 0.99354, valid PER 30.84%
EPOCH 11:
  batch 50 loss: 0.7663733959197998
  batch 100 loss: 0.7565914195775986
  batch 150 loss: 0.788241525888443
  batch 200 loss: 0.8188423776626587
  batch 250 loss: 0.8271268594264984
  batch 300 loss: 0.8044227981567382
  batch 350 loss: 0.8088296413421631
  batch 400 loss: 0.8292779922485352
  batch 450 loss: 0.8260843181610107
  batch 500 loss: 0.794105554819107
  batch 550 loss: 0.8101687157154083
  batch 600 loss: 0.8300313043594361
  batch 650 loss: 0.8539603543281555
  batch 700 loss: 0.773974586725235
  batch 750 loss: 0.8155103969573975
  batch 800 loss: 0.8363597196340561
  batch 850 loss: 0.8671851897239685
  batch 900 loss: 0.8303147912025451
LOSS train 0.83031 valid 0.98221, valid PER 30.16%
EPOCH 12:
  batch 50 loss: 0.7668692570924759
  batch 100 loss: 0.7645013272762299
  batch 150 loss: 0.7423125714063644
  batch 200 loss: 0.7561136627197266
  batch 250 loss: 0.7710824286937714
  batch 300 loss: 0.7668293344974518
  batch 350 loss: 0.7886316037178039
  batch 400 loss: 0.8042541146278381
  batch 450 loss: 0.7839373004436493
  batch 500 loss: 0.8035208940505981
  batch 550 loss: 0.7630048203468323
  batch 600 loss: 0.7743329799175263
  batch 650 loss: 0.8261562049388885
  batch 700 loss: 0.8447154843807221
  batch 750 loss: 0.7973848283290863
  batch 800 loss: 0.8111424374580384
  batch 850 loss: 0.8487762105464935
  batch 900 loss: 0.8303274726867675
LOSS train 0.83033 valid 0.99318, valid PER 30.11%
EPOCH 13:
  batch 50 loss: 0.728979789018631
  batch 100 loss: 0.7762929338216782
  batch 150 loss: 0.7289255267381668
  batch 200 loss: 0.7655714762210846
  batch 250 loss: 0.7672289419174194
  batch 300 loss: 0.7325291019678116
  batch 350 loss: 0.7404826128482819
  batch 400 loss: 0.7489173936843873
  batch 450 loss: 0.7799687004089355
  batch 500 loss: 0.7467196637392044
  batch 550 loss: 0.7787260556221008
  batch 600 loss: 0.74192243039608
  batch 650 loss: 0.7927106821537018
  batch 700 loss: 0.7975467997789383
  batch 750 loss: 0.7439237904548645
  batch 800 loss: 0.7776617193222046
  batch 850 loss: 0.8122573775053025
  batch 900 loss: 0.8030456352233887
LOSS train 0.80305 valid 1.00461, valid PER 29.84%
EPOCH 14:
  batch 50 loss: 0.7120369213819504
  batch 100 loss: 0.7486221575737
  batch 150 loss: 0.7413484740257263
  batch 200 loss: 0.717873991727829
  batch 250 loss: 0.7300255078077317
  batch 300 loss: 0.7795589876174926
  batch 350 loss: 0.7136984288692474
  batch 400 loss: 0.721474882364273
  batch 450 loss: 0.7448796284198761
  batch 500 loss: 0.7692932140827179
  batch 550 loss: 0.7619981324672699
  batch 600 loss: 0.7224830740690231
  batch 650 loss: 0.7679392838478089
  batch 700 loss: 0.7947034478187561
  batch 750 loss: 0.7517589950561523
  batch 800 loss: 0.7235633170604706
  batch 850 loss: 0.7729213881492615
  batch 900 loss: 0.7629668235778808
LOSS train 0.76297 valid 0.98213, valid PER 29.94%
EPOCH 15:
  batch 50 loss: 0.7092174100875854
  batch 100 loss: 0.6800522303581238
  batch 150 loss: 0.7003263390064239
  batch 200 loss: 0.7420773094892502
  batch 250 loss: 0.7617473900318146
  batch 300 loss: 0.7284064108133316
  batch 350 loss: 0.7271433329582214
  batch 400 loss: 0.7535044586658478
  batch 450 loss: 0.7583947956562043
  batch 500 loss: 0.7313553881645203
  batch 550 loss: 0.7445993900299073
  batch 600 loss: 0.768452752828598
  batch 650 loss: 0.8122104513645172
  batch 700 loss: 0.7876959419250489
  batch 750 loss: 0.7776000559329986
  batch 800 loss: 0.7514658617973328
  batch 850 loss: 0.7357782590389251
  batch 900 loss: 0.7903214395046234
LOSS train 0.79032 valid 1.09107, valid PER 32.25%
EPOCH 16:
  batch 50 loss: 0.7997122299671173
  batch 100 loss: 0.719805748462677
  batch 150 loss: 0.7136482989788056
  batch 200 loss: 0.7277075755596161
  batch 250 loss: 0.763929306268692
  batch 300 loss: 0.7426201260089874
  batch 350 loss: 0.7551090586185455
  batch 400 loss: 0.7269642376899719
  batch 450 loss: 0.7284367072582245
  batch 500 loss: 0.7016098606586456
  batch 550 loss: 0.7369631910324097
  batch 600 loss: 0.7315745949745178
  batch 650 loss: 0.7434633380174637
  batch 700 loss: 0.7156940567493438
  batch 750 loss: 0.7297553294897079
  batch 800 loss: 0.745712194442749
  batch 850 loss: 0.776555694937706
  batch 900 loss: 0.7633251321315765
LOSS train 0.76333 valid 0.98797, valid PER 28.90%
EPOCH 17:
  batch 50 loss: 0.7013873392343521
  batch 100 loss: 0.6959933704137802
  batch 150 loss: 0.7007022488117218
  batch 200 loss: 0.7138711833953857
  batch 250 loss: 0.7124366039037704
  batch 300 loss: 0.7285374957323074
  batch 350 loss: 0.6959761202335357
  batch 400 loss: 0.7670017123222351
  batch 450 loss: 0.7678006505966186
  batch 500 loss: 0.7270232033729553
  batch 550 loss: 0.7299508672952651
  batch 600 loss: 0.770255069732666
  batch 650 loss: 0.7392913663387298
  batch 700 loss: 0.7550234168767929
  batch 750 loss: 0.7328364771604537
  batch 800 loss: 0.7141500115394592
  batch 850 loss: 0.7476494961977005
  batch 900 loss: 0.7221993201971054
LOSS train 0.72220 valid 1.04516, valid PER 30.54%
EPOCH 18:
  batch 50 loss: 0.7029912662506104
  batch 100 loss: 0.7081887221336365
  batch 150 loss: 0.7201829528808594
  batch 200 loss: 0.7071652221679687
  batch 250 loss: 0.7144672787189483
  batch 300 loss: 0.682975800037384
  batch 350 loss: 0.7493290537595749
  batch 400 loss: 0.6965052437782288
  batch 450 loss: 0.7647136497497559
  batch 500 loss: 0.7275973457098007
  batch 550 loss: 0.7221816891431808
  batch 600 loss: 0.6835514307022095
  batch 650 loss: 0.735749660730362
  batch 700 loss: 0.7490141785144806
  batch 750 loss: 0.7341388976573944
  batch 800 loss: 0.6934076011180877
  batch 850 loss: 0.7168083727359772
  batch 900 loss: 0.7303725385665893
LOSS train 0.73037 valid 1.02045, valid PER 30.88%
EPOCH 19:
  batch 50 loss: 0.6493094658851624
  batch 100 loss: 0.6508937025070191
  batch 150 loss: 0.6763518023490905
  batch 200 loss: 0.6824699676036835
  batch 250 loss: 0.7443667709827423
  batch 300 loss: 0.7644672721624375
  batch 350 loss: 0.7505090981721878
  batch 400 loss: 0.7499979090690613
  batch 450 loss: 0.7539570009708405
  batch 500 loss: 0.7363896846771241
  batch 550 loss: 0.7153594315052032
  batch 600 loss: 0.734495177268982
  batch 650 loss: 0.8028075289726258
  batch 700 loss: 0.7256078487634658
  batch 750 loss: 0.6890961015224457
  batch 800 loss: 0.7318630421161652
  batch 850 loss: 0.7247365206480026
  batch 900 loss: 0.7260287833213807
LOSS train 0.72603 valid 1.05406, valid PER 31.11%
EPOCH 20:
  batch 50 loss: 0.7019803452491761
  batch 100 loss: 0.6956429994106292
  batch 150 loss: 0.6968910700082779
  batch 200 loss: 0.7058272767066955
  batch 250 loss: 0.6927385061979294
  batch 300 loss: 0.7299126046895981
  batch 350 loss: 0.6820396649837493
  batch 400 loss: 0.7131206542253494
  batch 450 loss: 0.7105822342634202
  batch 500 loss: 0.6980874824523926
  batch 550 loss: 0.7605766439437867
  batch 600 loss: 0.6959060513973236
  batch 650 loss: 0.7093713706731797
  batch 700 loss: 0.704694157242775
  batch 750 loss: 0.6805540215969086
  batch 800 loss: 0.7215625143051148
  batch 850 loss: 0.719432590007782
  batch 900 loss: 0.7192557454109192
LOSS train 0.71926 valid 1.01168, valid PER 29.34%
train_loss
[1.6362880849838257, 1.35434818983078, 1.1474824631214142, 1.159674698114395, 1.0413186192512511, 0.9862994539737702, 0.9775160491466522, 0.9434805965423584, 0.8661954748630524, 0.851287350654602, 0.8303147912025451, 0.8303274726867675, 0.8030456352233887, 0.7629668235778808, 0.7903214395046234, 0.7633251321315765, 0.7221993201971054, 0.7303725385665893, 0.7260287833213807, 0.7192557454109192]
valid_loss
[1.6615935564041138, 1.3318296670913696, 1.2646108865737915, 1.131903052330017, 1.0742961168289185, 1.069706678390503, 1.0277657508850098, 1.021242380142212, 1.001691460609436, 0.99354487657547, 0.9822072982788086, 0.9931803941726685, 1.0046131610870361, 0.9821323752403259, 1.0910698175430298, 0.9879661202430725, 1.0451583862304688, 1.0204519033432007, 1.0540589094161987, 1.0116801261901855]
valid_per
[65.06465804559393, 43.01426476469804, 37.51499800026663, 35.26196507132383, 33.06892414344754, 32.87561658445541, 32.862285028662846, 30.682575656579125, 30.695907212371683, 30.842554326089854, 30.155979202772965, 30.109318757498997, 29.8360218637515, 29.942674310091988, 32.24903346220504, 28.90281295827223, 30.54259432075723, 30.87588321557126, 31.109185441941072, 29.336088521530463]
Training finished in 3.0 minutes.
Model saved to checkpoints/20231208_020826/model_14
Loading model from checkpoints/20231208_020826/model_14
SUB: 15.72%, DEL: 13.39%, INS: 2.49%, COR: 70.89%, PER: 31.60%
