Namespace(seed=123, train_json='/rds/user/yo279/hpc-work/MLMI2/fbanks/train_fbank.json', val_json='/rds/user/yo279/hpc-work/MLMI2/fbanks/dev_fbank.json', test_json='/rds/user/yo279/hpc-work/MLMI2/fbanks/test_fbank.json', batch_size=4, num_layers=1, fbank_dims=23, model_dims=128, concat=1, lr=0.7, vocab='vocab_39.txt', report_interval=50, num_epochs=20, dropout_rate=0.5, clip_max_norm=1)
Total number of model parameters is 166952
EPOCH 1:
  batch 50 loss: 5.184296011924744
  batch 100 loss: 3.556502089500427
  batch 150 loss: 3.345412087440491
  batch 200 loss: 3.229409956932068
  batch 250 loss: 3.0906769132614134
  batch 300 loss: 2.8524841976165773
  batch 350 loss: 2.6559064531326295
  batch 400 loss: 2.5213515377044677
  batch 450 loss: 2.379493260383606
  batch 500 loss: 2.2150725984573363
  batch 550 loss: 2.1568737030029297
  batch 600 loss: 2.0526777029037477
  batch 650 loss: 1.951912670135498
  batch 700 loss: 1.921989688873291
  batch 750 loss: 1.8541143608093262
  batch 800 loss: 1.856934006214142
  batch 850 loss: 1.777733061313629
  batch 900 loss: 1.7359547162055968
LOSS train 1.73595 valid 1.67348, valid PER 53.47%
EPOCH 2:
  batch 50 loss: 1.6683436608314515
  batch 100 loss: 1.6150216794013976
  batch 150 loss: 1.6152447032928468
  batch 200 loss: 1.6295190572738647
  batch 250 loss: 1.616171452999115
  batch 300 loss: 1.5759772539138794
  batch 350 loss: 1.4869388103485108
  batch 400 loss: 1.513406710624695
  batch 450 loss: 1.456353120803833
  batch 500 loss: 1.5028828167915345
  batch 550 loss: 1.4944090223312378
  batch 600 loss: 1.4100282573699952
  batch 650 loss: 1.4636397671699524
  batch 700 loss: 1.4369704723358154
  batch 750 loss: 1.427334327697754
  batch 800 loss: 1.3340798425674438
  batch 850 loss: 1.357848469018936
  batch 900 loss: 1.3749619674682618
LOSS train 1.37496 valid 1.35486, valid PER 41.95%
EPOCH 3:
  batch 50 loss: 1.325800552368164
  batch 100 loss: 1.3061410689353943
  batch 150 loss: 1.3027703380584716
  batch 200 loss: 1.2824237847328186
  batch 250 loss: 1.2747197616100312
  batch 300 loss: 1.266001068353653
  batch 350 loss: 1.3179786324501037
  batch 400 loss: 1.2929386687278748
  batch 450 loss: 1.2581792068481445
  batch 500 loss: 1.2459068965911866
  batch 550 loss: 1.2623297762870789
  batch 600 loss: 1.2278691530227661
  batch 650 loss: 1.208845191001892
  batch 700 loss: 1.2295472121238709
  batch 750 loss: 1.2725114369392394
  batch 800 loss: 1.195317394733429
  batch 850 loss: 1.2434032142162323
  batch 900 loss: 1.1664638900756836
LOSS train 1.16646 valid 1.25327, valid PER 38.00%
EPOCH 4:
  batch 50 loss: 1.1464526665210724
  batch 100 loss: 1.1817534124851228
  batch 150 loss: 1.1287978506088256
  batch 200 loss: 1.1718736970424652
  batch 250 loss: 1.17705503821373
  batch 300 loss: 1.1890244793891906
  batch 350 loss: 1.0955000269412993
  batch 400 loss: 1.1615825951099397
  batch 450 loss: 1.1352586078643798
  batch 500 loss: 1.1205419075489045
  batch 550 loss: 1.165695194005966
  batch 600 loss: 1.1715228283405303
  batch 650 loss: 1.1502502274513244
  batch 700 loss: 1.118507775068283
  batch 750 loss: 1.10651895403862
  batch 800 loss: 1.0748396396636963
  batch 850 loss: 1.0986334824562072
  batch 900 loss: 1.1493149781227112
LOSS train 1.14931 valid 1.15921, valid PER 35.30%
EPOCH 5:
  batch 50 loss: 1.0594668614864349
  batch 100 loss: 1.0435853219032287
  batch 150 loss: 1.1172696769237518
  batch 200 loss: 1.0435946118831634
  batch 250 loss: 1.0422212982177734
  batch 300 loss: 1.0666614139080048
  batch 350 loss: 1.0469776642322541
  batch 400 loss: 1.0656829857826233
  batch 450 loss: 1.054709997177124
  batch 500 loss: 1.0820140528678894
  batch 550 loss: 1.026947295665741
  batch 600 loss: 1.0928131330013275
  batch 650 loss: 1.0488845205307007
  batch 700 loss: 1.0834771931171416
  batch 750 loss: 1.0178465163707733
  batch 800 loss: 1.0469263887405396
  batch 850 loss: 1.057817462682724
  batch 900 loss: 1.0621910738945006
LOSS train 1.06219 valid 1.07841, valid PER 33.71%
EPOCH 6:
  batch 50 loss: 1.0122919821739196
  batch 100 loss: 0.9631597197055817
  batch 150 loss: 0.9599303007125854
  batch 200 loss: 0.9955832326412201
  batch 250 loss: 1.0322041201591492
  batch 300 loss: 0.9843124377727509
  batch 350 loss: 0.9969817030429841
  batch 400 loss: 0.9818223309516907
  batch 450 loss: 1.0212849926948548
  batch 500 loss: 1.0091801285743713
  batch 550 loss: 1.0085695242881776
  batch 600 loss: 0.9725453186035157
  batch 650 loss: 1.0160486829280853
  batch 700 loss: 1.0120670151710511
  batch 750 loss: 0.9928782176971436
  batch 800 loss: 0.9898369824886322
  batch 850 loss: 0.9646765232086182
  batch 900 loss: 0.991277471780777
LOSS train 0.99128 valid 1.09983, valid PER 33.50%
EPOCH 7:
  batch 50 loss: 0.9697971260547638
  batch 100 loss: 0.9762514364719391
  batch 150 loss: 0.922889244556427
  batch 200 loss: 0.9205784350633621
  batch 250 loss: 0.934643896818161
  batch 300 loss: 0.9504042375087738
  batch 350 loss: 0.9285085535049439
  batch 400 loss: 0.9426248061656952
  batch 450 loss: 0.9294666385650635
  batch 500 loss: 0.9551144278049469
  batch 550 loss: 0.9440192592144012
  batch 600 loss: 0.9625545644760132
  batch 650 loss: 0.9442638683319092
  batch 700 loss: 0.9781755149364472
  batch 750 loss: 0.9508862602710724
  batch 800 loss: 0.9178794205188752
  batch 850 loss: 0.9660317659378052
  batch 900 loss: 0.978614239692688
LOSS train 0.97861 valid 1.04160, valid PER 32.26%
EPOCH 8:
  batch 50 loss: 0.8847643029689789
  batch 100 loss: 0.8885903465747833
  batch 150 loss: 0.8749405431747437
  batch 200 loss: 0.8834172701835632
  batch 250 loss: 0.8793961751461029
  batch 300 loss: 0.8501525175571442
  batch 350 loss: 0.9317321348190307
  batch 400 loss: 0.8737322497367859
  batch 450 loss: 0.8979924857616425
  batch 500 loss: 0.9341433691978455
  batch 550 loss: 0.8781033062934875
  batch 600 loss: 0.9349208068847656
  batch 650 loss: 0.9381655156612396
  batch 700 loss: 0.8800411021709442
  batch 750 loss: 0.8971325194835663
  batch 800 loss: 0.916820970773697
  batch 850 loss: 0.930834732055664
  batch 900 loss: 0.9273044574260711
LOSS train 0.92730 valid 1.02143, valid PER 30.73%
EPOCH 9:
  batch 50 loss: 0.824754170179367
  batch 100 loss: 0.8497570264339447
  batch 150 loss: 0.868467892408371
  batch 200 loss: 0.8099863457679749
  batch 250 loss: 0.8559385478496552
  batch 300 loss: 0.8785263299942017
  batch 350 loss: 0.8960018599033356
  batch 400 loss: 0.8686860620975494
  batch 450 loss: 0.8823656988143921
  batch 500 loss: 0.8390747380256652
  batch 550 loss: 0.88138263463974
  batch 600 loss: 0.8831690883636475
  batch 650 loss: 0.8611828911304474
  batch 700 loss: 0.8695613467693328
  batch 750 loss: 0.8904637050628662
  batch 800 loss: 0.8864589703083038
  batch 850 loss: 0.9001389873027802
  batch 900 loss: 0.8571413838863373
LOSS train 0.85714 valid 1.03133, valid PER 30.53%
EPOCH 10:
  batch 50 loss: 0.7926960480213165
  batch 100 loss: 0.7995688939094543
  batch 150 loss: 0.8318506765365601
  batch 200 loss: 0.855019109249115
  batch 250 loss: 0.8578105521202087
  batch 300 loss: 0.7922042095661164
  batch 350 loss: 0.8302329349517822
  batch 400 loss: 0.790209236741066
  batch 450 loss: 0.8209380757808685
  batch 500 loss: 0.8552180850505828
  batch 550 loss: 0.8820651638507843
  batch 600 loss: 0.8385218024253845
  batch 650 loss: 0.8305831730365754
  batch 700 loss: 0.8637240493297577
  batch 750 loss: 0.8362016975879669
  batch 800 loss: 0.8338887584209442
  batch 850 loss: 0.8510623562335968
  batch 900 loss: 0.8545673084259033
LOSS train 0.85457 valid 1.02987, valid PER 31.08%
EPOCH 11:
  batch 50 loss: 0.7661551737785339
  batch 100 loss: 0.7507973694801331
  batch 150 loss: 0.7749801385402679
  batch 200 loss: 0.8110139608383179
  batch 250 loss: 0.8255898368358612
  batch 300 loss: 0.7694850111007691
  batch 350 loss: 0.8026811206340789
  batch 400 loss: 0.8010428130626679
  batch 450 loss: 0.81445019364357
  batch 500 loss: 0.7848062086105346
  batch 550 loss: 0.8154111385345459
  batch 600 loss: 0.8014011174440384
  batch 650 loss: 0.8844415199756622
  batch 700 loss: 0.7966327965259552
  batch 750 loss: 0.8009763956069946
  batch 800 loss: 0.817589123249054
  batch 850 loss: 0.853422999382019
  batch 900 loss: 0.826651508808136
LOSS train 0.82665 valid 1.00053, valid PER 30.32%
EPOCH 12:
  batch 50 loss: 0.7681919312477112
  batch 100 loss: 0.7514544427394867
  batch 150 loss: 0.7164492517709732
  batch 200 loss: 0.7410152697563172
  batch 250 loss: 0.7771435225009918
  batch 300 loss: 0.7590025806427002
  batch 350 loss: 0.7727002757787704
  batch 400 loss: 0.7949592447280884
  batch 450 loss: 0.8206028008460998
  batch 500 loss: 0.8002457249164582
  batch 550 loss: 0.7440638667345048
  batch 600 loss: 0.7822064924240112
  batch 650 loss: 0.8380727469921112
  batch 700 loss: 0.8055919474363327
  batch 750 loss: 0.785297240614891
  batch 800 loss: 0.7678581100702285
  batch 850 loss: 0.8310240995883942
  batch 900 loss: 0.8368276154994965
LOSS train 0.83683 valid 0.99670, valid PER 29.52%
EPOCH 13:
  batch 50 loss: 0.7171519279479981
  batch 100 loss: 0.7494469785690308
  batch 150 loss: 0.7280544131994248
  batch 200 loss: 0.7589844071865082
  batch 250 loss: 0.7458156871795655
  batch 300 loss: 0.7562494003772735
  batch 350 loss: 0.7950722551345826
  batch 400 loss: 0.7954848343133927
  batch 450 loss: 0.8131490898132324
  batch 500 loss: 0.7504058277606964
  batch 550 loss: 0.7934190475940704
  batch 600 loss: 0.7674020504951478
  batch 650 loss: 0.7717187786102295
  batch 700 loss: 0.812352249622345
  batch 750 loss: 0.7861003971099854
  batch 800 loss: 0.7822245633602143
  batch 850 loss: 0.7972652161121369
  batch 900 loss: 0.7905703890323639
LOSS train 0.79057 valid 1.01592, valid PER 29.54%
EPOCH 14:
  batch 50 loss: 0.7140616309642792
  batch 100 loss: 0.7344249391555786
  batch 150 loss: 0.7289245808124543
  batch 200 loss: 0.7209921193122864
  batch 250 loss: 0.7379344427585601
  batch 300 loss: 0.7595145863294601
  batch 350 loss: 0.7055214309692383
  batch 400 loss: 0.7308934330940247
  batch 450 loss: 0.7688457083702087
  batch 500 loss: 0.7671617710590363
  batch 550 loss: 0.7718165349960328
  batch 600 loss: 0.7338324373960495
  batch 650 loss: 0.7574534130096435
  batch 700 loss: 0.7798172223567963
  batch 750 loss: 0.7480575025081635
  batch 800 loss: 0.7098872727155685
  batch 850 loss: 0.7653156089782714
  batch 900 loss: 0.7543165320158005
LOSS train 0.75432 valid 1.00604, valid PER 29.82%
EPOCH 15:
  batch 50 loss: 0.6811867231130599
  batch 100 loss: 0.6764937859773635
  batch 150 loss: 0.7016072487831115
  batch 200 loss: 0.7283238697052002
  batch 250 loss: 0.7182444846630096
  batch 300 loss: 0.702261905670166
  batch 350 loss: 0.7339835107326508
  batch 400 loss: 0.7061461770534515
  batch 450 loss: 0.7114860057830811
  batch 500 loss: 0.6940699684619903
  batch 550 loss: 0.7019668537378311
  batch 600 loss: 0.7404407262802124
  batch 650 loss: 0.7440669554471969
  batch 700 loss: 0.7704191982746125
  batch 750 loss: 0.7582416319847107
  batch 800 loss: 0.7235130500793457
  batch 850 loss: 0.7353609359264374
  batch 900 loss: 0.7414288437366485
LOSS train 0.74143 valid 1.00712, valid PER 29.42%
EPOCH 16:
  batch 50 loss: 0.7053911006450653
  batch 100 loss: 0.6595305526256561
  batch 150 loss: 0.6724318873882293
  batch 200 loss: 0.6759711694717407
  batch 250 loss: 0.684537593126297
  batch 300 loss: 0.6926183605194092
  batch 350 loss: 0.7199256467819214
  batch 400 loss: 0.719271548986435
  batch 450 loss: 0.7222487986087799
  batch 500 loss: 0.6712507486343384
  batch 550 loss: 0.7033842515945434
  batch 600 loss: 0.6955384957790375
  batch 650 loss: 0.730667188167572
  batch 700 loss: 0.7306347090005875
  batch 750 loss: 0.7062582105398179
  batch 800 loss: 0.7185820716619492
  batch 850 loss: 0.7362406122684478
  batch 900 loss: 0.7128397393226623
LOSS train 0.71284 valid 1.00394, valid PER 28.56%
EPOCH 17:
  batch 50 loss: 0.6691036593914031
  batch 100 loss: 0.6712495100498199
  batch 150 loss: 0.6461759597063065
  batch 200 loss: 0.6594456112384797
  batch 250 loss: 0.6760503762960434
  batch 300 loss: 0.6635549110174179
  batch 350 loss: 0.6506409496068954
  batch 400 loss: 0.7146917355060577
  batch 450 loss: 0.6916370493173599
  batch 500 loss: 0.6848612767457962
  batch 550 loss: 0.7474782872200012
  batch 600 loss: 0.7361482030153275
  batch 650 loss: 0.7122093206644058
  batch 700 loss: 0.6705424040555954
  batch 750 loss: 0.6805134224891662
  batch 800 loss: 0.6999604797363281
  batch 850 loss: 0.7314014875888825
  batch 900 loss: 0.6847209358215331
LOSS train 0.68472 valid 1.14579, valid PER 32.40%
EPOCH 18:
  batch 50 loss: 0.7602160000801086
  batch 100 loss: 0.7376092994213104
  batch 150 loss: 0.7286509096622467
  batch 200 loss: 0.6969084507226944
  batch 250 loss: 0.7124750316143036
  batch 300 loss: 0.6642323243618011
  batch 350 loss: 0.7026849687099457
  batch 400 loss: 0.6567621672153473
  batch 450 loss: 0.696751424074173
  batch 500 loss: 0.6838424444198609
  batch 550 loss: 0.6831883478164673
  batch 600 loss: 0.6658389866352081
  batch 650 loss: 0.6700494885444641
  batch 700 loss: 0.698718991279602
  batch 750 loss: 0.6475789374113083
  batch 800 loss: 0.6889405310153961
  batch 850 loss: 0.6904836308956146
  batch 900 loss: 0.7275124847888946
LOSS train 0.72751 valid 1.04437, valid PER 30.01%
EPOCH 19:
  batch 50 loss: 0.6128239738941192
  batch 100 loss: 0.6246980339288711
  batch 150 loss: 0.6172826331853867
  batch 200 loss: 0.6449174529314041
  batch 250 loss: 0.6366952002048493
  batch 300 loss: 0.6576177543401718
  batch 350 loss: 0.6392125970125199
  batch 400 loss: 0.6603898620605468
  batch 450 loss: 0.6810259234905243
  batch 500 loss: 0.6838764542341232
  batch 550 loss: 0.6525884580612182
  batch 600 loss: 0.6420943588018417
  batch 650 loss: 0.7187080621719361
  batch 700 loss: 0.6488214749097824
  batch 750 loss: 0.6770221370458603
  batch 800 loss: 0.6944365763664245
  batch 850 loss: 0.6850232833623886
  batch 900 loss: 0.6820861691236496
LOSS train 0.68209 valid 1.02230, valid PER 28.83%
EPOCH 20:
  batch 50 loss: 0.6208293050527572
  batch 100 loss: 0.6479913628101349
  batch 150 loss: 0.6275284606218338
  batch 200 loss: 0.6362263667583465
  batch 250 loss: 0.6250936490297317
  batch 300 loss: 0.6672906267642975
  batch 350 loss: 0.6177532207965851
  batch 400 loss: 0.6273155611753464
  batch 450 loss: 0.6628761345148086
  batch 500 loss: 0.6111984866857528
  batch 550 loss: 0.7059253579378129
  batch 600 loss: 0.6511464995145798
  batch 650 loss: 0.6820300668478012
  batch 700 loss: 0.6623402881622314
  batch 750 loss: 0.6225331979990005
  batch 800 loss: 0.6941377139091491
  batch 850 loss: 0.6553953194618225
  batch 900 loss: 0.6839545887708663
LOSS train 0.68395 valid 1.02212, valid PER 29.00%
train_loss
[1.7359547162055968, 1.3749619674682618, 1.1664638900756836, 1.1493149781227112, 1.0621910738945006, 0.991277471780777, 0.978614239692688, 0.9273044574260711, 0.8571413838863373, 0.8545673084259033, 0.826651508808136, 0.8368276154994965, 0.7905703890323639, 0.7543165320158005, 0.7414288437366485, 0.7128397393226623, 0.6847209358215331, 0.7275124847888946, 0.6820861691236496, 0.6839545887708663]
valid_loss
[1.6734750270843506, 1.3548610210418701, 1.253273844718933, 1.1592050790786743, 1.0784121751785278, 1.0998257398605347, 1.0416005849838257, 1.0214273929595947, 1.0313256978988647, 1.029868245124817, 1.0005338191986084, 0.996702253818512, 1.015921950340271, 1.006043791770935, 1.0071194171905518, 1.0039364099502563, 1.1457862854003906, 1.0443744659423828, 1.022303581237793, 1.022119402885437]
valid_per
[53.47287028396214, 41.947740301293166, 38.00159978669511, 35.301959738701505, 33.70883882149047, 33.49553392880949, 32.25569924010132, 30.729236101853086, 30.52926276496467, 31.075856552459673, 30.322623650179974, 29.516064524730034, 29.542727636315156, 29.822690307958936, 29.416077856285828, 28.562858285561926, 32.40234635381949, 30.00933208905479, 28.829489401413145, 29.002799626716435]
Training finished in 3.0 minutes.
Model saved to checkpoints/20231208_091231/model_12
Loading model from checkpoints/20231208_091231/model_12
SUB: 16.88%, DEL: 11.71%, INS: 2.17%, COR: 71.42%, PER: 30.75%
