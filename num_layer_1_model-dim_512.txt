Namespace(seed=123, train_json='/rds/user/yo279/hpc-work/MLMI2/fbanks/train_fbank.json', val_json='/rds/user/yo279/hpc-work/MLMI2/fbanks/dev_fbank.json', test_json='/rds/user/yo279/hpc-work/MLMI2/fbanks/test_fbank.json', batch_size=4, num_layers=1, fbank_dims=23, model_dims=512, concat=1, lr=0.5, vocab='vocab_39.txt', report_interval=50, num_epochs=20, dropout_rate=0.5, clip_max_norm=1)
Total number of model parameters is 2240552
EPOCH 1:
  batch 50 loss: 4.338212089538574
  batch 100 loss: 3.0258346033096313
  batch 150 loss: 2.901230273246765
  batch 200 loss: 2.717090535163879
  batch 250 loss: 2.641236720085144
  batch 300 loss: 2.483536472320557
  batch 350 loss: 2.4006562519073484
  batch 400 loss: 2.3075594401359556
  batch 450 loss: 2.168384053707123
  batch 500 loss: 2.036749591827393
  batch 550 loss: 2.007160711288452
  batch 600 loss: 1.9344421172142028
  batch 650 loss: 1.927135727405548
  batch 700 loss: 1.8535929656028747
  batch 750 loss: 1.7832919430732728
  batch 800 loss: 1.7614187502861023
  batch 850 loss: 1.7115338110923768
  batch 900 loss: 1.6924512314796447
LOSS train 1.69245 valid 1.63949, valid PER 54.83%
EPOCH 2:
  batch 50 loss: 1.6620225358009337
  batch 100 loss: 1.5834939193725586
  batch 150 loss: 1.6047794890403748
  batch 200 loss: 1.5850204586982728
  batch 250 loss: 1.575871193408966
  batch 300 loss: 1.524180953502655
  batch 350 loss: 1.4463288688659668
  batch 400 loss: 1.4667771720886231
  batch 450 loss: 1.4209905791282653
  batch 500 loss: 1.4547258067131041
  batch 550 loss: 1.4689669704437256
  batch 600 loss: 1.3931765365600586
  batch 650 loss: 1.4418169927597047
  batch 700 loss: 1.4019931864738464
  batch 750 loss: 1.3808084428310394
  batch 800 loss: 1.334743663072586
  batch 850 loss: 1.3182275807857513
  batch 900 loss: 1.359037654399872
LOSS train 1.35904 valid 1.32652, valid PER 41.37%
EPOCH 3:
  batch 50 loss: 1.2857100343704224
  batch 100 loss: 1.271470398902893
  batch 150 loss: 1.2655771040916444
  batch 200 loss: 1.2418323314189912
  batch 250 loss: 1.236764028072357
  batch 300 loss: 1.2275447940826416
  batch 350 loss: 1.2744321250915527
  batch 400 loss: 1.2511821126937865
  batch 450 loss: 1.2138885414600373
  batch 500 loss: 1.1969448935985565
  batch 550 loss: 1.2228502309322358
  batch 600 loss: 1.1667199277877807
  batch 650 loss: 1.1682639515399933
  batch 700 loss: 1.1809444105625153
  batch 750 loss: 1.2250131916999818
  batch 800 loss: 1.1627868020534515
  batch 850 loss: 1.1901267993450164
  batch 900 loss: 1.101242161989212
LOSS train 1.10124 valid 1.19805, valid PER 36.07%
EPOCH 4:
  batch 50 loss: 1.1003355991840362
  batch 100 loss: 1.1119605088233948
  batch 150 loss: 1.0752058851718902
  batch 200 loss: 1.0963710856437683
  batch 250 loss: 1.09867462515831
  batch 300 loss: 1.0961121618747711
  batch 350 loss: 1.025240023136139
  batch 400 loss: 1.0672632551193237
  batch 450 loss: 1.0736975598335265
  batch 500 loss: 1.0556740033626557
  batch 550 loss: 1.0884806060791015
  batch 600 loss: 1.1170656645298005
  batch 650 loss: 1.0635487914085389
  batch 700 loss: 1.0303807699680327
  batch 750 loss: 1.0220477271080017
  batch 800 loss: 0.9988206005096436
  batch 850 loss: 1.0458883130550385
  batch 900 loss: 1.0752694761753083
LOSS train 1.07527 valid 1.05309, valid PER 32.50%
EPOCH 5:
  batch 50 loss: 0.9629399967193604
  batch 100 loss: 0.974583066701889
  batch 150 loss: 1.0224001133441925
  batch 200 loss: 0.9529475009441376
  batch 250 loss: 0.9545070028305054
  batch 300 loss: 0.9659892034530639
  batch 350 loss: 0.9554760420322418
  batch 400 loss: 0.9662178540229798
  batch 450 loss: 0.960210542678833
  batch 500 loss: 0.9946639752388
  batch 550 loss: 0.940859260559082
  batch 600 loss: 1.0349193000793457
  batch 650 loss: 0.9656463086605072
  batch 700 loss: 0.9929657804965973
  batch 750 loss: 0.9256810462474823
  batch 800 loss: 0.9599710917472839
  batch 850 loss: 0.949193365573883
  batch 900 loss: 0.9559584641456604
LOSS train 0.95596 valid 0.99649, valid PER 30.73%
EPOCH 6:
  batch 50 loss: 0.9219115173816681
  batch 100 loss: 0.8548640859127045
  batch 150 loss: 0.8585299450159073
  batch 200 loss: 0.8805258166790009
  batch 250 loss: 0.9216201722621917
  batch 300 loss: 0.8888237738609314
  batch 350 loss: 0.8889619970321655
  batch 400 loss: 0.8890678369998932
  batch 450 loss: 0.9196253204345703
  batch 500 loss: 0.8997985553741455
  batch 550 loss: 0.8886774504184722
  batch 600 loss: 0.8529800629615784
  batch 650 loss: 0.8939094007015228
  batch 700 loss: 0.8922513043880462
  batch 750 loss: 0.8761100792884826
  batch 800 loss: 0.8689032661914825
  batch 850 loss: 0.8599232292175293
  batch 900 loss: 0.8810749280452729
LOSS train 0.88107 valid 1.00359, valid PER 30.12%
EPOCH 7:
  batch 50 loss: 0.8245660185813903
  batch 100 loss: 0.8108821761608124
  batch 150 loss: 0.827625139951706
  batch 200 loss: 0.7968353056907653
  batch 250 loss: 0.7883111643791199
  batch 300 loss: 0.7964432191848755
  batch 350 loss: 0.8112312483787537
  batch 400 loss: 0.8086449933052063
  batch 450 loss: 0.8293912398815155
  batch 500 loss: 0.8211329710483551
  batch 550 loss: 0.8129649627208709
  batch 600 loss: 0.8302202463150025
  batch 650 loss: 0.8188598680496216
  batch 700 loss: 0.8310833835601806
  batch 750 loss: 0.80239128947258
  batch 800 loss: 0.8083850419521332
  batch 850 loss: 0.8250518882274628
  batch 900 loss: 0.8528632354736329
LOSS train 0.85286 valid 0.95318, valid PER 28.85%
EPOCH 8:
  batch 50 loss: 0.7374068915843963
  batch 100 loss: 0.7156397521495819
  batch 150 loss: 0.7498759377002716
  batch 200 loss: 0.7499801576137543
  batch 250 loss: 0.7506742453575135
  batch 300 loss: 0.7127928674221039
  batch 350 loss: 0.785858461856842
  batch 400 loss: 0.7517353284358979
  batch 450 loss: 0.7575436592102051
  batch 500 loss: 0.7959343922138215
  batch 550 loss: 0.7247089672088624
  batch 600 loss: 0.7756409764289856
  batch 650 loss: 0.786429010629654
  batch 700 loss: 0.7399185615777969
  batch 750 loss: 0.7636784887313843
  batch 800 loss: 0.783271180987358
  batch 850 loss: 0.7486184394359588
  batch 900 loss: 0.7728173971176148
LOSS train 0.77282 valid 0.93699, valid PER 28.12%
EPOCH 9:
  batch 50 loss: 0.6456312263011932
  batch 100 loss: 0.6620085978507996
  batch 150 loss: 0.6818167173862457
  batch 200 loss: 0.6567025250196457
  batch 250 loss: 0.691448656320572
  batch 300 loss: 0.7028071260452271
  batch 350 loss: 0.7177818924188614
  batch 400 loss: 0.7002700203657151
  batch 450 loss: 0.6799053829908371
  batch 500 loss: 0.6884730315208435
  batch 550 loss: 0.700462132692337
  batch 600 loss: 0.7218555247783661
  batch 650 loss: 0.7031086409091949
  batch 700 loss: 0.673146151304245
  batch 750 loss: 0.6846535873413085
  batch 800 loss: 0.7096793484687806
  batch 850 loss: 0.7207534325122833
  batch 900 loss: 0.6882720983028412
LOSS train 0.68827 valid 0.93457, valid PER 27.60%
EPOCH 10:
  batch 50 loss: 0.5825128918886184
  batch 100 loss: 0.6054852128028869
  batch 150 loss: 0.6367443418502807
  batch 200 loss: 0.6335467725992203
  batch 250 loss: 0.6544989132881165
  batch 300 loss: 0.6202033579349517
  batch 350 loss: 0.6407427561283111
  batch 400 loss: 0.6042246448993683
  batch 450 loss: 0.6236053836345673
  batch 500 loss: 0.659800552725792
  batch 550 loss: 0.686575801372528
  batch 600 loss: 0.6561986976861953
  batch 650 loss: 0.634856715798378
  batch 700 loss: 0.6667158907651901
  batch 750 loss: 0.6381021308898925
  batch 800 loss: 0.6653657174110412
  batch 850 loss: 0.6678465533256531
  batch 900 loss: 0.6736149245500564
LOSS train 0.67361 valid 0.97478, valid PER 29.14%
EPOCH 11:
  batch 50 loss: 0.5340604096651077
  batch 100 loss: 0.5183760529756546
  batch 150 loss: 0.537493924498558
  batch 200 loss: 0.5994110959768295
  batch 250 loss: 0.5791527485847473
  batch 300 loss: 0.5616450077295303
  batch 350 loss: 0.5922455018758774
  batch 400 loss: 0.6227972257137299
  batch 450 loss: 0.6108913832902908
  batch 500 loss: 0.5858608520030976
  batch 550 loss: 0.6062498378753662
  batch 600 loss: 0.5820443791151046
  batch 650 loss: 0.6531819874048232
  batch 700 loss: 0.5850990170240402
  batch 750 loss: 0.5821695226430893
  batch 800 loss: 0.6303411597013473
  batch 850 loss: 0.6291435921192169
  batch 900 loss: 0.6300307017564774
LOSS train 0.63003 valid 0.94402, valid PER 27.08%
EPOCH 12:
  batch 50 loss: 0.5146539261937142
  batch 100 loss: 0.5018905639648438
  batch 150 loss: 0.4682620549201965
  batch 200 loss: 0.5247727504372597
  batch 250 loss: 0.5246144163608552
  batch 300 loss: 0.5074036836624145
  batch 350 loss: 0.5043661737442017
  batch 400 loss: 0.5592253237962723
  batch 450 loss: 0.5488258689641953
  batch 500 loss: 0.5673870426416397
  batch 550 loss: 0.5260874515771866
  batch 600 loss: 0.5494124460220337
  batch 650 loss: 0.579232405424118
  batch 700 loss: 0.5805572557449341
  batch 750 loss: 0.5625566893815994
  batch 800 loss: 0.5677692699432373
  batch 850 loss: 0.6024484246969223
  batch 900 loss: 0.5934237790107727
LOSS train 0.59342 valid 0.92459, valid PER 26.76%
EPOCH 13:
  batch 50 loss: 0.4530723249912262
  batch 100 loss: 0.45752674162387846
  batch 150 loss: 0.4512828412652016
  batch 200 loss: 0.4699731248617172
  batch 250 loss: 0.45443788439035415
  batch 300 loss: 0.4730770707130432
  batch 350 loss: 0.4728158265352249
  batch 400 loss: 0.4907213509082794
  batch 450 loss: 0.4765803635120392
  batch 500 loss: 0.4964850336313248
  batch 550 loss: 0.52618214905262
  batch 600 loss: 0.504854588508606
  batch 650 loss: 0.5409370934963227
  batch 700 loss: 0.5354694128036499
  batch 750 loss: 0.4849163067340851
  batch 800 loss: 0.5115569192171097
  batch 850 loss: 0.5370306164026261
  batch 900 loss: 0.5377785760164261
LOSS train 0.53778 valid 0.95759, valid PER 26.60%
EPOCH 14:
  batch 50 loss: 0.41832259237766267
  batch 100 loss: 0.39435858637094495
  batch 150 loss: 0.4060338747501373
  batch 200 loss: 0.4004398423433304
  batch 250 loss: 0.42242798328399656
  batch 300 loss: 0.4517282071709633
  batch 350 loss: 0.4333930730819702
  batch 400 loss: 0.43468033641576764
  batch 450 loss: 0.4563069522380829
  batch 500 loss: 0.45804445624351503
  batch 550 loss: 0.4606158632040024
  batch 600 loss: 0.4328406235575676
  batch 650 loss: 0.4649517983198166
  batch 700 loss: 0.498064421415329
  batch 750 loss: 0.4581855130195618
  batch 800 loss: 0.4486994940042496
  batch 850 loss: 0.49093867719173434
  batch 900 loss: 0.4932981824874878
LOSS train 0.49330 valid 0.98831, valid PER 27.05%
EPOCH 15:
  batch 50 loss: 0.3602757403254509
  batch 100 loss: 0.36900031834840774
  batch 150 loss: 0.3690385863184929
  batch 200 loss: 0.4028008431196213
  batch 250 loss: 0.40934058219194414
  batch 300 loss: 0.37437933146953584
  batch 350 loss: 0.3906808379292488
  batch 400 loss: 0.4119530230760574
  batch 450 loss: 0.4069911041855812
  batch 500 loss: 0.4057057946920395
  batch 550 loss: 0.4104902899265289
  batch 600 loss: 0.4145695203542709
  batch 650 loss: 0.4283274227380753
  batch 700 loss: 0.427876709997654
  batch 750 loss: 0.4309784147143364
  batch 800 loss: 0.4206325286626816
  batch 850 loss: 0.4233566159009933
  batch 900 loss: 0.45646302282810214
LOSS train 0.45646 valid 1.02444, valid PER 26.76%
EPOCH 16:
  batch 50 loss: 0.331862625181675
  batch 100 loss: 0.3278681442141533
  batch 150 loss: 0.33388146728277207
  batch 200 loss: 0.33899613440036774
  batch 250 loss: 0.3498548579216003
  batch 300 loss: 0.3263501027226448
  batch 350 loss: 0.35597378730773926
  batch 400 loss: 0.37847763657569883
  batch 450 loss: 0.37627868115901947
  batch 500 loss: 0.34947498977184294
  batch 550 loss: 0.3524427419900894
  batch 600 loss: 0.35656329154968264
  batch 650 loss: 0.3808421388268471
  batch 700 loss: 0.36685933768749235
  batch 750 loss: 0.38713068902492526
  batch 800 loss: 0.3988496258854866
  batch 850 loss: 0.4055742698907852
  batch 900 loss: 0.40611235558986664
LOSS train 0.40611 valid 1.03003, valid PER 26.46%
EPOCH 17:
  batch 50 loss: 0.28571963757276536
  batch 100 loss: 0.3058277001976967
  batch 150 loss: 0.28963388949632646
  batch 200 loss: 0.2801456007361412
  batch 250 loss: 0.3169893017411232
  batch 300 loss: 0.31019336968660355
  batch 350 loss: 0.3067720550298691
  batch 400 loss: 0.32653016299009324
  batch 450 loss: 0.3144220530986786
  batch 500 loss: 0.308046942949295
  batch 550 loss: 0.31230978190898895
  batch 600 loss: 0.3339652919769287
  batch 650 loss: 0.3340411052107811
  batch 700 loss: 0.33856975644826887
  batch 750 loss: 0.3496745231747627
  batch 800 loss: 0.3378954243659973
  batch 850 loss: 0.3513948130607605
  batch 900 loss: 0.3444863533973694
LOSS train 0.34449 valid 1.07446, valid PER 26.42%
EPOCH 18:
  batch 50 loss: 0.27353865146636963
  batch 100 loss: 0.2624704220890999
  batch 150 loss: 0.2624881032109261
  batch 200 loss: 0.2657883223891258
  batch 250 loss: 0.2771571269631386
  batch 300 loss: 0.2513655796647072
  batch 350 loss: 0.2733629064261913
  batch 400 loss: 0.2758493426442146
  batch 450 loss: 0.28488627761602403
  batch 500 loss: 0.29092202872037887
  batch 550 loss: 0.30135625883936884
  batch 600 loss: 0.30455307990312575
  batch 650 loss: 0.29943465381860734
  batch 700 loss: 0.33275541812181475
  batch 750 loss: 0.31337384045124056
  batch 800 loss: 0.31371197760105135
  batch 850 loss: 0.3282738411426544
  batch 900 loss: 0.328108925819397
LOSS train 0.32811 valid 1.10312, valid PER 27.26%
EPOCH 19:
  batch 50 loss: 0.25017715990543365
  batch 100 loss: 0.22538609236478804
  batch 150 loss: 0.22176281809806825
  batch 200 loss: 0.22902636647224425
  batch 250 loss: 0.23182534784078598
  batch 300 loss: 0.23824426382780076
  batch 350 loss: 0.21755375027656554
  batch 400 loss: 0.2418417139351368
  batch 450 loss: 0.26063184171915055
  batch 500 loss: 0.24795356005430222
  batch 550 loss: 0.23846767961978912
  batch 600 loss: 0.2400280398130417
  batch 650 loss: 0.29188392505049704
  batch 700 loss: 0.275863801240921
  batch 750 loss: 0.26763630092144014
  batch 800 loss: 0.29326161950826646
  batch 850 loss: 0.2840034395456314
  batch 900 loss: 0.29411778554320334
LOSS train 0.29412 valid 1.14336, valid PER 26.86%
EPOCH 20:
  batch 50 loss: 0.23675429418683053
  batch 100 loss: 0.2027067671716213
  batch 150 loss: 0.18041881397366524
  batch 200 loss: 0.19529418587684633
  batch 250 loss: 0.1772613002359867
  batch 300 loss: 0.2042890989780426
  batch 350 loss: 0.20203712403774263
  batch 400 loss: 0.21017082944512366
  batch 450 loss: 0.20911093771457673
  batch 500 loss: 0.1968958942592144
  batch 550 loss: 0.24657634317874907
  batch 600 loss: 0.22347557216882705
  batch 650 loss: 0.22633146733045578
  batch 700 loss: 0.2523849368095398
  batch 750 loss: 0.23062886238098146
  batch 800 loss: 0.2571846051514149
  batch 850 loss: 0.2506429812312126
  batch 900 loss: 0.25395736321806905
LOSS train 0.25396 valid 1.19011, valid PER 27.06%
train_loss
[1.6924512314796447, 1.359037654399872, 1.101242161989212, 1.0752694761753083, 0.9559584641456604, 0.8810749280452729, 0.8528632354736329, 0.7728173971176148, 0.6882720983028412, 0.6736149245500564, 0.6300307017564774, 0.5934237790107727, 0.5377785760164261, 0.4932981824874878, 0.45646302282810214, 0.40611235558986664, 0.3444863533973694, 0.328108925819397, 0.29411778554320334, 0.25395736321806905]
valid_loss
[1.639490008354187, 1.3265219926834106, 1.1980516910552979, 1.053086757659912, 0.9964861869812012, 1.0035895109176636, 0.9531815052032471, 0.936991810798645, 0.9345690011978149, 0.9747772812843323, 0.9440203905105591, 0.9245941638946533, 0.9575915932655334, 0.988311231136322, 1.024436116218567, 1.0300264358520508, 1.0744566917419434, 1.103123426437378, 1.143363356590271, 1.1901144981384277]
valid_per
[54.832688974803354, 41.36781762431676, 36.068524196773765, 32.49566724436742, 30.729236101853086, 30.115984535395278, 28.849486735101987, 28.11625116651113, 27.59632049060125, 29.14278096253833, 27.083055592587655, 26.75643247566991, 26.596453806159182, 27.049726703106252, 26.75643247566991, 26.45647247033729, 26.423143580855886, 27.25636581789095, 26.8630849220104, 27.063058258898813]
Training finished in 7.0 minutes.
Model saved to checkpoints/20231208_092602/model_12
Loading model from checkpoints/20231208_092602/model_12
SUB: 16.27%, DEL: 9.48%, INS: 3.02%, COR: 74.25%, PER: 28.76%
