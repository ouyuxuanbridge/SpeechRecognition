Namespace(seed=123, train_json='/rds/user/yo279/hpc-work/MLMI2/fbanks/train_fbank.json', val_json='/rds/user/yo279/hpc-work/MLMI2/fbanks/dev_fbank.json', test_json='/rds/user/yo279/hpc-work/MLMI2/fbanks/test_fbank.json', batch_size=4, num_layers=1, fbank_dims=23, model_dims=128, concat=1, lr=0.5, vocab='vocab_39.txt', report_interval=50, num_epochs=20, dropout_rate=0.5, clip_max_norm=1)
Total number of model parameters is 166952
EPOCH 1:
  batch 50 loss: 4.078926520347595
  batch 100 loss: 3.1571469688415528
  batch 150 loss: 3.025892481803894
  batch 200 loss: 2.90785740852356
  batch 250 loss: 2.832492322921753
  batch 300 loss: 2.6980484199523924
  batch 350 loss: 2.5264759063720703
  batch 400 loss: 2.442958478927612
  batch 450 loss: 2.30784716129303
  batch 500 loss: 2.1766840863227843
  batch 550 loss: 2.1037157583236694
  batch 600 loss: 2.062194650173187
  batch 650 loss: 1.9539001750946046
  batch 700 loss: 1.9732557249069214
  batch 750 loss: 1.8868712258338929
  batch 800 loss: 1.8785325932502746
  batch 850 loss: 1.8036850666999817
  batch 900 loss: 1.7955842590332032
LOSS train 1.79558 valid 1.77763, valid PER 67.79%
EPOCH 2:
  batch 50 loss: 1.7408120107650757
  batch 100 loss: 1.6925298619270324
  batch 150 loss: 1.6500577163696288
  batch 200 loss: 1.6655173540115356
  batch 250 loss: 1.6741215753555299
  batch 300 loss: 1.6174689197540284
  batch 350 loss: 1.5318920135498046
  batch 400 loss: 1.5353037881851197
  batch 450 loss: 1.4801767730712891
  batch 500 loss: 1.5227480983734132
  batch 550 loss: 1.5133387398719789
  batch 600 loss: 1.4442549228668213
  batch 650 loss: 1.4665540528297425
  batch 700 loss: 1.467284607887268
  batch 750 loss: 1.419456911087036
  batch 800 loss: 1.3561961269378662
  batch 850 loss: 1.3904781758785247
  batch 900 loss: 1.421591670513153
LOSS train 1.42159 valid 1.35423, valid PER 42.99%
EPOCH 3:
  batch 50 loss: 1.3328361344337463
  batch 100 loss: 1.3389469373226166
  batch 150 loss: 1.3501875519752502
  batch 200 loss: 1.3077121710777282
  batch 250 loss: 1.278237944841385
  batch 300 loss: 1.2728427958488464
  batch 350 loss: 1.3193970346450805
  batch 400 loss: 1.3181098484992981
  batch 450 loss: 1.2731592392921447
  batch 500 loss: 1.2578124678134919
  batch 550 loss: 1.3129007411003113
  batch 600 loss: 1.2685884726047516
  batch 650 loss: 1.1951762700080872
  batch 700 loss: 1.2484685146808625
  batch 750 loss: 1.3082219922542573
  batch 800 loss: 1.2080768752098083
  batch 850 loss: 1.238223958015442
  batch 900 loss: 1.1814593601226806
LOSS train 1.18146 valid 1.27296, valid PER 38.83%
EPOCH 4:
  batch 50 loss: 1.1640622758865355
  batch 100 loss: 1.2122083854675294
  batch 150 loss: 1.1329084360599517
  batch 200 loss: 1.1675681912899016
  batch 250 loss: 1.1892689883708953
  batch 300 loss: 1.1854150187969208
  batch 350 loss: 1.1159805297851562
  batch 400 loss: 1.1392801260948182
  batch 450 loss: 1.1451359522342681
  batch 500 loss: 1.1350417816638947
  batch 550 loss: 1.1423534011840821
  batch 600 loss: 1.1704101169109344
  batch 650 loss: 1.145103384256363
  batch 700 loss: 1.1245874059200287
  batch 750 loss: 1.0986510527133941
  batch 800 loss: 1.0694600129127503
  batch 850 loss: 1.1311276400089263
  batch 900 loss: 1.1557114112377167
LOSS train 1.15571 valid 1.15022, valid PER 35.32%
EPOCH 5:
  batch 50 loss: 1.0622623980045318
  batch 100 loss: 1.0665976774692536
  batch 150 loss: 1.1033461141586303
  batch 200 loss: 1.0258101034164429
  batch 250 loss: 1.0685033798217773
  batch 300 loss: 1.057404214143753
  batch 350 loss: 1.0864424574375153
  batch 400 loss: 1.067386236190796
  batch 450 loss: 1.0707485353946686
  batch 500 loss: 1.0722672605514527
  batch 550 loss: 1.0203004896640777
  batch 600 loss: 1.0917041611671447
  batch 650 loss: 1.0683520114421845
  batch 700 loss: 1.102384593486786
  batch 750 loss: 1.025242269039154
  batch 800 loss: 1.0552491903305055
  batch 850 loss: 1.0465794169902802
  batch 900 loss: 1.0761357605457307
LOSS train 1.07614 valid 1.08110, valid PER 33.42%
EPOCH 6:
  batch 50 loss: 1.024263082742691
  batch 100 loss: 1.016445050239563
  batch 150 loss: 0.9895813965797424
  batch 200 loss: 1.0148813378810884
  batch 250 loss: 1.0474397897720338
  batch 300 loss: 1.0054276955127717
  batch 350 loss: 1.0021738147735595
  batch 400 loss: 1.0010378563404083
  batch 450 loss: 1.033450973033905
  batch 500 loss: 1.0219155621528626
  batch 550 loss: 1.0365451169013977
  batch 600 loss: 0.9908209812641143
  batch 650 loss: 1.013592051267624
  batch 700 loss: 1.008348948955536
  batch 750 loss: 0.9923255586624146
  batch 800 loss: 0.9941407012939453
  batch 850 loss: 0.9903037250041962
  batch 900 loss: 1.01836940407753
LOSS train 1.01837 valid 1.11389, valid PER 34.07%
EPOCH 7:
  batch 50 loss: 0.9896138322353363
  batch 100 loss: 1.000629404783249
  batch 150 loss: 0.9768798863887787
  batch 200 loss: 0.9447154641151428
  batch 250 loss: 0.9520494949817657
  batch 300 loss: 0.9552438068389892
  batch 350 loss: 0.9530129134654999
  batch 400 loss: 0.9637255990505218
  batch 450 loss: 0.9746279799938202
  batch 500 loss: 0.9450457739830017
  batch 550 loss: 0.9776791357994079
  batch 600 loss: 0.9552899527549744
  batch 650 loss: 0.9539434003829956
  batch 700 loss: 0.9637105417251587
  batch 750 loss: 0.9352939796447753
  batch 800 loss: 0.9493636155128479
  batch 850 loss: 0.9641489791870117
  batch 900 loss: 1.001259560585022
LOSS train 1.00126 valid 1.03882, valid PER 32.20%
EPOCH 8:
  batch 50 loss: 0.933378028869629
  batch 100 loss: 0.9145545732975006
  batch 150 loss: 0.9187844872474671
  batch 200 loss: 0.9057621669769287
  batch 250 loss: 0.9441558468341827
  batch 300 loss: 0.9149438107013702
  batch 350 loss: 0.964596256017685
  batch 400 loss: 0.9297121989727021
  batch 450 loss: 0.9394156014919282
  batch 500 loss: 0.9537880969047546
  batch 550 loss: 0.9206176674365998
  batch 600 loss: 0.9431733667850495
  batch 650 loss: 0.9789435923099518
  batch 700 loss: 0.9255740284919739
  batch 750 loss: 0.9567988574504852
  batch 800 loss: 0.9512604832649231
  batch 850 loss: 0.9476097679138183
  batch 900 loss: 0.980016233921051
LOSS train 0.98002 valid 1.03639, valid PER 31.74%
EPOCH 9:
  batch 50 loss: 0.880328562259674
  batch 100 loss: 0.9146496272087097
  batch 150 loss: 0.9337134790420533
  batch 200 loss: 0.8899847853183747
  batch 250 loss: 0.9223050856590271
  batch 300 loss: 0.9141682171821595
  batch 350 loss: 0.9260791099071503
  batch 400 loss: 0.9021366143226623
  batch 450 loss: 0.9105739450454712
  batch 500 loss: 0.8793178963661193
  batch 550 loss: 0.9191664850711823
  batch 600 loss: 0.9601184844970703
  batch 650 loss: 0.9237924110889435
  batch 700 loss: 0.9202354264259338
  batch 750 loss: 0.9205259215831757
  batch 800 loss: 0.9375663602352142
  batch 850 loss: 0.9139760661125184
  batch 900 loss: 0.8836558854579926
LOSS train 0.88366 valid 0.99077, valid PER 30.66%
EPOCH 10:
  batch 50 loss: 0.83635089635849
  batch 100 loss: 0.8813964521884918
  batch 150 loss: 0.9156278228759765
  batch 200 loss: 0.9057521963119507
  batch 250 loss: 0.8915255546569825
  batch 300 loss: 0.8336045837402344
  batch 350 loss: 0.8825022721290589
  batch 400 loss: 0.8406666076183319
  batch 450 loss: 0.8380413937568665
  batch 500 loss: 0.889565896987915
  batch 550 loss: 0.8919031536579132
  batch 600 loss: 0.9183732390403747
  batch 650 loss: 0.906886647939682
  batch 700 loss: 0.8890561544895172
  batch 750 loss: 0.8705106556415558
  batch 800 loss: 0.8910668671131134
  batch 850 loss: 0.8778092360496521
  batch 900 loss: 0.8818043208122254
LOSS train 0.88180 valid 1.00928, valid PER 31.78%
EPOCH 11:
  batch 50 loss: 0.8316778910160064
  batch 100 loss: 0.8299899244308472
  batch 150 loss: 0.819423326253891
  batch 200 loss: 0.8662508738040924
  batch 250 loss: 0.8666551041603089
  batch 300 loss: 0.8225045001506806
  batch 350 loss: 0.840535826086998
  batch 400 loss: 0.8816811597347259
  batch 450 loss: 0.8512932503223419
  batch 500 loss: 0.837413901090622
  batch 550 loss: 0.8568658435344696
  batch 600 loss: 0.8153012382984162
  batch 650 loss: 0.8884664034843445
  batch 700 loss: 0.8172942721843719
  batch 750 loss: 0.8769615793228149
  batch 800 loss: 0.896695716381073
  batch 850 loss: 0.9154628157615662
  batch 900 loss: 0.8923581671714783
LOSS train 0.89236 valid 0.98173, valid PER 30.49%
EPOCH 12:
  batch 50 loss: 0.8408884876966476
  batch 100 loss: 0.811492178440094
  batch 150 loss: 0.8088669306039811
  batch 200 loss: 0.8168775016069412
  batch 250 loss: 0.828221515417099
  batch 300 loss: 0.8265209603309631
  batch 350 loss: 0.8041237378120423
  batch 400 loss: 0.8487038457393646
  batch 450 loss: 0.8527156019210815
  batch 500 loss: 0.8734198653697968
  batch 550 loss: 0.7824677014350891
  batch 600 loss: 0.8053632199764251
  batch 650 loss: 0.8603562223911285
  batch 700 loss: 0.8665553390979767
  batch 750 loss: 0.8214194571971893
  batch 800 loss: 0.829487932920456
  batch 850 loss: 0.8664933979511261
  batch 900 loss: 0.8853758347034454
LOSS train 0.88538 valid 0.96932, valid PER 29.97%
EPOCH 13:
  batch 50 loss: 0.8027248477935791
  batch 100 loss: 0.8152846455574035
  batch 150 loss: 0.7989776027202606
  batch 200 loss: 0.8368213033676147
  batch 250 loss: 0.8272807836532593
  batch 300 loss: 0.8044630432128906
  batch 350 loss: 0.8133280611038208
  batch 400 loss: 0.8233800685405731
  batch 450 loss: 0.8367090129852295
  batch 500 loss: 0.8267043495178222
  batch 550 loss: 0.8425725257396698
  batch 600 loss: 0.8442008256912231
  batch 650 loss: 0.8279813206195832
  batch 700 loss: 0.8258359360694886
  batch 750 loss: 0.7852498984336853
  batch 800 loss: 0.7957045447826385
  batch 850 loss: 0.8228659617900849
  batch 900 loss: 0.8343547093868255
LOSS train 0.83435 valid 0.95570, valid PER 29.42%
EPOCH 14:
  batch 50 loss: 0.7677599680423737
  batch 100 loss: 0.8119429528713227
  batch 150 loss: 0.8036145555973053
  batch 200 loss: 0.8020092034339905
  batch 250 loss: 0.8116018790006637
  batch 300 loss: 0.8105789804458619
  batch 350 loss: 0.7585378515720368
  batch 400 loss: 0.7623633456230163
  batch 450 loss: 0.8154259526729584
  batch 500 loss: 0.8016883563995362
  batch 550 loss: 0.8293617749214173
  batch 600 loss: 0.8077238082885743
  batch 650 loss: 0.810911796092987
  batch 700 loss: 0.8238385021686554
  batch 750 loss: 0.7897566676139831
  batch 800 loss: 0.7718461966514587
  batch 850 loss: 0.8309067547321319
  batch 900 loss: 0.8073461681604386
LOSS train 0.80735 valid 0.98801, valid PER 29.92%
EPOCH 15:
  batch 50 loss: 0.7480033075809479
  batch 100 loss: 0.7440198761224747
  batch 150 loss: 0.7709656882286072
  batch 200 loss: 0.801752370595932
  batch 250 loss: 0.7881222355365753
  batch 300 loss: 0.7557453906536102
  batch 350 loss: 0.7512338411808014
  batch 400 loss: 0.7506361180543899
  batch 450 loss: 0.7498516434431076
  batch 500 loss: 0.7372570943832397
  batch 550 loss: 0.7807024645805359
  batch 600 loss: 0.7799963188171387
  batch 650 loss: 0.8092770910263062
  batch 700 loss: 0.8281063032150269
  batch 750 loss: 0.7953898429870605
  batch 800 loss: 0.8430319201946258
  batch 850 loss: 0.7820907366275788
  batch 900 loss: 0.7960387563705444
LOSS train 0.79604 valid 0.97714, valid PER 29.95%
EPOCH 16:
  batch 50 loss: 0.781218433380127
  batch 100 loss: 0.7570699536800385
  batch 150 loss: 0.7473884773254394
  batch 200 loss: 0.7506816458702087
  batch 250 loss: 0.7789648342132568
  batch 300 loss: 0.7575451529026032
  batch 350 loss: 0.7731622052192688
  batch 400 loss: 0.776313591003418
  batch 450 loss: 0.7929466307163239
  batch 500 loss: 0.7278791332244873
  batch 550 loss: 0.768849811553955
  batch 600 loss: 0.7571701180934906
  batch 650 loss: 0.7810947799682617
  batch 700 loss: 0.7473151290416717
  batch 750 loss: 0.7649327027797699
  batch 800 loss: 0.7628197211027146
  batch 850 loss: 0.7741879934072494
  batch 900 loss: 0.774611154794693
LOSS train 0.77461 valid 0.95987, valid PER 29.20%
EPOCH 17:
  batch 50 loss: 0.7265107619762421
  batch 100 loss: 0.7295755368471145
  batch 150 loss: 0.712807965874672
  batch 200 loss: 0.7102929019927978
  batch 250 loss: 0.7632722425460815
  batch 300 loss: 0.7563278305530549
  batch 350 loss: 0.7286552381515503
  batch 400 loss: 0.7800320148468017
  batch 450 loss: 0.7508333605527878
  batch 500 loss: 0.7100148493051529
  batch 550 loss: 0.8027416300773621
  batch 600 loss: 0.800531976222992
  batch 650 loss: 0.7573701548576355
  batch 700 loss: 0.7375561189651489
  batch 750 loss: 0.7292590546607971
  batch 800 loss: 0.7526264357566833
  batch 850 loss: 0.7789566445350647
  batch 900 loss: 0.742823988199234
LOSS train 0.74282 valid 0.97064, valid PER 29.30%
EPOCH 18:
  batch 50 loss: 0.7184039878845215
  batch 100 loss: 0.7232797312736511
  batch 150 loss: 0.7552716255187988
  batch 200 loss: 0.7397132623195648
  batch 250 loss: 0.741507762670517
  batch 300 loss: 0.7048777139186859
  batch 350 loss: 0.7165733635425567
  batch 400 loss: 0.7072153437137604
  batch 450 loss: 0.7663165855407715
  batch 500 loss: 0.7361694228649139
  batch 550 loss: 0.7365047025680542
  batch 600 loss: 0.7352962279319764
  batch 650 loss: 0.706542541384697
  batch 700 loss: 0.7453477358818055
  batch 750 loss: 0.736846786737442
  batch 800 loss: 0.7539055573940278
  batch 850 loss: 0.7380522322654725
  batch 900 loss: 0.7513150489330291
LOSS train 0.75132 valid 0.95613, valid PER 29.45%
EPOCH 19:
  batch 50 loss: 0.665457735657692
  batch 100 loss: 0.6517231380939483
  batch 150 loss: 0.6937695747613907
  batch 200 loss: 0.6920541793107986
  batch 250 loss: 0.703066332936287
  batch 300 loss: 0.6976501703262329
  batch 350 loss: 0.7010517138242721
  batch 400 loss: 0.7038832116127014
  batch 450 loss: 0.7237822645902634
  batch 500 loss: 0.740515466928482
  batch 550 loss: 0.7320636177062988
  batch 600 loss: 0.7236602818965912
  batch 650 loss: 0.7864093339443207
  batch 700 loss: 0.7113387972116471
  batch 750 loss: 0.6970388311147689
  batch 800 loss: 0.7289788085222244
  batch 850 loss: 0.7504330468177796
  batch 900 loss: 0.7577249264717102
LOSS train 0.75772 valid 0.96724, valid PER 29.71%
EPOCH 20:
  batch 50 loss: 0.6891697043180466
  batch 100 loss: 0.7023096859455109
  batch 150 loss: 0.6915513664484024
  batch 200 loss: 0.6911109375953675
  batch 250 loss: 0.6885555481910706
  batch 300 loss: 0.718195098042488
  batch 350 loss: 0.6809488481283188
  batch 400 loss: 0.6999367237091064
  batch 450 loss: 0.7185807663202286
  batch 500 loss: 0.6772841769456863
  batch 550 loss: 0.7544630533456802
  batch 600 loss: 0.6871594589948654
  batch 650 loss: 0.7247064310312271
  batch 700 loss: 0.7343901526927948
  batch 750 loss: 0.7107495975494384
  batch 800 loss: 0.7259746944904327
  batch 850 loss: 0.7067308920621872
  batch 900 loss: 0.7175813269615173
LOSS train 0.71758 valid 0.96595, valid PER 28.61%
train_loss
[1.7955842590332032, 1.421591670513153, 1.1814593601226806, 1.1557114112377167, 1.0761357605457307, 1.01836940407753, 1.001259560585022, 0.980016233921051, 0.8836558854579926, 0.8818043208122254, 0.8923581671714783, 0.8853758347034454, 0.8343547093868255, 0.8073461681604386, 0.7960387563705444, 0.774611154794693, 0.742823988199234, 0.7513150489330291, 0.7577249264717102, 0.7175813269615173]
valid_loss
[1.777626872062683, 1.3542289733886719, 1.272963285446167, 1.1502189636230469, 1.081099033355713, 1.1138885021209717, 1.0388202667236328, 1.0363892316818237, 0.9907738566398621, 1.0092780590057373, 0.9817309975624084, 0.969322919845581, 0.9556958675384521, 0.9880121350288391, 0.9771371483802795, 0.9598748087882996, 0.9706355333328247, 0.95612633228302, 0.9672372341156006, 0.9659470319747925]
valid_per
[67.79096120517264, 42.98760165311292, 38.83482202373017, 35.31529129449407, 33.415544594054126, 34.068790827889615, 32.20237301693108, 31.742434342087723, 30.662578322890283, 31.775763231569126, 30.489268097586987, 29.96933742167711, 29.42274363418211, 29.922676976403146, 29.94934008798827, 29.19610718570857, 29.302759632049057, 29.44940674576723, 29.70937208372217, 28.609518730835887]
Training finished in 3.0 minutes.
Model saved to checkpoints/20231207_232655/model_13
Loading model from checkpoints/20231207_232655/model_13
SUB: 17.46%, DEL: 11.09%, INS: 2.64%, COR: 71.44%, PER: 31.20%
