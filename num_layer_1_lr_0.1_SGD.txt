Namespace(seed=123, train_json='/rds/user/yo279/hpc-work/MLMI2/fbanks/train_fbank.json', val_json='/rds/user/yo279/hpc-work/MLMI2/fbanks/dev_fbank.json', test_json='/rds/user/yo279/hpc-work/MLMI2/fbanks/test_fbank.json', batch_size=4, num_layers=1, fbank_dims=23, model_dims=128, concat=1, lr=0.1, vocab='vocab_39.txt', report_interval=50, num_epochs=20, dropout_rate=0.5, clip_max_norm=1)
Total number of model parameters is 166952
EPOCH 1:
  batch 50 loss: 4.703920540809631
  batch 100 loss: 3.2822726345062256
  batch 150 loss: 3.2466491794586183
  batch 200 loss: 3.2242303228378297
  batch 250 loss: 3.1893146085739135
  batch 300 loss: 3.139902210235596
  batch 350 loss: 3.102582154273987
  batch 400 loss: 3.061384611129761
  batch 450 loss: 2.9875805282592776
  batch 500 loss: 2.8845100355148316
  batch 550 loss: 2.8117526721954347
  batch 600 loss: 2.7523003911972044
  batch 650 loss: 2.6787721967697142
  batch 700 loss: 2.652746486663818
  batch 750 loss: 2.597480978965759
  batch 800 loss: 2.5633809566497803
  batch 850 loss: 2.5323817348480224
  batch 900 loss: 2.4788942384719848
LOSS train 2.47889 valid 2.44931, valid PER 81.74%
EPOCH 2:
  batch 50 loss: 2.446539993286133
  batch 100 loss: 2.3895377016067503
  batch 150 loss: 2.3325746774673464
  batch 200 loss: 2.314130439758301
  batch 250 loss: 2.3137725400924682
  batch 300 loss: 2.2833900117874144
  batch 350 loss: 2.2114201021194457
  batch 400 loss: 2.217248630523682
  batch 450 loss: 2.1789314436912535
  batch 500 loss: 2.1604419445991514
  batch 550 loss: 2.1640617299079894
  batch 600 loss: 2.112591950893402
  batch 650 loss: 2.124674298763275
  batch 700 loss: 2.0922934913635256
  batch 750 loss: 2.0884725189208986
  batch 800 loss: 2.032812521457672
  batch 850 loss: 2.017428307533264
  batch 900 loss: 2.0278346824645994
LOSS train 2.02783 valid 1.99840, valid PER 75.24%
EPOCH 3:
  batch 50 loss: 2.0022847032546998
  batch 100 loss: 1.9524312424659729
  batch 150 loss: 1.9635365104675293
  batch 200 loss: 1.9456594967842102
  batch 250 loss: 1.9091394519805909
  batch 300 loss: 1.8962454557418824
  batch 350 loss: 1.9403824067115785
  batch 400 loss: 1.9010852622985839
  batch 450 loss: 1.8355318737030029
  batch 500 loss: 1.845920946598053
  batch 550 loss: 1.823462257385254
  batch 600 loss: 1.7973579812049865
  batch 650 loss: 1.7799708533287049
  batch 700 loss: 1.797234468460083
  batch 750 loss: 1.8217239260673523
  batch 800 loss: 1.748706684112549
  batch 850 loss: 1.7688358306884766
  batch 900 loss: 1.7020184874534607
LOSS train 1.70202 valid 1.77048, valid PER 66.19%
EPOCH 4:
  batch 50 loss: 1.7236291313171386
  batch 100 loss: 1.723633050918579
  batch 150 loss: 1.671721510887146
  batch 200 loss: 1.7091822624206543
  batch 250 loss: 1.689281361103058
  batch 300 loss: 1.682704782485962
  batch 350 loss: 1.6009999561309813
  batch 400 loss: 1.654069333076477
  batch 450 loss: 1.6442170572280883
  batch 500 loss: 1.6078872609138488
  batch 550 loss: 1.620549077987671
  batch 600 loss: 1.6294461965560914
  batch 650 loss: 1.6254261946678161
  batch 700 loss: 1.585700511932373
  batch 750 loss: 1.54502765417099
  batch 800 loss: 1.524869945049286
  batch 850 loss: 1.5552561068534851
  batch 900 loss: 1.5885790753364564
LOSS train 1.58858 valid 1.56324, valid PER 57.33%
EPOCH 5:
  batch 50 loss: 1.5328933572769166
  batch 100 loss: 1.528656361103058
  batch 150 loss: 1.525344455242157
  batch 200 loss: 1.4710024404525757
  batch 250 loss: 1.4831331157684327
  batch 300 loss: 1.4903178429603576
  batch 350 loss: 1.4925011682510376
  batch 400 loss: 1.4781611585617065
  batch 450 loss: 1.4641386246681214
  batch 500 loss: 1.4698132753372193
  batch 550 loss: 1.3894714307785034
  batch 600 loss: 1.4900316309928894
  batch 650 loss: 1.4097267937660218
  batch 700 loss: 1.4679591679573059
  batch 750 loss: 1.3828666722774505
  batch 800 loss: 1.4224099612236023
  batch 850 loss: 1.4197391986846923
  batch 900 loss: 1.4270094323158264
LOSS train 1.42701 valid 1.41728, valid PER 44.65%
EPOCH 6:
  batch 50 loss: 1.4192711043357848
  batch 100 loss: 1.3540696573257447
  batch 150 loss: 1.3623122024536132
  batch 200 loss: 1.3533109784126283
  batch 250 loss: 1.3757978749275208
  batch 300 loss: 1.338305149078369
  batch 350 loss: 1.334295711517334
  batch 400 loss: 1.3237701988220214
  batch 450 loss: 1.3566370034217834
  batch 500 loss: 1.3188543105125428
  batch 550 loss: 1.3386246156692505
  batch 600 loss: 1.3051950228214264
  batch 650 loss: 1.30969997048378
  batch 700 loss: 1.3061218357086182
  batch 750 loss: 1.2897022867202759
  batch 800 loss: 1.2808793210983276
  batch 850 loss: 1.2835252118110656
  batch 900 loss: 1.302626247406006
LOSS train 1.30263 valid 1.31989, valid PER 42.47%
EPOCH 7:
  batch 50 loss: 1.2783494091033936
  batch 100 loss: 1.2832597923278808
  batch 150 loss: 1.2559974384307862
  batch 200 loss: 1.2445009064674377
  batch 250 loss: 1.261047512292862
  batch 300 loss: 1.231675353050232
  batch 350 loss: 1.2353170037269592
  batch 400 loss: 1.2454795825481415
  batch 450 loss: 1.2381429755687714
  batch 500 loss: 1.2264342391490937
  batch 550 loss: 1.2263456320762633
  batch 600 loss: 1.2486121332645417
  batch 650 loss: 1.2217773592472077
  batch 700 loss: 1.2243266808986664
  batch 750 loss: 1.2137928414344787
  batch 800 loss: 1.1997773838043213
  batch 850 loss: 1.2267385709285736
  batch 900 loss: 1.2639243483543396
LOSS train 1.26392 valid 1.23728, valid PER 39.45%
EPOCH 8:
  batch 50 loss: 1.1977450609207154
  batch 100 loss: 1.1953851580619812
  batch 150 loss: 1.1844073617458344
  batch 200 loss: 1.1575997936725617
  batch 250 loss: 1.1975074422359466
  batch 300 loss: 1.12623291015625
  batch 350 loss: 1.1869365215301513
  batch 400 loss: 1.1628388488292694
  batch 450 loss: 1.1719468760490417
  batch 500 loss: 1.2112370872497558
  batch 550 loss: 1.1445170032978058
  batch 600 loss: 1.182863371372223
  batch 650 loss: 1.2066793131828308
  batch 700 loss: 1.146115117073059
  batch 750 loss: 1.1510078287124634
  batch 800 loss: 1.1719953954219817
  batch 850 loss: 1.1816617810726167
  batch 900 loss: 1.1564641141891479
LOSS train 1.15646 valid 1.19505, valid PER 36.86%
EPOCH 9:
  batch 50 loss: 1.0896653032302857
  batch 100 loss: 1.1734414386749268
  batch 150 loss: 1.144852682352066
  batch 200 loss: 1.1070931303501128
  batch 250 loss: 1.131329483985901
  batch 300 loss: 1.1418519556522368
  batch 350 loss: 1.153223353624344
  batch 400 loss: 1.1406747770309449
  batch 450 loss: 1.14408625125885
  batch 500 loss: 1.110904929637909
  batch 550 loss: 1.1463306629657746
  batch 600 loss: 1.1329149913787842
  batch 650 loss: 1.2823193359375
  batch 700 loss: 1.1962924563884736
  batch 750 loss: 1.1865453207492829
  batch 800 loss: 1.1810207235813142
  batch 850 loss: 1.1827494299411774
  batch 900 loss: 1.1145928025245666
LOSS train 1.11459 valid 1.17871, valid PER 36.63%
EPOCH 10:
  batch 50 loss: 1.085954066514969
  batch 100 loss: 1.1078435015678405
  batch 150 loss: 1.1276413893699646
  batch 200 loss: 1.1247380077838898
  batch 250 loss: 1.1262133753299712
  batch 300 loss: 1.0893283188343048
  batch 350 loss: 1.1139083969593049
  batch 400 loss: 1.0762611067295074
  batch 450 loss: 1.0736867260932923
  batch 500 loss: 1.1185535264015198
  batch 550 loss: 1.120849094390869
  batch 600 loss: 1.1082478260993958
  batch 650 loss: 1.0670976674556731
  batch 700 loss: 1.0864869749546051
  batch 750 loss: 1.0923500537872315
  batch 800 loss: 1.1034489679336548
  batch 850 loss: 1.097497832775116
  batch 900 loss: 1.1065158355236053
LOSS train 1.10652 valid 1.14932, valid PER 36.85%
EPOCH 11:
  batch 50 loss: 1.0497834134101867
  batch 100 loss: 1.0389087104797363
  batch 150 loss: 1.0375732862949372
  batch 200 loss: 1.1016328632831573
  batch 250 loss: 1.0712641382217407
  batch 300 loss: 1.0479763114452363
  batch 350 loss: 1.0581475055217744
  batch 400 loss: 1.073744730949402
  batch 450 loss: 1.0705923116207123
  batch 500 loss: 1.0423815059661865
  batch 550 loss: 1.050548392534256
  batch 600 loss: 1.035422121286392
  batch 650 loss: 1.095112875699997
  batch 700 loss: 1.0124211502075195
  batch 750 loss: 1.0260212588310242
  batch 800 loss: 1.08117364525795
  batch 850 loss: 1.0842916893959045
  batch 900 loss: 1.0825448715686798
LOSS train 1.08254 valid 1.09280, valid PER 33.62%
EPOCH 12:
  batch 50 loss: 1.0344985628128052
  batch 100 loss: 1.0223746919631957
  batch 150 loss: 1.0125616884231567
  batch 200 loss: 1.0123808121681213
  batch 250 loss: 1.0521013271808624
  batch 300 loss: 1.0211060702800752
  batch 350 loss: 1.0146319139003754
  batch 400 loss: 1.0563796651363373
  batch 450 loss: 1.0514454448223114
  batch 500 loss: 1.043438616991043
  batch 550 loss: 0.9684934020042419
  batch 600 loss: 0.9942136573791504
  batch 650 loss: 1.0406606328487396
  batch 700 loss: 1.0254466509819031
  batch 750 loss: 1.0112925553321839
  batch 800 loss: 1.0033641493320464
  batch 850 loss: 1.0450436329841615
  batch 900 loss: 1.0459558308124541
LOSS train 1.04596 valid 1.06542, valid PER 33.88%
EPOCH 13:
  batch 50 loss: 0.9715390586853028
  batch 100 loss: 1.0047432470321656
  batch 150 loss: 0.9749481022357941
  batch 200 loss: 0.9930980384349823
  batch 250 loss: 0.9945051181316376
  batch 300 loss: 0.97136181473732
  batch 350 loss: 0.9840774965286255
  batch 400 loss: 1.0192715954780578
  batch 450 loss: 1.0213751995563507
  batch 500 loss: 0.9843448102474213
  batch 550 loss: 1.0024517667293549
  batch 600 loss: 0.9822854208946228
  batch 650 loss: 1.0029033458232879
  batch 700 loss: 1.0010460603237152
  batch 750 loss: 0.9518528366088868
  batch 800 loss: 0.9870947885513306
  batch 850 loss: 1.02420734167099
  batch 900 loss: 1.0036385595798492
LOSS train 1.00364 valid 1.07150, valid PER 33.68%
EPOCH 14:
  batch 50 loss: 0.9825072979927063
  batch 100 loss: 0.9884853100776673
  batch 150 loss: 0.9597197580337524
  batch 200 loss: 0.9717754077911377
  batch 250 loss: 0.9628650975227356
  batch 300 loss: 0.9959222400188446
  batch 350 loss: 0.9448633682727814
  batch 400 loss: 0.9615553116798401
  batch 450 loss: 0.9589621770381928
  batch 500 loss: 0.9744199669361114
  batch 550 loss: 1.0097510027885437
  batch 600 loss: 0.9318933153152466
  batch 650 loss: 0.9755774116516114
  batch 700 loss: 1.0057178139686584
  batch 750 loss: 0.9585665929317474
  batch 800 loss: 0.9354100966453552
  batch 850 loss: 0.9775479567050934
  batch 900 loss: 0.9701334524154663
LOSS train 0.97013 valid 1.04292, valid PER 33.32%
EPOCH 15:
  batch 50 loss: 0.9529312121868133
  batch 100 loss: 0.9328960084915161
  batch 150 loss: 0.9360169076919556
  batch 200 loss: 0.9619554686546326
  batch 250 loss: 0.9570356559753418
  batch 300 loss: 0.923732100725174
  batch 350 loss: 0.9379613256454468
  batch 400 loss: 0.9385409545898438
  batch 450 loss: 0.9423456513881683
  batch 500 loss: 0.9109351444244385
  batch 550 loss: 0.94509641289711
  batch 600 loss: 0.9578184914588929
  batch 650 loss: 0.9820139861106872
  batch 700 loss: 0.9660037469863891
  batch 750 loss: 0.9516529178619385
  batch 800 loss: 0.9413581717014313
  batch 850 loss: 0.9275081670284271
  batch 900 loss: 0.9316509664058685
LOSS train 0.93165 valid 1.02768, valid PER 31.82%
EPOCH 16:
  batch 50 loss: 0.9563514304161072
  batch 100 loss: 0.8953386974334717
  batch 150 loss: 0.90221386551857
  batch 200 loss: 0.9217184817790985
  batch 250 loss: 0.9468226206302642
  batch 300 loss: 0.930087741613388
  batch 350 loss: 0.936136337518692
  batch 400 loss: 0.9301590192317962
  batch 450 loss: 0.9554615795612336
  batch 500 loss: 0.8920524764060974
  batch 550 loss: 0.9457546186447143
  batch 600 loss: 0.9213944995403289
  batch 650 loss: 0.9369694852828979
  batch 700 loss: 0.9038808226585389
  batch 750 loss: 0.9303865242004394
  batch 800 loss: 0.925632871389389
  batch 850 loss: 0.9178161501884461
  batch 900 loss: 0.9038986718654632
LOSS train 0.90390 valid 1.01385, valid PER 31.12%
EPOCH 17:
  batch 50 loss: 0.9098329997062683
  batch 100 loss: 0.9146447110176087
  batch 150 loss: 0.8978414273262024
  batch 200 loss: 0.8917196655273437
  batch 250 loss: 0.9099711084365845
  batch 300 loss: 0.9126405727863312
  batch 350 loss: 0.8794956660270691
  batch 400 loss: 0.9328452634811402
  batch 450 loss: 0.9193299448490143
  batch 500 loss: 0.8900289988517761
  batch 550 loss: 0.9077669680118561
  batch 600 loss: 0.9475063157081604
  batch 650 loss: 0.8816799139976501
  batch 700 loss: 0.9063245153427124
  batch 750 loss: 0.8781155014038086
  batch 800 loss: 0.889778842329979
  batch 850 loss: 0.886275987625122
  batch 900 loss: 0.8845941722393036
LOSS train 0.88459 valid 0.99256, valid PER 30.32%
EPOCH 18:
  batch 50 loss: 0.8836007940769196
  batch 100 loss: 0.8774338662624359
  batch 150 loss: 0.8830663919448852
  batch 200 loss: 0.8975593054294586
  batch 250 loss: 0.89401358127594
  batch 300 loss: 0.8800848615169525
  batch 350 loss: 0.8871450507640839
  batch 400 loss: 0.8673014891147613
  batch 450 loss: 0.9049164927005768
  batch 500 loss: 0.8886757457256317
  batch 550 loss: 0.8855010914802551
  batch 600 loss: 0.8698130118846893
  batch 650 loss: 0.8579792070388794
  batch 700 loss: 0.9073016738891602
  batch 750 loss: 0.8691844975948334
  batch 800 loss: 0.8829584538936615
  batch 850 loss: 0.8671645438671112
  batch 900 loss: 0.9112985372543335
LOSS train 0.91130 valid 1.01687, valid PER 32.55%
EPOCH 19:
  batch 50 loss: 0.8272456884384155
  batch 100 loss: 0.8358383476734161
  batch 150 loss: 0.8623138952255249
  batch 200 loss: 0.866063951253891
  batch 250 loss: 0.8854418969154358
  batch 300 loss: 0.8741052198410034
  batch 350 loss: 0.8625275957584381
  batch 400 loss: 0.8742219960689545
  batch 450 loss: 0.8763962697982788
  batch 500 loss: 0.8781621134281159
  batch 550 loss: 0.8578668129444122
  batch 600 loss: 0.8678896117210388
  batch 650 loss: 0.9138972866535187
  batch 700 loss: 0.855849746465683
  batch 750 loss: 0.8412778759002686
  batch 800 loss: 0.8828823506832123
  batch 850 loss: 0.8834865689277649
  batch 900 loss: 0.8742334711551666
LOSS train 0.87423 valid 1.01259, valid PER 31.31%
EPOCH 20:
  batch 50 loss: 0.8471388745307923
  batch 100 loss: 0.8333545315265656
  batch 150 loss: 0.8330631589889527
  batch 200 loss: 0.8598260509967804
  batch 250 loss: 0.8280964535474777
  batch 300 loss: 0.8621359503269196
  batch 350 loss: 0.834605941772461
  batch 400 loss: 0.826263644695282
  batch 450 loss: 0.8439486145973205
  batch 500 loss: 0.8061815476417542
  batch 550 loss: 0.8907907569408416
  batch 600 loss: 0.831250866651535
  batch 650 loss: 0.8688987386226654
  batch 700 loss: 0.8713719117641449
  batch 750 loss: 0.8359732437133789
  batch 800 loss: 0.8749024927616119
  batch 850 loss: 0.8696989274024963
  batch 900 loss: 0.8810434401035309
LOSS train 0.88104 valid 0.97364, valid PER 29.87%
train_loss
[2.4788942384719848, 2.0278346824645994, 1.7020184874534607, 1.5885790753364564, 1.4270094323158264, 1.302626247406006, 1.2639243483543396, 1.1564641141891479, 1.1145928025245666, 1.1065158355236053, 1.0825448715686798, 1.0459558308124541, 1.0036385595798492, 0.9701334524154663, 0.9316509664058685, 0.9038986718654632, 0.8845941722393036, 0.9112985372543335, 0.8742334711551666, 0.8810434401035309]
valid_loss
[2.4493093490600586, 1.9983986616134644, 1.7704813480377197, 1.5632418394088745, 1.4172776937484741, 1.3198882341384888, 1.2372772693634033, 1.1950501203536987, 1.1787102222442627, 1.149321436882019, 1.092795491218567, 1.0654219388961792, 1.0714980363845825, 1.0429229736328125, 1.0276768207550049, 1.0138468742370605, 0.9925589561462402, 1.016874074935913, 1.012587547302246, 0.9736385941505432]
valid_per
[81.73576856419145, 75.23663511531797, 66.19117451006532, 57.332355685908546, 44.64738034928676, 42.474336755099316, 39.454739368084255, 36.85508598853487, 36.628449540061325, 36.84842021063858, 33.61551793094254, 33.882149046793764, 33.67550993200907, 33.315557925609916, 31.822423676843087, 31.122516997733634, 30.322623650179974, 32.54899346753766, 31.309158778829488, 29.869350753232904]
Training finished in 3.0 minutes.
Model saved to checkpoints/20231208_015819/model_20
Loading model from checkpoints/20231208_015819/model_20
SUB: 16.77%, DEL: 12.40%, INS: 2.39%, COR: 70.83%, PER: 31.56%
