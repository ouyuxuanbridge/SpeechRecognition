Namespace(seed=123, train_json='/rds/user/yo279/hpc-work/MLMI2/fbanks/train_fbank.json', val_json='/rds/user/yo279/hpc-work/MLMI2/fbanks/dev_fbank.json', test_json='/rds/user/yo279/hpc-work/MLMI2/fbanks/test_fbank.json', batch_size=4, num_layers=1, fbank_dims=23, model_dims=128, concat=1, lr=0.5, vocab='vocab_39.txt', report_interval=50, num_epochs=20)
Total number of model parameters is 166952
EPOCH 1:
  batch 50 loss: 4.078943762779236
  batch 100 loss: 3.1571321868896485
  batch 150 loss: 3.0258720207214354
  batch 200 loss: 2.9078861951828
  batch 250 loss: 2.8325089979171754
  batch 300 loss: 2.6998116540908814
  batch 350 loss: 2.526219186782837
  batch 400 loss: 2.434307565689087
  batch 450 loss: 2.429817600250244
  batch 500 loss: 2.200040557384491
  batch 550 loss: 2.1227547264099123
  batch 600 loss: 2.0743041920661924
  batch 650 loss: 1.9583495616912843
  batch 700 loss: 1.9808858251571655
  batch 750 loss: 1.9019553756713867
  batch 800 loss: 1.8801327133178711
  batch 850 loss: 1.8085240745544433
  batch 900 loss: 1.8013047504425048
LOSS train 1.80130 valid 1.79601, valid PER 68.10%
EPOCH 2:
  batch 50 loss: 1.745570378303528
  batch 100 loss: 1.6920379996299744
  batch 150 loss: 1.6593891191482544
  batch 200 loss: 1.672100076675415
  batch 250 loss: 1.6848648643493653
  batch 300 loss: 1.6290304446220398
  batch 350 loss: 1.5417749094963074
  batch 400 loss: 1.546577455997467
  batch 450 loss: 1.487037832736969
  batch 500 loss: 1.5279654169082642
  batch 550 loss: 1.5348679828643799
  batch 600 loss: 1.4628000926971436
  batch 650 loss: 1.486122636795044
  batch 700 loss: 1.4733404874801637
  batch 750 loss: 1.4333934903144836
  batch 800 loss: 1.355558168888092
  batch 850 loss: 1.3679626417160033
  batch 900 loss: 1.4006253933906556
LOSS train 1.40063 valid 1.38010, valid PER 42.57%
EPOCH 3:
  batch 50 loss: 1.341933674812317
  batch 100 loss: 1.3164095544815064
  batch 150 loss: 1.3273020005226135
  batch 200 loss: 1.3064028358459472
  batch 250 loss: 1.288372495174408
  batch 300 loss: 1.2839986491203308
  batch 350 loss: 1.324523320198059
  batch 400 loss: 1.2879467809200287
  batch 450 loss: 1.241770977973938
  batch 500 loss: 1.2347461354732514
  batch 550 loss: 1.2649043118953704
  batch 600 loss: 1.2368862783908845
  batch 650 loss: 1.1841704595088958
  batch 700 loss: 1.2352737069129944
  batch 750 loss: 1.270335327386856
  batch 800 loss: 1.1965659511089326
  batch 850 loss: 1.2378917992115022
  batch 900 loss: 1.180996927022934
LOSS train 1.18100 valid 1.22872, valid PER 37.78%
EPOCH 4:
  batch 50 loss: 1.1543698728084564
  batch 100 loss: 1.1847444248199464
  batch 150 loss: 1.1514085376262664
  batch 200 loss: 1.160716073513031
  batch 250 loss: 1.2025401139259337
  batch 300 loss: 1.175477020740509
  batch 350 loss: 1.0988481295108796
  batch 400 loss: 1.1374212193489075
  batch 450 loss: 1.1293912053108215
  batch 500 loss: 1.1304650676250458
  batch 550 loss: 1.1465271770954133
  batch 600 loss: 1.1626355350017548
  batch 650 loss: 1.1415101385116577
  batch 700 loss: 1.1187517189979552
  batch 750 loss: 1.0911477708816528
  batch 800 loss: 1.0648989856243134
  batch 850 loss: 1.1080761337280274
  batch 900 loss: 1.1700490653514861
LOSS train 1.17005 valid 1.14629, valid PER 35.72%
EPOCH 5:
  batch 50 loss: 1.058957061767578
  batch 100 loss: 1.0806443965435029
  batch 150 loss: 1.111862256526947
  batch 200 loss: 1.035976014137268
  batch 250 loss: 1.060673133134842
  batch 300 loss: 1.0719023859500885
  batch 350 loss: 1.0754295325279235
  batch 400 loss: 1.1113453686237336
  batch 450 loss: 1.0664459645748139
  batch 500 loss: 1.0641802847385406
  batch 550 loss: 1.0235389041900635
  batch 600 loss: 1.1069784450531006
  batch 650 loss: 1.0393312132358552
  batch 700 loss: 1.0804391384124756
  batch 750 loss: 1.008833212852478
  batch 800 loss: 1.0749197173118592
  batch 850 loss: 1.065529307126999
  batch 900 loss: 1.0512434732913971
LOSS train 1.05124 valid 1.09369, valid PER 33.56%
EPOCH 6:
  batch 50 loss: 1.0396267247200013
  batch 100 loss: 1.0020645892620086
  batch 150 loss: 1.0128055930137634
  batch 200 loss: 1.0255559229850768
  batch 250 loss: 1.05060387134552
  batch 300 loss: 1.0036444628238679
  batch 350 loss: 1.0117390632629395
  batch 400 loss: 1.020630021095276
  batch 450 loss: 1.0474738323688506
  batch 500 loss: 1.0376905405521393
  batch 550 loss: 1.0368079602718354
  batch 600 loss: 0.9883929073810578
  batch 650 loss: 1.015118488073349
  batch 700 loss: 0.993170838356018
  batch 750 loss: 1.0014600241184235
  batch 800 loss: 1.005750093460083
  batch 850 loss: 0.9777186942100525
  batch 900 loss: 0.9978353083133698
LOSS train 0.99784 valid 1.07163, valid PER 33.81%
EPOCH 7:
  batch 50 loss: 0.9632551944255829
  batch 100 loss: 0.982077122926712
  batch 150 loss: 0.9772306632995605
  batch 200 loss: 0.9499818015098572
  batch 250 loss: 0.9468821632862091
  batch 300 loss: 0.9493413853645325
  batch 350 loss: 0.9478388178348541
  batch 400 loss: 0.9650258636474609
  batch 450 loss: 0.9774136734008789
  batch 500 loss: 0.954039056301117
  batch 550 loss: 0.9678314912319184
  batch 600 loss: 0.9575637400150299
  batch 650 loss: 0.9752760910987854
  batch 700 loss: 0.9796183502674103
  batch 750 loss: 0.9483454930782318
  batch 800 loss: 0.9434693944454193
  batch 850 loss: 0.9751179504394532
  batch 900 loss: 1.0029480469226837
LOSS train 1.00295 valid 1.04199, valid PER 32.89%
EPOCH 8:
  batch 50 loss: 0.9191830933094025
  batch 100 loss: 0.8961697494983674
  batch 150 loss: 0.9002326536178589
  batch 200 loss: 0.9102716159820556
  batch 250 loss: 0.9434852182865143
  batch 300 loss: 0.891814820766449
  batch 350 loss: 0.9560605764389039
  batch 400 loss: 0.914593939781189
  batch 450 loss: 0.9435711371898651
  batch 500 loss: 0.9639310419559479
  batch 550 loss: 0.8977787077426911
  batch 600 loss: 0.9489695835113525
  batch 650 loss: 0.9690478968620301
  batch 700 loss: 0.9104282963275909
  batch 750 loss: 0.9291185116767884
  batch 800 loss: 0.9570647597312927
  batch 850 loss: 0.9307127952575683
  batch 900 loss: 0.935459212064743
LOSS train 0.93546 valid 1.02799, valid PER 31.23%
EPOCH 9:
  batch 50 loss: 0.8558887040615082
  batch 100 loss: 0.8953809762001037
  batch 150 loss: 0.9036813187599182
  batch 200 loss: 0.8843371546268464
  batch 250 loss: 0.9252753150463104
  batch 300 loss: 0.9006090009212494
  batch 350 loss: 0.922508989572525
  batch 400 loss: 0.8948715972900391
  batch 450 loss: 0.897399582862854
  batch 500 loss: 0.8737009000778199
  batch 550 loss: 0.9210960900783539
  batch 600 loss: 0.9565077960491181
  batch 650 loss: 0.9097501623630524
  batch 700 loss: 0.8856882345676422
  batch 750 loss: 0.9055529344081878
  batch 800 loss: 0.9305608856678009
  batch 850 loss: 0.894265033006668
  batch 900 loss: 0.8928932690620422
LOSS train 0.89289 valid 1.00750, valid PER 31.38%
EPOCH 10:
  batch 50 loss: 0.8316688334941864
  batch 100 loss: 0.8645145690441132
  batch 150 loss: 0.879668732881546
  batch 200 loss: 0.8813282239437104
  batch 250 loss: 0.8792066633701324
  batch 300 loss: 0.8410667991638183
  batch 350 loss: 0.888689661026001
  batch 400 loss: 0.8306013989448547
  batch 450 loss: 0.8424507999420165
  batch 500 loss: 0.8725507974624633
  batch 550 loss: 0.8914448320865631
  batch 600 loss: 0.8666479611396789
  batch 650 loss: 0.8564347851276398
  batch 700 loss: 0.8783481764793396
  batch 750 loss: 0.8571231973171234
  batch 800 loss: 0.885351231098175
  batch 850 loss: 0.8895510613918305
  batch 900 loss: 0.870076699256897
LOSS train 0.87008 valid 1.01741, valid PER 32.03%
EPOCH 11:
  batch 50 loss: 0.8180891764163971
  batch 100 loss: 0.7811487281322479
  batch 150 loss: 0.809145599603653
  batch 200 loss: 0.8461705750226974
  batch 250 loss: 0.8369175660610199
  batch 300 loss: 0.8146686255931854
  batch 350 loss: 0.8377593696117401
  batch 400 loss: 0.8518781650066376
  batch 450 loss: 0.837329478263855
  batch 500 loss: 0.812884167432785
  batch 550 loss: 0.8373088574409485
  batch 600 loss: 0.824491320848465
  batch 650 loss: 0.8894453155994415
  batch 700 loss: 0.8207483530044556
  batch 750 loss: 0.8669915676116944
  batch 800 loss: 0.8899959790706634
  batch 850 loss: 0.8882749152183532
  batch 900 loss: 0.8895666289329529
LOSS train 0.88957 valid 0.99646, valid PER 30.86%
EPOCH 12:
  batch 50 loss: 0.8248946118354797
  batch 100 loss: 0.7969606077671051
  batch 150 loss: 0.8118045914173126
  batch 200 loss: 0.8088487720489502
  batch 250 loss: 0.857335364818573
  batch 300 loss: 0.8449012815952301
  batch 350 loss: 0.8447988426685333
  batch 400 loss: 0.8668680620193482
  batch 450 loss: 0.8785792541503906
  batch 500 loss: 0.8861127424240113
  batch 550 loss: 0.795318775177002
  batch 600 loss: 0.8209146404266358
  batch 650 loss: 0.8522785115242004
  batch 700 loss: 0.8601735210418702
  batch 750 loss: 0.8182618391513824
  batch 800 loss: 0.8288929826021194
  batch 850 loss: 0.8498428940773011
  batch 900 loss: 0.8542588758468628
LOSS train 0.85426 valid 0.98519, valid PER 30.79%
EPOCH 13:
  batch 50 loss: 0.783049635887146
  batch 100 loss: 0.7894373333454132
  batch 150 loss: 0.7758425956964493
  batch 200 loss: 0.802658595442772
  batch 250 loss: 0.7965207141637802
  batch 300 loss: 0.7974749803543091
  batch 350 loss: 0.8088870692253113
  batch 400 loss: 0.7935839641094208
  batch 450 loss: 0.8084565246105194
  batch 500 loss: 0.7709267687797546
  batch 550 loss: 0.8211878424882889
  batch 600 loss: 0.8105007541179657
  batch 650 loss: 0.8209575140476226
  batch 700 loss: 0.8271089339256287
  batch 750 loss: 0.7978269672393798
  batch 800 loss: 0.8011167287826538
  batch 850 loss: 0.8186684310436249
  batch 900 loss: 0.8215502536296845
LOSS train 0.82155 valid 0.98721, valid PER 30.31%
EPOCH 14:
  batch 50 loss: 0.7843898916244507
  batch 100 loss: 0.7721283566951752
  batch 150 loss: 0.7806565630435943
  batch 200 loss: 0.7730887567996979
  batch 250 loss: 0.7785408800840378
  batch 300 loss: 0.8161830234527588
  batch 350 loss: 0.7304319328069687
  batch 400 loss: 0.7457732379436492
  batch 450 loss: 0.7522704970836639
  batch 500 loss: 0.7773571765422821
  batch 550 loss: 0.8053686237335205
  batch 600 loss: 0.7466938841342926
  batch 650 loss: 0.8216555172204971
  batch 700 loss: 0.8114474761486054
  batch 750 loss: 0.7728274464607239
  batch 800 loss: 0.7425578224658966
  batch 850 loss: 0.794244931936264
  batch 900 loss: 0.8211252248287201
LOSS train 0.82113 valid 1.01540, valid PER 31.10%
EPOCH 15:
  batch 50 loss: 0.8108458077907562
  batch 100 loss: 0.7665863931179047
  batch 150 loss: 0.7564220547676086
  batch 200 loss: 0.7902416729927063
  batch 250 loss: 0.7968857920169831
  batch 300 loss: 0.7477299547195435
  batch 350 loss: 0.7692119801044464
  batch 400 loss: 0.7574981570243835
  batch 450 loss: 0.7759463167190552
  batch 500 loss: 0.7563451313972473
  batch 550 loss: 0.7604309666156769
  batch 600 loss: 0.8087831258773803
  batch 650 loss: 0.8082401895523071
  batch 700 loss: 0.799481475353241
  batch 750 loss: 0.7949040520191193
  batch 800 loss: 0.7601090049743653
  batch 850 loss: 0.7352475035190582
  batch 900 loss: 0.765103662610054
LOSS train 0.76510 valid 0.98447, valid PER 30.66%
EPOCH 16:
  batch 50 loss: 0.739323593378067
  batch 100 loss: 0.7694558644294739
  batch 150 loss: 0.7308691602945327
  batch 200 loss: 0.7375960266590118
  batch 250 loss: 0.7462345409393311
  batch 300 loss: 0.7569617974758148
  batch 350 loss: 0.774959510564804
  batch 400 loss: 0.7661345368623733
  batch 450 loss: 0.7919739490747452
  batch 500 loss: 0.7237777376174926
  batch 550 loss: 0.7721259295940399
  batch 600 loss: 0.7580150216817856
  batch 650 loss: 0.7764190399646759
  batch 700 loss: 0.7399900722503662
  batch 750 loss: 0.7636131811141967
  batch 800 loss: 0.7748317170143127
  batch 850 loss: 0.7733892571926116
  batch 900 loss: 0.7757453727722168
LOSS train 0.77575 valid 0.97366, valid PER 29.58%
EPOCH 17:
  batch 50 loss: 0.7370106792449951
  batch 100 loss: 0.7448555755615235
  batch 150 loss: 0.7153727275133133
  batch 200 loss: 0.7099957698583603
  batch 250 loss: 0.7463562142848968
  batch 300 loss: 0.747474594116211
  batch 350 loss: 0.7184518814086914
  batch 400 loss: 0.7531191372871399
  batch 450 loss: 0.7601771450042725
  batch 500 loss: 0.7037602072954178
  batch 550 loss: 0.7361459052562713
  batch 600 loss: 0.7756146496534347
  batch 650 loss: 0.7479278552532196
  batch 700 loss: 0.71099176466465
  batch 750 loss: 0.7150064945220947
  batch 800 loss: 0.7307249891757965
  batch 850 loss: 0.7331522643566132
  batch 900 loss: 0.7064933788776397
LOSS train 0.70649 valid 0.99888, valid PER 29.49%
EPOCH 18:
  batch 50 loss: 0.756730819940567
  batch 100 loss: 0.7436548721790314
  batch 150 loss: 0.727756724357605
  batch 200 loss: 0.7196811217069626
  batch 250 loss: 0.7310366141796112
  batch 300 loss: 0.7045036089420319
  batch 350 loss: 0.7798984956741333
  batch 400 loss: 0.7113385713100433
  batch 450 loss: 0.7442607235908508
  batch 500 loss: 0.7293890327215194
  batch 550 loss: 0.7351791179180145
  batch 600 loss: 0.728158347606659
  batch 650 loss: 0.6976431596279145
  batch 700 loss: 0.7485653901100159
  batch 750 loss: 0.7293365222215652
  batch 800 loss: 0.7164281070232391
  batch 850 loss: 0.718915730714798
  batch 900 loss: 0.7613223886489868
LOSS train 0.76132 valid 1.17497, valid PER 33.77%
EPOCH 19:
  batch 50 loss: 0.7427238142490387
  batch 100 loss: 0.7180563044548035
  batch 150 loss: 0.7242467653751373
  batch 200 loss: 0.7269575488567352
  batch 250 loss: 0.7476450729370118
  batch 300 loss: 0.7779999017715454
  batch 350 loss: 0.761416722536087
  batch 400 loss: 0.7650097525119781
  batch 450 loss: 0.7790522825717926
  batch 500 loss: 0.7686325311660767
  batch 550 loss: 0.7292043375968933
  batch 600 loss: 0.7284154427051545
  batch 650 loss: 0.7913728058338165
  batch 700 loss: 0.716091992855072
  batch 750 loss: 0.7195106202363968
  batch 800 loss: 0.7419183218479156
  batch 850 loss: 0.7463589024543762
  batch 900 loss: 0.7221637666225433
LOSS train 0.72216 valid 0.97161, valid PER 29.44%
EPOCH 20:
  batch 50 loss: 0.6799761140346527
  batch 100 loss: 0.6794164752960206
  batch 150 loss: 0.6698566752672196
  batch 200 loss: 0.6891478204727173
  batch 250 loss: 0.6867063021659852
  batch 300 loss: 0.722925797700882
  batch 350 loss: 0.67257847905159
  batch 400 loss: 0.7097172790765762
  batch 450 loss: 0.7005736690759659
  batch 500 loss: 0.6705684649944306
  batch 550 loss: 0.7646462154388428
  batch 600 loss: 0.6918854808807373
  batch 650 loss: 0.7125376451015473
  batch 700 loss: 0.7131268620491028
  batch 750 loss: 0.6872373563051224
  batch 800 loss: 0.7961183416843415
  batch 850 loss: 0.7401591598987579
  batch 900 loss: 0.730892403125763
LOSS train 0.73089 valid 0.99443, valid PER 29.56%
Training finished in 4.0 minutes.
Model saved to checkpoints/20231205_153150/model_19
Loading model from checkpoints/20231205_153150/model_19
SUB: 17.16%, DEL: 10.89%, INS: 2.79%, COR: 71.96%, PER: 30.84%
