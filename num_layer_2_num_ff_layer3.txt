Namespace(seed=123, train_json='/rds/user/yo279/hpc-work/MLMI2/fbanks/train_fbank.json', val_json='/rds/user/yo279/hpc-work/MLMI2/fbanks/dev_fbank.json', test_json='/rds/user/yo279/hpc-work/MLMI2/fbanks/test_fbank.json', batch_size=4, num_layers=2, fbank_dims=23, model_dims=128, concat=1, lr=0.5, vocab='vocab_39.txt', report_interval=50, num_epochs=20, dropout_rate=0.5, clip_max_norm=1, num_ff_layers=3)
Total number of model parameters is 565496
EPOCH 1:
  batch 50 loss: 4.436395540237426
  batch 100 loss: 3.3686876678466797
  batch 150 loss: 3.345472846031189
  batch 200 loss: 3.318715753555298
  batch 250 loss: 3.3236296129226686
  batch 300 loss: 3.2821795749664306
  batch 350 loss: 3.268801255226135
  batch 400 loss: 3.2589302825927735
  batch 450 loss: 3.2400714683532716
  batch 500 loss: 3.2097508478164674
  batch 550 loss: 3.130877594947815
  batch 600 loss: 3.0911331605911254
  batch 650 loss: 3.159457883834839
  batch 700 loss: 3.0110861253738403
  batch 750 loss: 2.9472078466415406
  batch 800 loss: 2.8833278036117553
  batch 850 loss: 2.8445966339111326
  batch 900 loss: 3.1113750743865967
LOSS train 3.11138 valid 2.76349, valid PER 79.55%
EPOCH 2:
  batch 50 loss: 2.7838245248794555
  batch 100 loss: 2.706208872795105
  batch 150 loss: 2.661887149810791
  batch 200 loss: 2.6233319568634035
  batch 250 loss: 2.5876282119750975
  batch 300 loss: 2.5226062631607054
  batch 350 loss: 2.4481715822219847
  batch 400 loss: 2.460223217010498
  batch 450 loss: 2.3155004501342775
  batch 500 loss: 2.2728299164772032
  batch 550 loss: 2.298688271045685
  batch 600 loss: 2.200766313076019
  batch 650 loss: 2.219215779304504
  batch 700 loss: 2.1425081634521486
  batch 750 loss: 2.1669894218444825
  batch 800 loss: 2.0650625610351563
  batch 850 loss: 2.0267704200744627
  batch 900 loss: 2.033365452289581
LOSS train 2.03337 valid 1.89339, valid PER 59.93%
EPOCH 3:
  batch 50 loss: 1.9871895122528076
  batch 100 loss: 1.9008332514762878
  batch 150 loss: 2.026682379245758
  batch 200 loss: 1.8764217352867127
  batch 250 loss: 1.80221284866333
  batch 300 loss: 1.8284256649017334
  batch 350 loss: 1.8467410731315612
  batch 400 loss: 1.795730631351471
  batch 450 loss: 1.8115000414848328
  batch 500 loss: 1.7348251223564148
  batch 550 loss: 1.6638805842399598
  batch 600 loss: 1.6714510416984558
  batch 650 loss: 1.5977224373817445
  batch 700 loss: 1.616508858203888
  batch 750 loss: 1.650998170375824
  batch 800 loss: 1.5907997465133668
  batch 850 loss: 1.5870214343070983
  batch 900 loss: 1.489318859577179
LOSS train 1.48932 valid 1.59559, valid PER 49.57%
EPOCH 4:
  batch 50 loss: 1.5004599475860596
  batch 100 loss: 1.5436754894256592
  batch 150 loss: 1.3956060194969178
  batch 200 loss: 1.4614734983444213
  batch 250 loss: 1.433491780757904
  batch 300 loss: 1.4779179525375366
  batch 350 loss: 1.3064973187446594
  batch 400 loss: 1.4427484273910522
  batch 450 loss: 1.3920256423950195
  batch 500 loss: 1.3354187858104707
  batch 550 loss: 1.320076563358307
  batch 600 loss: 1.4004502892494202
  batch 650 loss: 1.3931276631355285
  batch 700 loss: 1.307152099609375
  batch 750 loss: 1.280312695503235
  batch 800 loss: 1.2645954608917236
  batch 850 loss: 1.2602459144592286
  batch 900 loss: 1.3384723341464997
LOSS train 1.33847 valid 1.24628, valid PER 38.43%
EPOCH 5:
  batch 50 loss: 1.2413292360305785
  batch 100 loss: 1.207799723148346
  batch 150 loss: 1.2528680968284607
  batch 200 loss: 1.1575539445877074
  batch 250 loss: 1.2038311016559602
  batch 300 loss: 1.192088804244995
  batch 350 loss: 1.213095610141754
  batch 400 loss: 1.2480491554737092
  batch 450 loss: 1.228477576971054
  batch 500 loss: 1.2076497709751128
  batch 550 loss: 1.1229822027683258
  batch 600 loss: 1.2387635505199432
  batch 650 loss: 1.1526949310302734
  batch 700 loss: 1.1968872034549713
  batch 750 loss: 1.1268340849876404
  batch 800 loss: 1.1511878764629364
  batch 850 loss: 1.1440573513507843
  batch 900 loss: 1.1761363935470581
LOSS train 1.17614 valid 1.15783, valid PER 35.26%
EPOCH 6:
  batch 50 loss: 1.1810459172725678
  batch 100 loss: 1.07804793715477
  batch 150 loss: 1.1165107989311218
  batch 200 loss: 1.1015042817592622
  batch 250 loss: 1.150423356294632
  batch 300 loss: 1.1182733750343323
  batch 350 loss: 1.0871170675754547
  batch 400 loss: 1.0904446756839752
  batch 450 loss: 1.1476571333408356
  batch 500 loss: 1.1149775755405427
  batch 550 loss: 1.1154350602626801
  batch 600 loss: 1.0836414682865143
  batch 650 loss: 1.11508984208107
  batch 700 loss: 1.0852975642681122
  batch 750 loss: 1.0608457493782044
  batch 800 loss: 1.0692830491065979
  batch 850 loss: 1.0634684455394745
  batch 900 loss: 1.0698820400238036
LOSS train 1.06988 valid 1.11746, valid PER 33.05%
EPOCH 7:
  batch 50 loss: 1.060476392507553
  batch 100 loss: 1.0777513921260833
  batch 150 loss: 1.0419172906875611
  batch 200 loss: 1.0766006624698639
  batch 250 loss: 1.062973302602768
  batch 300 loss: 1.0086084580421448
  batch 350 loss: 1.048812325000763
  batch 400 loss: 1.0506216430664062
  batch 450 loss: 1.0187670004367828
  batch 500 loss: 1.0260672426223756
  batch 550 loss: 1.030528107881546
  batch 600 loss: 1.0195607769489288
  batch 650 loss: 1.0168800723552704
  batch 700 loss: 1.05681631565094
  batch 750 loss: 0.9781978952884675
  batch 800 loss: 0.9922574532032012
  batch 850 loss: 1.0405817246437072
  batch 900 loss: 1.0410057556629182
LOSS train 1.04101 valid 1.03468, valid PER 32.30%
EPOCH 8:
  batch 50 loss: 0.9863260781764984
  batch 100 loss: 0.9655038404464722
  batch 150 loss: 0.946501179933548
  batch 200 loss: 0.9400032436847687
  batch 250 loss: 0.9577207970619201
  batch 300 loss: 0.9388608860969544
  batch 350 loss: 1.0002197527885437
  batch 400 loss: 0.9429588782787323
  batch 450 loss: 0.9772877430915833
  batch 500 loss: 0.9970766127109527
  batch 550 loss: 0.9310572421550751
  batch 600 loss: 0.9692357778549194
  batch 650 loss: 0.9774460697174072
  batch 700 loss: 0.9523592627048493
  batch 750 loss: 0.9572389817237854
  batch 800 loss: 0.9599281573295593
  batch 850 loss: 0.9524708545207977
  batch 900 loss: 0.9846583259105682
LOSS train 0.98466 valid 0.98657, valid PER 29.08%
EPOCH 9:
  batch 50 loss: 0.8599443519115448
  batch 100 loss: 0.9196577382087707
  batch 150 loss: 0.9252981472015381
  batch 200 loss: 0.8949085021018982
  batch 250 loss: 0.9259760260581971
  batch 300 loss: 0.9229107069969177
  batch 350 loss: 0.9240452682971955
  batch 400 loss: 0.9274099111557007
  batch 450 loss: 0.8993914866447449
  batch 500 loss: 0.8789404809474946
  batch 550 loss: 0.9163140904903412
  batch 600 loss: 0.9483638393878937
  batch 650 loss: 0.903134628534317
  batch 700 loss: 0.9276601421833038
  batch 750 loss: 0.9213494181632995
  batch 800 loss: 0.9712178385257721
  batch 850 loss: 0.9800550103187561
  batch 900 loss: 0.9139964270591736
LOSS train 0.91400 valid 0.96596, valid PER 29.26%
EPOCH 10:
  batch 50 loss: 0.8483249115943908
  batch 100 loss: 0.8518378400802612
  batch 150 loss: 0.8789388406276702
  batch 200 loss: 0.8803166854381561
  batch 250 loss: 0.8965807950496674
  batch 300 loss: 0.8715459215641022
  batch 350 loss: 0.9332896029949188
  batch 400 loss: 0.8950110590457916
  batch 450 loss: 0.8996995615959168
  batch 500 loss: 0.923683340549469
  batch 550 loss: 0.9285964405536652
  batch 600 loss: 0.9141027188301086
  batch 650 loss: 0.8782303607463837
  batch 700 loss: 0.8655772316455841
  batch 750 loss: 0.8603346168994903
  batch 800 loss: 0.8711516988277436
  batch 850 loss: 0.8901372814178466
  batch 900 loss: 0.8687304449081421
LOSS train 0.86873 valid 0.95653, valid PER 29.58%
EPOCH 11:
  batch 50 loss: 0.8256314504146576
  batch 100 loss: 0.8028536307811737
  batch 150 loss: 0.8007669734954834
  batch 200 loss: 0.8824297118186951
  batch 250 loss: 0.8714249449968338
  batch 300 loss: 0.8436784756183624
  batch 350 loss: 0.8384758186340332
  batch 400 loss: 0.8721220362186431
  batch 450 loss: 0.8597845304012298
  batch 500 loss: 0.8119457292556763
  batch 550 loss: 0.8286749398708344
  batch 600 loss: 0.8151077878475189
  batch 650 loss: 0.8983523285388947
  batch 700 loss: 0.8139596974849701
  batch 750 loss: 0.8263337409496307
  batch 800 loss: 0.8612976968288422
  batch 850 loss: 0.9438670361042023
  batch 900 loss: 0.8957238161563873
LOSS train 0.89572 valid 0.94681, valid PER 28.48%
EPOCH 12:
  batch 50 loss: 0.8266050910949707
  batch 100 loss: 0.8345684742927552
  batch 150 loss: 0.7779849553108216
  batch 200 loss: 0.8128776282072068
  batch 250 loss: 0.8378752827644348
  batch 300 loss: 0.8115681064128876
  batch 350 loss: 0.8026258826255799
  batch 400 loss: 0.8121867847442626
  batch 450 loss: 0.8513199806213378
  batch 500 loss: 0.8746308064460755
  batch 550 loss: 0.7924367272853852
  batch 600 loss: 0.8241158413887024
  batch 650 loss: 0.8796617782115936
  batch 700 loss: 0.8379955816268921
  batch 750 loss: 0.8037301814556121
  batch 800 loss: 0.7957260084152221
  batch 850 loss: 0.8516462075710297
  batch 900 loss: 0.8449415546655655
LOSS train 0.84494 valid 0.87857, valid PER 26.70%
EPOCH 13:
  batch 50 loss: 0.7620036208629608
  batch 100 loss: 0.8041223657131195
  batch 150 loss: 0.7832946753501893
  batch 200 loss: 0.912183917760849
  batch 250 loss: 0.8527210891246796
  batch 300 loss: 0.809557004570961
  batch 350 loss: 0.808388249874115
  batch 400 loss: 0.903933584690094
  batch 450 loss: 0.845585607290268
  batch 500 loss: 0.8266130232810974
  batch 550 loss: 0.8598667430877686
  batch 600 loss: 0.8098823273181915
  batch 650 loss: 0.862148631811142
  batch 700 loss: 0.8210927391052246
  batch 750 loss: 0.7612584722042084
  batch 800 loss: 0.7817496633529664
  batch 850 loss: 0.8307284212112427
  batch 900 loss: 0.7941200256347656
LOSS train 0.79412 valid 0.92195, valid PER 27.13%
EPOCH 14:
  batch 50 loss: 0.7424424970149994
  batch 100 loss: 0.7789723002910613
  batch 150 loss: 0.7837544071674347
  batch 200 loss: 0.7346566766500473
  batch 250 loss: 0.7385371112823487
  batch 300 loss: 0.7881409430503845
  batch 350 loss: 0.7249426341056824
  batch 400 loss: 0.7558453047275543
  batch 450 loss: 0.7840991330146789
  batch 500 loss: 0.7952454280853272
  batch 550 loss: 0.7865579843521118
  batch 600 loss: 0.7772732192277908
  batch 650 loss: 0.8110499048233032
  batch 700 loss: 0.8122711896896362
  batch 750 loss: 0.7892217123508454
  batch 800 loss: 0.7587257838249206
  batch 850 loss: 0.803556832075119
  batch 900 loss: 0.8358220839500428
LOSS train 0.83582 valid 0.92165, valid PER 28.54%
EPOCH 15:
  batch 50 loss: 0.7590588867664337
  batch 100 loss: 0.7315541744232178
  batch 150 loss: 0.7049833869934082
  batch 200 loss: 0.7456026828289032
  batch 250 loss: 0.7327568876743317
  batch 300 loss: 0.7465004819631577
  batch 350 loss: 0.7021267586946487
  batch 400 loss: 0.721508514881134
  batch 450 loss: 0.741619234085083
  batch 500 loss: 0.6991054737567901
  batch 550 loss: 0.732708483338356
  batch 600 loss: 0.7523812109231949
  batch 650 loss: 0.7573922657966614
  batch 700 loss: 0.7301860839128494
  batch 750 loss: 0.7616570723056794
  batch 800 loss: 0.8029364800453186
  batch 850 loss: 0.8101670479774475
  batch 900 loss: 0.7799006497859955
LOSS train 0.77990 valid 0.93268, valid PER 28.16%
EPOCH 16:
  batch 50 loss: 0.7377219748497009
  batch 100 loss: 0.7221555316448212
  batch 150 loss: 0.7505320221185684
  batch 200 loss: 0.7558269894123077
  batch 250 loss: 0.7371752977371215
  batch 300 loss: 0.7033477640151977
  batch 350 loss: 0.7125316971540451
  batch 400 loss: 0.7014531219005584
  batch 450 loss: 0.7335209673643113
  batch 500 loss: 0.6741651892662048
  batch 550 loss: 0.6913696962594986
  batch 600 loss: 0.695734156370163
  batch 650 loss: 0.6983461391925812
  batch 700 loss: 0.689887969493866
  batch 750 loss: 0.7285575896501542
  batch 800 loss: 0.7226767474412918
  batch 850 loss: 0.7028127723932266
  batch 900 loss: 0.6906534457206726
LOSS train 0.69065 valid 0.88603, valid PER 25.69%
EPOCH 17:
  batch 50 loss: 0.672895433306694
  batch 100 loss: 0.6664020389318466
  batch 150 loss: 0.6649893373250961
  batch 200 loss: 0.6806257086992263
  batch 250 loss: 0.6994374018907547
  batch 300 loss: 0.6914145576953888
  batch 350 loss: 0.6699065017700195
  batch 400 loss: 0.7380556970834732
  batch 450 loss: 0.6945788180828094
  batch 500 loss: 0.6982493335008622
  batch 550 loss: 0.6837420153617859
  batch 600 loss: 0.7642437666654587
  batch 650 loss: 0.7535286283493042
  batch 700 loss: 0.7365782654285431
  batch 750 loss: 0.7156725555658341
  batch 800 loss: 0.6986195474863053
  batch 850 loss: 0.7230909079313278
  batch 900 loss: 0.68085607111454
LOSS train 0.68086 valid 0.86994, valid PER 25.29%
EPOCH 18:
  batch 50 loss: 0.6366286617517471
  batch 100 loss: 0.6807162880897522
  batch 150 loss: 0.6767777454853058
  batch 200 loss: 0.6710679501295089
  batch 250 loss: 0.6450922310352325
  batch 300 loss: 0.6265569180250168
  batch 350 loss: 0.6761479592323303
  batch 400 loss: 0.6283458572626114
  batch 450 loss: 0.6879877483844757
  batch 500 loss: 0.6779568648338318
  batch 550 loss: 0.6861439770460129
  batch 600 loss: 0.655409688949585
  batch 650 loss: 0.6636680859327316
  batch 700 loss: 0.7223987364768982
  batch 750 loss: 0.6967540550231933
  batch 800 loss: 0.6545292937755585
  batch 850 loss: 0.6629739201068878
  batch 900 loss: 0.7040678882598876
LOSS train 0.70407 valid 0.86288, valid PER 25.62%
EPOCH 19:
  batch 50 loss: 0.5943047040700913
  batch 100 loss: 0.6033601832389831
  batch 150 loss: 0.6154205703735351
  batch 200 loss: 0.6415790838003158
  batch 250 loss: 0.6347865986824036
  batch 300 loss: 0.6208037483692169
  batch 350 loss: 0.6261618220806122
  batch 400 loss: 0.627877013683319
  batch 450 loss: 0.6636018496751785
  batch 500 loss: 0.6396078586578369
  batch 550 loss: 0.6068200260400772
  batch 600 loss: 0.6260943478345871
  batch 650 loss: 0.70637233376503
  batch 700 loss: 0.6234683352708816
  batch 750 loss: 0.6089744424819946
  batch 800 loss: 0.6370717710256577
  batch 850 loss: 0.6231356644630432
  batch 900 loss: 0.6768639302253723
LOSS train 0.67686 valid 0.88596, valid PER 27.70%
EPOCH 20:
  batch 50 loss: 0.575535352230072
  batch 100 loss: 0.5475893974304199
  batch 150 loss: 0.6041466999053955
  batch 200 loss: 0.6209748077392578
  batch 250 loss: 0.5789392274618149
  batch 300 loss: 0.619787090420723
  batch 350 loss: 0.57610684633255
  batch 400 loss: 0.6153041690587997
  batch 450 loss: 0.620161954164505
  batch 500 loss: 0.5934475553035736
  batch 550 loss: 0.6681892800331116
  batch 600 loss: 0.6212037527561187
  batch 650 loss: 0.6611562067270279
  batch 700 loss: 0.6493069952726365
  batch 750 loss: 0.5860243910551071
  batch 800 loss: 0.6231889057159424
  batch 850 loss: 0.64046491086483
  batch 900 loss: 0.6228965920209885
LOSS train 0.62290 valid 0.90720, valid PER 25.76%
train_loss
[3.1113750743865967, 2.033365452289581, 1.489318859577179, 1.3384723341464997, 1.1761363935470581, 1.0698820400238036, 1.0410057556629182, 0.9846583259105682, 0.9139964270591736, 0.8687304449081421, 0.8957238161563873, 0.8449415546655655, 0.7941200256347656, 0.8358220839500428, 0.7799006497859955, 0.6906534457206726, 0.68085607111454, 0.7040678882598876, 0.6768639302253723, 0.6228965920209885]
valid_loss
[2.7634923458099365, 1.8933892250061035, 1.5955853462219238, 1.2462786436080933, 1.157829761505127, 1.117463231086731, 1.0346826314926147, 0.9865717887878418, 0.965959906578064, 0.9565313458442688, 0.946812629699707, 0.8785731196403503, 0.9219539165496826, 0.9216505289077759, 0.9326805472373962, 0.8860347270965576, 0.8699418306350708, 0.8628846406936646, 0.885962724685669, 0.9071987867355347]
valid_per
[79.54939341421144, 59.92534328756166, 49.566724436741765, 38.42820957205706, 35.25529929342754, 33.0489268097587, 32.30235968537528, 29.082788961471806, 29.26276496467138, 29.576056525796563, 28.482868950806562, 26.703106252499666, 27.129716037861616, 28.542860951873084, 28.156245833888814, 25.689908012265033, 25.289961338488204, 25.616584455405945, 27.702972936941737, 25.756565791227835]
Training finished in 4.0 minutes.
Model saved to checkpoints/20231208_130830/model_18
Loading model from checkpoints/20231208_130830/model_18
SUB: 16.38%, DEL: 8.91%, INS: 2.02%, COR: 74.71%, PER: 27.30%
