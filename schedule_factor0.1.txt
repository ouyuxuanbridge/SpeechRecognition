Namespace(seed=123, train_json='/rds/user/yo279/hpc-work/MLMI2/fbanks/train_fbank.json', val_json='/rds/user/yo279/hpc-work/MLMI2/fbanks/dev_fbank.json', test_json='/rds/user/yo279/hpc-work/MLMI2/fbanks/test_fbank.json', batch_size=4, num_layers=1, fbank_dims=23, model_dims=128, concat=1, lr=0.5, vocab='vocab_39.txt', report_interval=50, num_epochs=20, dropout_rate=0.5, clip_max_norm=1, num_ff_layers=2, scheduler_factor=0.1)
Total number of model parameters is 166952
EPOCH 1:
  batch 50 loss: 4.078926424980164
  batch 100 loss: 3.1571474266052246
  batch 150 loss: 3.0258984422683715
  batch 200 loss: 2.9078385066986083
  batch 250 loss: 2.8329377508163454
  batch 300 loss: 2.7092234897613525
  batch 350 loss: 2.5512432765960695
  batch 400 loss: 2.4298265743255616
  batch 450 loss: 2.36793258190155
  batch 500 loss: 2.2977595925331116
  batch 550 loss: 2.25881982088089
  batch 600 loss: 2.1535765075683595
  batch 650 loss: 2.022587535381317
  batch 700 loss: 2.0234898686408997
  batch 750 loss: 1.9887672305107116
  batch 800 loss: 1.9553564190864563
  batch 850 loss: 1.9298668169975282
  batch 900 loss: 1.845281035900116
LOSS train 1.84528 valid 1.82000, valid PER 67.12%
EPOCH 2:
  batch 50 loss: 1.7741594076156617
  batch 100 loss: 1.7663721680641173
  batch 150 loss: 1.6933698654174805
  batch 200 loss: 1.711291720867157
  batch 250 loss: 1.7034817576408385
  batch 300 loss: 1.6573467350006104
  batch 350 loss: 1.5615093088150025
  batch 400 loss: 1.5715506815910338
  batch 450 loss: 1.5114016842842102
  batch 500 loss: 1.540228533744812
  batch 550 loss: 1.5550020670890807
  batch 600 loss: 1.4756197452545166
  batch 650 loss: 1.4982367157936096
  batch 700 loss: 1.4859094309806824
  batch 750 loss: 1.4370328545570374
  batch 800 loss: 1.3786203265190125
  batch 850 loss: 1.382905089855194
  batch 900 loss: 1.411391305923462
LOSS train 1.41139 valid 1.40685, valid PER 42.99%
EPOCH 3:
  batch 50 loss: 1.360913872718811
  batch 100 loss: 1.331698489189148
  batch 150 loss: 1.3479794692993163
  batch 200 loss: 1.2902255082130432
  batch 250 loss: 1.281267523765564
  batch 300 loss: 1.276093888282776
  batch 350 loss: 1.3241448378562928
  batch 400 loss: 1.2807888162136079
  batch 450 loss: 1.289217131137848
  batch 500 loss: 1.272534728050232
  batch 550 loss: 1.2713140285015105
  batch 600 loss: 1.237295788526535
  batch 650 loss: 1.2014415180683136
  batch 700 loss: 1.2532086336612702
  batch 750 loss: 1.2850007486343384
  batch 800 loss: 1.206830061674118
  batch 850 loss: 1.2361937606334685
  batch 900 loss: 1.1730696880817413
LOSS train 1.17307 valid 1.29140, valid PER 39.01%
EPOCH 4:
  batch 50 loss: 1.1734491729736327
  batch 100 loss: 1.1999767374992372
  batch 150 loss: 1.1628670108318329
  batch 200 loss: 1.1894925701618195
  batch 250 loss: 1.1982909643650055
  batch 300 loss: 1.1955823230743408
  batch 350 loss: 1.130430932044983
  batch 400 loss: 1.158797993659973
  batch 450 loss: 1.1404461801052093
  batch 500 loss: 1.1326589500904083
  batch 550 loss: 1.1558338701725006
  batch 600 loss: 1.169605929851532
  batch 650 loss: 1.1768476855754852
  batch 700 loss: 1.1552323806285858
  batch 750 loss: 1.1011155450344086
  batch 800 loss: 1.0853901922702789
  batch 850 loss: 1.118527170419693
  batch 900 loss: 1.1534717965126038
LOSS train 1.15347 valid 1.12296, valid PER 35.19%
EPOCH 5:
  batch 50 loss: 1.0479353392124175
  batch 100 loss: 1.0663231134414672
  batch 150 loss: 1.114836084842682
  batch 200 loss: 1.0456674671173096
  batch 250 loss: 1.0635715556144714
  batch 300 loss: 1.0796160900592804
  batch 350 loss: 1.0806518626213073
  batch 400 loss: 1.0590650856494903
  batch 450 loss: 1.0718018209934235
  batch 500 loss: 1.0603863680362702
  batch 550 loss: 1.0255813789367676
  batch 600 loss: 1.093688087463379
  batch 650 loss: 1.0543590700626373
  batch 700 loss: 1.096121026277542
  batch 750 loss: 1.0202660107612609
  batch 800 loss: 1.0533871006965638
  batch 850 loss: 1.0474733996391297
  batch 900 loss: 1.071582065820694
LOSS train 1.07158 valid 1.07368, valid PER 33.84%
EPOCH 6:
  batch 50 loss: 1.0473565077781677
  batch 100 loss: 1.0160082411766052
  batch 150 loss: 1.0090818643569945
  batch 200 loss: 1.0007331120967864
  batch 250 loss: 1.058247218132019
  batch 300 loss: 1.0012296044826507
  batch 350 loss: 0.9957309246063233
  batch 400 loss: 1.013150681257248
  batch 450 loss: 1.0321585416793824
  batch 500 loss: 1.0189734971523285
  batch 550 loss: 1.0324514389038086
  batch 600 loss: 0.9989337646961212
  batch 650 loss: 1.007602254152298
  batch 700 loss: 1.0184333634376526
  batch 750 loss: 0.9874645328521728
  batch 800 loss: 0.9959976732730865
  batch 850 loss: 0.9717020428180695
  batch 900 loss: 0.9994414472579956
LOSS train 0.99944 valid 1.07777, valid PER 33.18%
EPOCH 7:
  batch 50 loss: 0.9902024185657501
  batch 100 loss: 0.9943398141860962
  batch 150 loss: 0.971739262342453
  batch 200 loss: 0.945332442522049
  batch 250 loss: 0.9487919342517853
  batch 300 loss: 0.9454781007766724
  batch 350 loss: 0.9474744200706482
  batch 400 loss: 0.9753865730762482
  batch 450 loss: 0.9699279582500457
  batch 500 loss: 0.9457004308700562
  batch 550 loss: 0.9515508651733399
  batch 600 loss: 0.9431163036823272
  batch 650 loss: 0.9537494909763337
  batch 700 loss: 0.9949963247776031
  batch 750 loss: 0.9518339920043946
  batch 800 loss: 0.9465366840362549
  batch 850 loss: 0.9611683559417724
  batch 900 loss: 1.0048841059207916
LOSS train 1.00488 valid 1.05059, valid PER 32.94%
EPOCH 8:
  batch 50 loss: 0.9322896111011505
  batch 100 loss: 0.9219888317584991
  batch 150 loss: 0.9410548460483551
  batch 200 loss: 0.9116603064537049
  batch 250 loss: 0.949623111486435
  batch 300 loss: 0.879991158246994
  batch 350 loss: 0.9359608995914459
  batch 400 loss: 0.9245044040679932
  batch 450 loss: 0.9803194379806519
  batch 500 loss: 0.9878365099430084
  batch 550 loss: 0.9155879032611847
  batch 600 loss: 0.9508587789535522
  batch 650 loss: 0.9856394922733307
  batch 700 loss: 0.9356877160072327
  batch 750 loss: 0.9189682519435882
  batch 800 loss: 0.9494751012325287
  batch 850 loss: 0.9465092134475708
  batch 900 loss: 0.9309273719787597
LOSS train 0.93093 valid 1.01563, valid PER 31.22%
EPOCH 9:
  batch 50 loss: 0.8503784596920013
  batch 100 loss: 0.8786919355392456
  batch 150 loss: 0.8868265664577484
  batch 200 loss: 0.8434294050931931
  batch 250 loss: 0.8957531535625458
  batch 300 loss: 0.8910454106330872
  batch 350 loss: 0.9108545708656312
  batch 400 loss: 0.888109393119812
  batch 450 loss: 0.9188008403778076
  batch 500 loss: 0.8538530910015106
  batch 550 loss: 0.898441241979599
  batch 600 loss: 0.9068466627597809
  batch 650 loss: 0.9024881649017334
  batch 700 loss: 0.8830671894550324
  batch 750 loss: 0.881269919872284
  batch 800 loss: 0.9219794809818268
  batch 850 loss: 0.9001829946041107
  batch 900 loss: 0.8722176444530487
LOSS train 0.87222 valid 0.99643, valid PER 30.83%
EPOCH 10:
  batch 50 loss: 0.8166163766384125
  batch 100 loss: 0.8493919730186462
  batch 150 loss: 0.8908015656471252
  batch 200 loss: 0.9088272726535798
  batch 250 loss: 0.8898021101951599
  batch 300 loss: 0.8174345910549163
  batch 350 loss: 0.8876440739631652
  batch 400 loss: 0.8470405566692353
  batch 450 loss: 0.8417030680179596
  batch 500 loss: 0.8846172666549683
  batch 550 loss: 0.8765891301631927
  batch 600 loss: 0.8594691908359527
  batch 650 loss: 0.8858696007728577
  batch 700 loss: 0.881182131767273
  batch 750 loss: 0.8965130054950714
  batch 800 loss: 0.8877343130111695
  batch 850 loss: 0.8800185513496399
  batch 900 loss: 0.8779263699054718
LOSS train 0.87793 valid 1.01585, valid PER 32.04%
EPOCH 11:
  batch 50 loss: 0.8552385973930359
  batch 100 loss: 0.8872166705131531
  batch 150 loss: 0.8188918244838714
  batch 200 loss: 0.9056980240345002
  batch 250 loss: 0.8793801426887512
  batch 300 loss: 0.8425742316246033
  batch 350 loss: 0.8714937496185303
  batch 400 loss: 0.8747092664241791
  batch 450 loss: 0.8612139117717743
  batch 500 loss: 0.8711860477924347
  batch 550 loss: 0.9080104434490204
  batch 600 loss: 0.8777654933929443
  batch 650 loss: 0.938059059381485
  batch 700 loss: 0.843753126859665
  batch 750 loss: 0.8638086462020874
  batch 800 loss: 0.9043490195274353
  batch 850 loss: 0.8985921084880829
  batch 900 loss: 0.8902028489112854
LOSS train 0.89020 valid 0.98511, valid PER 30.13%
EPOCH 12:
  batch 50 loss: 0.8480841517448425
  batch 100 loss: 0.8367838644981385
  batch 150 loss: 0.8218511068820953
  batch 200 loss: 0.8151643228530884
  batch 250 loss: 0.8780679726600646
  batch 300 loss: 0.8388056945800781
  batch 350 loss: 0.8298627865314484
  batch 400 loss: 0.8576720356941223
  batch 450 loss: 0.8463459765911102
  batch 500 loss: 0.8626820111274719
  batch 550 loss: 0.7987590408325196
  batch 600 loss: 0.8273278081417084
  batch 650 loss: 0.8756970894336701
  batch 700 loss: 0.8722001326084137
  batch 750 loss: 0.8349469196796417
  batch 800 loss: 0.8248101329803467
  batch 850 loss: 0.8742785370349884
  batch 900 loss: 0.8975997066497803
LOSS train 0.89760 valid 0.97150, valid PER 29.97%
EPOCH 13:
  batch 50 loss: 0.7942236959934235
  batch 100 loss: 0.8094731867313385
  batch 150 loss: 0.7627883613109588
  batch 200 loss: 0.7947915410995483
  batch 250 loss: 0.8019163453578949
  batch 300 loss: 0.7997034657001495
  batch 350 loss: 0.8147116482257843
  batch 400 loss: 0.8234017038345337
  batch 450 loss: 0.8172550475597382
  batch 500 loss: 0.7899567484855652
  batch 550 loss: 0.820618234872818
  batch 600 loss: 0.8167692744731903
  batch 650 loss: 0.8387607872486115
  batch 700 loss: 0.8306030666828156
  batch 750 loss: 0.798628169298172
  batch 800 loss: 0.8112397599220276
  batch 850 loss: 0.8393022620677948
  batch 900 loss: 0.8467054080963134
LOSS train 0.84671 valid 0.96920, valid PER 29.46%
EPOCH 14:
  batch 50 loss: 0.7766672730445862
  batch 100 loss: 0.7660350811481476
  batch 150 loss: 0.7970949327945709
  batch 200 loss: 0.8134034287929535
  batch 250 loss: 0.7909788340330124
  batch 300 loss: 0.8237027096748352
  batch 350 loss: 0.7508814144134521
  batch 400 loss: 0.7881337380409241
  batch 450 loss: 0.7911080801486969
  batch 500 loss: 0.8188833868503571
  batch 550 loss: 0.8185692965984345
  batch 600 loss: 0.7682013970613479
  batch 650 loss: 0.8209049779176713
  batch 700 loss: 0.815593284368515
  batch 750 loss: 0.8104299318790436
  batch 800 loss: 0.7782047009468078
  batch 850 loss: 0.8437685513496399
  batch 900 loss: 0.8102937698364258
LOSS train 0.81029 valid 0.97062, valid PER 29.84%
EPOCH 15:
  batch 50 loss: 0.7623891001939773
  batch 100 loss: 0.737787835597992
  batch 150 loss: 0.7550523543357849
  batch 200 loss: 0.7961918151378632
  batch 250 loss: 0.808586778640747
  batch 300 loss: 0.7800682485103607
  batch 350 loss: 0.7712911057472229
  batch 400 loss: 0.767954934835434
  batch 450 loss: 0.7908561211824418
  batch 500 loss: 0.7298811686038971
  batch 550 loss: 0.7799632680416108
  batch 600 loss: 0.78018394947052
  batch 650 loss: 0.8226988184452056
  batch 700 loss: 0.829108077287674
  batch 750 loss: 0.7969714081287385
  batch 800 loss: 0.7724809408187866
  batch 850 loss: 0.7735609364509582
  batch 900 loss: 0.7852243226766586
LOSS train 0.78522 valid 0.97863, valid PER 30.40%
EPOCH 16:
  batch 50 loss: 0.7708167171478272
  batch 100 loss: 0.7322416746616364
  batch 150 loss: 0.7374760514497757
  batch 200 loss: 0.7503527784347535
  batch 250 loss: 0.7642542350292206
  batch 300 loss: 0.7420582830905914
  batch 350 loss: 0.7773322582244873
  batch 400 loss: 0.7839853048324585
  batch 450 loss: 0.771972018480301
  batch 500 loss: 0.7453108263015747
  batch 550 loss: 0.7602260321378708
  batch 600 loss: 0.765828161239624
  batch 650 loss: 0.7590925967693329
  batch 700 loss: 0.7352350562810898
  batch 750 loss: 0.7487188065052033
  batch 800 loss: 0.7554707151651382
  batch 850 loss: 0.7838970267772675
  batch 900 loss: 0.7597191262245179
LOSS train 0.75972 valid 0.97563, valid PER 29.68%
EPOCH 17:
  batch 50 loss: 0.7301415228843688
  batch 100 loss: 0.7282699692249298
  batch 150 loss: 0.7249311006069183
  batch 200 loss: 0.7189160704612731
  batch 250 loss: 0.7430597591400147
  batch 300 loss: 0.7294376266002655
  batch 350 loss: 0.703593966960907
  batch 400 loss: 0.7473032140731811
  batch 450 loss: 0.7289233946800232
  batch 500 loss: 0.7076063060760498
  batch 550 loss: 0.7382800954580307
  batch 600 loss: 0.7810378134250641
  batch 650 loss: 0.7396004521846771
  batch 700 loss: 0.7229730653762817
  batch 750 loss: 0.7182256877422333
  batch 800 loss: 0.7435673820972443
  batch 850 loss: 0.7530626928806305
  batch 900 loss: 0.7385446977615356
LOSS train 0.73854 valid 0.96805, valid PER 29.10%
EPOCH 18:
  batch 50 loss: 0.6961627578735352
  batch 100 loss: 0.7347261881828309
  batch 150 loss: 0.7293244558572769
  batch 200 loss: 0.7185041427612304
  batch 250 loss: 0.7230211663246154
  batch 300 loss: 0.7055476868152618
  batch 350 loss: 0.726463497877121
  batch 400 loss: 0.6930139750242233
  batch 450 loss: 0.7360000419616699
  batch 500 loss: 0.7252212178707123
  batch 550 loss: 0.7140085422992706
  batch 600 loss: 0.6949087327718735
  batch 650 loss: 0.7186017525196076
  batch 700 loss: 0.7579146146774292
  batch 750 loss: 0.7313140672445297
  batch 800 loss: 0.7188865226507187
  batch 850 loss: 0.7286780780553818
  batch 900 loss: 0.7315612709522248
LOSS train 0.73156 valid 0.94317, valid PER 29.36%
EPOCH 19:
  batch 50 loss: 0.6487804329395295
  batch 100 loss: 0.6604896140098572
  batch 150 loss: 0.6861799454689026
  batch 200 loss: 0.6850236457586288
  batch 250 loss: 0.7116736310720444
  batch 300 loss: 0.7218746829032898
  batch 350 loss: 0.7245803272724152
  batch 400 loss: 0.7012887823581696
  batch 450 loss: 0.7223269498348236
  batch 500 loss: 0.7220221418142319
  batch 550 loss: 0.7404130470752716
  batch 600 loss: 0.7200252747535706
  batch 650 loss: 0.7828751385211945
  batch 700 loss: 0.7024250447750091
  batch 750 loss: 0.7140994274616241
  batch 800 loss: 0.7197977793216705
  batch 850 loss: 0.7196395343542099
  batch 900 loss: 0.7145053172111511
LOSS train 0.71451 valid 0.95521, valid PER 29.02%
EPOCH 20:
  batch 50 loss: 0.677480458021164
  batch 100 loss: 0.667186706662178
  batch 150 loss: 0.6691996067762375
  batch 200 loss: 0.6727640074491501
  batch 250 loss: 0.6621006339788437
  batch 300 loss: 0.6959745788574219
  batch 350 loss: 0.663283320069313
  batch 400 loss: 0.7049184095859528
  batch 450 loss: 0.715808185338974
  batch 500 loss: 0.6681514286994934
  batch 550 loss: 0.7457693016529083
  batch 600 loss: 0.676742651462555
  batch 650 loss: 0.6947921586036682
  batch 700 loss: 0.7214881020784378
  batch 750 loss: 0.7079974102973938
  batch 800 loss: 0.7326073956489563
  batch 850 loss: 0.7248065197467803
  batch 900 loss: 0.7006315195560455
LOSS train 0.70063 valid 0.98044, valid PER 29.44%
train_loss
[1.845281035900116, 1.411391305923462, 1.1730696880817413, 1.1534717965126038, 1.071582065820694, 0.9994414472579956, 1.0048841059207916, 0.9309273719787597, 0.8722176444530487, 0.8779263699054718, 0.8902028489112854, 0.8975997066497803, 0.8467054080963134, 0.8102937698364258, 0.7852243226766586, 0.7597191262245179, 0.7385446977615356, 0.7315612709522248, 0.7145053172111511, 0.7006315195560455]
valid_loss
[1.8200047016143799, 1.4068504571914673, 1.2914016246795654, 1.1229649782180786, 1.0736751556396484, 1.0777721405029297, 1.0505882501602173, 1.0156258344650269, 0.9964338541030884, 1.0158507823944092, 0.9851072430610657, 0.9714986681938171, 0.9692006707191467, 0.9706218242645264, 0.9786288738250732, 0.9756342172622681, 0.9680493474006653, 0.9431725144386292, 0.9552081823348999, 0.9804394841194153]
valid_per
[67.11771763764831, 42.98760165311292, 39.00813224903346, 35.188641514464734, 33.8354886015198, 33.18224236768431, 32.942274363418214, 31.222503666177843, 30.829222770297292, 32.03572856952406, 30.129316091187842, 29.96933742167711, 29.462738301559792, 29.8360218637515, 30.402612984935338, 29.67604319424077, 29.096120517264364, 29.356085855219305, 29.016131182508996, 29.44274096787095]
Training finished in 3.0 minutes.
Model saved to checkpoints/20231208_152804/model_18
Loading model from checkpoints/20231208_152804/model_18
SUB: 16.27%, DEL: 12.19%, INS: 1.93%, COR: 71.54%, PER: 30.39%
