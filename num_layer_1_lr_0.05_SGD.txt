Namespace(seed=123, train_json='/rds/user/yo279/hpc-work/MLMI2/fbanks/train_fbank.json', val_json='/rds/user/yo279/hpc-work/MLMI2/fbanks/dev_fbank.json', test_json='/rds/user/yo279/hpc-work/MLMI2/fbanks/test_fbank.json', batch_size=4, num_layers=1, fbank_dims=23, model_dims=128, concat=1, lr=0.05, vocab='vocab_39.txt', report_interval=50, num_epochs=20, dropout_rate=0.5, clip_max_norm=1)
Total number of model parameters is 166952
EPOCH 1:
  batch 50 loss: 5.557441968917846
  batch 100 loss: 3.3489153385162354
  batch 150 loss: 3.288629117012024
  batch 200 loss: 3.263542904853821
  batch 250 loss: 3.2468045949935913
  batch 300 loss: 3.2168844509124757
  batch 350 loss: 3.208130512237549
  batch 400 loss: 3.20997305393219
  batch 450 loss: 3.194192118644714
  batch 500 loss: 3.1699608755111695
  batch 550 loss: 3.1500615072250366
  batch 600 loss: 3.1265105962753297
  batch 650 loss: 3.101114091873169
  batch 700 loss: 3.095544114112854
  batch 750 loss: 3.054995312690735
  batch 800 loss: 3.0257340049743653
  batch 850 loss: 2.9974577236175537
  batch 900 loss: 2.9420603704452515
LOSS train 2.94206 valid 2.91096, valid PER 88.17%
EPOCH 2:
  batch 50 loss: 2.904200096130371
  batch 100 loss: 2.852159786224365
  batch 150 loss: 2.800641541481018
  batch 200 loss: 2.783462824821472
  batch 250 loss: 2.756042757034302
  batch 300 loss: 2.7327675008773804
  batch 350 loss: 2.6797195291519165
  batch 400 loss: 2.676699900627136
  batch 450 loss: 2.646669726371765
  batch 500 loss: 2.6191044998168946
  batch 550 loss: 2.613080916404724
  batch 600 loss: 2.563540210723877
  batch 650 loss: 2.562493028640747
  batch 700 loss: 2.529904913902283
  batch 750 loss: 2.5270085954666137
  batch 800 loss: 2.4779050493240358
  batch 850 loss: 2.4531871604919435
  batch 900 loss: 2.4555327796936037
LOSS train 2.45553 valid 2.44011, valid PER 81.53%
EPOCH 3:
  batch 50 loss: 2.4337648057937624
  batch 100 loss: 2.3822497844696047
  batch 150 loss: 2.3838137722015382
  batch 200 loss: 2.368739171028137
  batch 250 loss: 2.3329515409469606
  batch 300 loss: 2.332070951461792
  batch 350 loss: 2.3512062549591066
  batch 400 loss: 2.307577452659607
  batch 450 loss: 2.2695601773262024
  batch 500 loss: 2.2745489931106566
  batch 550 loss: 2.2490370416641237
  batch 600 loss: 2.210807592868805
  batch 650 loss: 2.1899993062019347
  batch 700 loss: 2.2027475094795226
  batch 750 loss: 2.2374736642837525
  batch 800 loss: 2.175844655036926
  batch 850 loss: 2.1788141179084777
  batch 900 loss: 2.1508420872688294
LOSS train 2.15084 valid 2.16758, valid PER 77.60%
EPOCH 4:
  batch 50 loss: 2.1543704795837404
  batch 100 loss: 2.1407796716690064
  batch 150 loss: 2.087232143878937
  batch 200 loss: 2.1298850059509276
  batch 250 loss: 2.10066171169281
  batch 300 loss: 2.1042398476600646
  batch 350 loss: 2.038443305492401
  batch 400 loss: 2.0678024291992188
  batch 450 loss: 2.061213240623474
  batch 500 loss: 2.0106582140922544
  batch 550 loss: 2.0402200269699096
  batch 600 loss: 2.043259389400482
  batch 650 loss: 2.037136688232422
  batch 700 loss: 2.0030155992507934
  batch 750 loss: 1.9696806597709655
  batch 800 loss: 1.9463799715042114
  batch 850 loss: 1.9618872594833374
  batch 900 loss: 1.9865574431419373
LOSS train 1.98656 valid 1.98056, valid PER 73.30%
EPOCH 5:
  batch 50 loss: 1.9509937834739686
  batch 100 loss: 1.916203134059906
  batch 150 loss: 1.9434144115447998
  batch 200 loss: 1.8896520137786865
  batch 250 loss: 1.8939717864990235
  batch 300 loss: 1.8963592767715454
  batch 350 loss: 1.899554226398468
  batch 400 loss: 1.8817858600616455
  batch 450 loss: 1.8771497750282287
  batch 500 loss: 1.8783219170570373
  batch 550 loss: 1.8102831482887267
  batch 600 loss: 1.8781836676597594
  batch 650 loss: 1.828273766040802
  batch 700 loss: 1.8557330298423766
  batch 750 loss: 1.805978648662567
  batch 800 loss: 1.831837751865387
  batch 850 loss: 1.8239237999916076
  batch 900 loss: 1.8195029091835022
LOSS train 1.81950 valid 1.82783, valid PER 68.74%
EPOCH 6:
  batch 50 loss: 1.8235521006584168
  batch 100 loss: 1.7803790116310119
  batch 150 loss: 1.7546513390541076
  batch 200 loss: 1.764354646205902
  batch 250 loss: 1.7888500308990478
  batch 300 loss: 1.727589294910431
  batch 350 loss: 1.7434008169174193
  batch 400 loss: 1.7219832968711852
  batch 450 loss: 1.7628197002410888
  batch 500 loss: 1.7287081980705261
  batch 550 loss: 1.735227680206299
  batch 600 loss: 1.6973807001113892
  batch 650 loss: 1.7105059242248535
  batch 700 loss: 1.7025594449043273
  batch 750 loss: 1.6987437510490417
  batch 800 loss: 1.6656950998306275
  batch 850 loss: 1.679171085357666
  batch 900 loss: 1.6869939136505128
LOSS train 1.68699 valid 1.75196, valid PER 67.28%
EPOCH 7:
  batch 50 loss: 1.6760425996780395
  batch 100 loss: 1.6730969977378845
  batch 150 loss: 1.6466345071792603
  batch 200 loss: 1.6361894035339355
  batch 250 loss: 1.6427094984054564
  batch 300 loss: 1.616090352535248
  batch 350 loss: 1.603337094783783
  batch 400 loss: 1.6265794348716736
  batch 450 loss: 1.5997675561904907
  batch 500 loss: 1.5963793778419495
  batch 550 loss: 1.606976351737976
  batch 600 loss: 1.6038339233398438
  batch 650 loss: 1.5868114185333253
  batch 700 loss: 1.5851495671272278
  batch 750 loss: 1.5647571897506714
  batch 800 loss: 1.562975399494171
  batch 850 loss: 1.5691674613952638
  batch 900 loss: 1.6113255882263184
LOSS train 1.61133 valid 1.57360, valid PER 57.40%
EPOCH 8:
  batch 50 loss: 1.5489065623283387
  batch 100 loss: 1.5481064486503602
  batch 150 loss: 1.5336204719543458
  batch 200 loss: 1.5033676958084106
  batch 250 loss: 1.5447925424575806
  batch 300 loss: 1.4613216638565063
  batch 350 loss: 1.5275936436653137
  batch 400 loss: 1.5130871868133544
  batch 450 loss: 1.5139327597618104
  batch 500 loss: 1.5524332571029662
  batch 550 loss: 1.4748476028442383
  batch 600 loss: 1.5117104768753051
  batch 650 loss: 1.5310552740097045
  batch 700 loss: 1.4750088953971863
  batch 750 loss: 1.4829567337036134
  batch 800 loss: 1.4892142152786254
  batch 850 loss: 1.4986275291442872
  batch 900 loss: 1.4651502895355224
LOSS train 1.46515 valid 1.50059, valid PER 51.05%
EPOCH 9:
  batch 50 loss: 1.4058495879173278
  batch 100 loss: 1.4736483955383302
  batch 150 loss: 1.4543850469589232
  batch 200 loss: 1.4132672548294067
  batch 250 loss: 1.4491753268241883
  batch 300 loss: 1.4427750277519227
  batch 350 loss: 1.451036012172699
  batch 400 loss: 1.4292630982398986
  batch 450 loss: 1.4358551812171936
  batch 500 loss: 1.3968704342842102
  batch 550 loss: 1.4199658918380738
  batch 600 loss: 1.4283549618721008
  batch 650 loss: 1.389240882396698
  batch 700 loss: 1.4019167852401733
  batch 750 loss: 1.4013486480712891
  batch 800 loss: 1.4002273392677307
  batch 850 loss: 1.416029634475708
  batch 900 loss: 1.3610431933403015
LOSS train 1.36104 valid 1.40566, valid PER 47.15%
EPOCH 10:
  batch 50 loss: 1.3403041005134582
  batch 100 loss: 1.357363328933716
  batch 150 loss: 1.3770168328285217
  batch 200 loss: 1.3861343050003052
  batch 250 loss: 1.3740912461280823
  batch 300 loss: 1.3246295547485352
  batch 350 loss: 1.3431841158866882
  batch 400 loss: 1.3286449539661407
  batch 450 loss: 1.306993887424469
  batch 500 loss: 1.3611438608169555
  batch 550 loss: 1.364430742263794
  batch 600 loss: 1.3439294505119324
  batch 650 loss: 1.3078259754180908
  batch 700 loss: 1.3132248759269713
  batch 750 loss: 1.3134000039100646
  batch 800 loss: 1.3392557978630066
  batch 850 loss: 1.3258589696884155
  batch 900 loss: 1.3324903297424315
LOSS train 1.33249 valid 1.36008, valid PER 46.13%
EPOCH 11:
  batch 50 loss: 1.29145263671875
  batch 100 loss: 1.277012779712677
  batch 150 loss: 1.2663053727149964
  batch 200 loss: 1.3167162132263184
  batch 250 loss: 1.2966717982292175
  batch 300 loss: 1.26978950381279
  batch 350 loss: 1.279533429145813
  batch 400 loss: 1.3030815839767456
  batch 450 loss: 1.2761983907222747
  batch 500 loss: 1.2544230568408965
  batch 550 loss: 1.2658053600788117
  batch 600 loss: 1.2556332635879517
  batch 650 loss: 1.3085364389419556
  batch 700 loss: 1.2356925082206727
  batch 750 loss: 1.2436803472042084
  batch 800 loss: 1.294970278739929
  batch 850 loss: 1.2852889370918275
  batch 900 loss: 1.2796725845336914
LOSS train 1.27967 valid 1.28733, valid PER 41.27%
EPOCH 12:
  batch 50 loss: 1.246815000772476
  batch 100 loss: 1.232170407772064
  batch 150 loss: 1.230340781211853
  batch 200 loss: 1.2229624950885774
  batch 250 loss: 1.2645326340198517
  batch 300 loss: 1.233052464723587
  batch 350 loss: 1.2289930176734924
  batch 400 loss: 1.2554402887821197
  batch 450 loss: 1.2539820647239686
  batch 500 loss: 1.2482123398780822
  batch 550 loss: 1.1751106524467467
  batch 600 loss: 1.1856954777240754
  batch 650 loss: 1.2476673829555511
  batch 700 loss: 1.2304663705825805
  batch 750 loss: 1.200611605644226
  batch 800 loss: 1.1900512957572937
  batch 850 loss: 1.2295701003074646
  batch 900 loss: 1.235576354265213
LOSS train 1.23558 valid 1.24338, valid PER 39.93%
EPOCH 13:
  batch 50 loss: 1.1758269000053405
  batch 100 loss: 1.2060190558433532
  batch 150 loss: 1.164557409286499
  batch 200 loss: 1.1905291950702668
  batch 250 loss: 1.1920089423656464
  batch 300 loss: 1.167816560268402
  batch 350 loss: 1.1758433997631073
  batch 400 loss: 1.2061255288124084
  batch 450 loss: 1.2155678248405457
  batch 500 loss: 1.1691890859603882
  batch 550 loss: 1.1749056911468505
  batch 600 loss: 1.17008234500885
  batch 650 loss: 1.1868483746051788
  batch 700 loss: 1.1953888082504271
  batch 750 loss: 1.142771121263504
  batch 800 loss: 1.1671091842651367
  batch 850 loss: 1.2093740439414977
  batch 900 loss: 1.1913356411457061
LOSS train 1.19134 valid 1.22182, valid PER 38.89%
EPOCH 14:
  batch 50 loss: 1.1602050626277924
  batch 100 loss: 1.1755585885047912
  batch 150 loss: 1.1422697114944458
  batch 200 loss: 1.156038761138916
  batch 250 loss: 1.1463178372383118
  batch 300 loss: 1.1703852832317352
  batch 350 loss: 1.130536950826645
  batch 400 loss: 1.149993484020233
  batch 450 loss: 1.1270740139484405
  batch 500 loss: 1.1465647733211517
  batch 550 loss: 1.177953201532364
  batch 600 loss: 1.1153521728515625
  batch 650 loss: 1.1608721101284027
  batch 700 loss: 1.1842837536334991
  batch 750 loss: 1.116546688079834
  batch 800 loss: 1.0943916523456574
  batch 850 loss: 1.162934491634369
  batch 900 loss: 1.1444493198394776
LOSS train 1.14445 valid 1.19436, valid PER 38.24%
EPOCH 15:
  batch 50 loss: 1.1410589909553528
  batch 100 loss: 1.101448003053665
  batch 150 loss: 1.1138478875160218
  batch 200 loss: 1.1224134457111359
  batch 250 loss: 1.137493302822113
  batch 300 loss: 1.1120334005355834
  batch 350 loss: 1.1231207358837128
  batch 400 loss: 1.104113439321518
  batch 450 loss: 1.1115998387336732
  batch 500 loss: 1.0825546991825103
  batch 550 loss: 1.1091108810901642
  batch 600 loss: 1.1309517681598664
  batch 650 loss: 1.1495654809474944
  batch 700 loss: 1.1292648947238921
  batch 750 loss: 1.1142838299274445
  batch 800 loss: 1.0949086117744447
  batch 850 loss: 1.099535838365555
  batch 900 loss: 1.1118699944019317
LOSS train 1.11187 valid 1.19863, valid PER 36.96%
EPOCH 16:
  batch 50 loss: 1.1377445614337922
  batch 100 loss: 1.071098426580429
  batch 150 loss: 1.062413740158081
  batch 200 loss: 1.0803058624267579
  batch 250 loss: 1.1226356649398803
  batch 300 loss: 1.093554527759552
  batch 350 loss: 1.106954379081726
  batch 400 loss: 1.0942290222644806
  batch 450 loss: 1.1224436461925507
  batch 500 loss: 1.0711319077014922
  batch 550 loss: 1.1272828495502472
  batch 600 loss: 1.0921741580963136
  batch 650 loss: 1.1043764758110046
  batch 700 loss: 1.0781567358970643
  batch 750 loss: 1.098156259059906
  batch 800 loss: 1.0968062925338744
  batch 850 loss: 1.0778363227844239
  batch 900 loss: 1.080599366426468
LOSS train 1.08060 valid 1.15584, valid PER 34.80%
EPOCH 17:
  batch 50 loss: 1.0965414309501649
  batch 100 loss: 1.0795541715621948
  batch 150 loss: 1.0553918778896332
  batch 200 loss: 1.0621989011764525
  batch 250 loss: 1.0751725578308104
  batch 300 loss: 1.0866845393180846
  batch 350 loss: 1.0438500261306762
  batch 400 loss: 1.0945470762252807
  batch 450 loss: 1.0926171839237213
  batch 500 loss: 1.0652598774433135
  batch 550 loss: 1.0741877114772798
  batch 600 loss: 1.1235567092895509
  batch 650 loss: 1.041428495645523
  batch 700 loss: 1.0606244063377381
  batch 750 loss: 1.027948340177536
  batch 800 loss: 1.0565095388889312
  batch 850 loss: 1.0488794887065886
  batch 900 loss: 1.0437922382354736
LOSS train 1.04379 valid 1.11899, valid PER 35.17%
EPOCH 18:
  batch 50 loss: 1.0518860197067261
  batch 100 loss: 1.0573465490341187
  batch 150 loss: 1.0557752466201782
  batch 200 loss: 1.0501613247394561
  batch 250 loss: 1.0395973944664
  batch 300 loss: 1.0361295139789581
  batch 350 loss: 1.0708791160583495
  batch 400 loss: 1.0303968071937561
  batch 450 loss: 1.06520702958107
  batch 500 loss: 1.0623529613018037
  batch 550 loss: 1.0392245519161225
  batch 600 loss: 1.029840567111969
  batch 650 loss: 1.016950511932373
  batch 700 loss: 1.0775307822227478
  batch 750 loss: 1.0277994227409364
  batch 800 loss: 1.041556158065796
  batch 850 loss: 1.0252924573421478
  batch 900 loss: 1.062519392967224
LOSS train 1.06252 valid 1.12401, valid PER 35.70%
EPOCH 19:
  batch 50 loss: 0.9894396412372589
  batch 100 loss: 1.007461963891983
  batch 150 loss: 1.0208719050884247
  batch 200 loss: 1.0231537592411042
  batch 250 loss: 1.0412463426589966
  batch 300 loss: 1.0280946683883667
  batch 350 loss: 1.0285821926593781
  batch 400 loss: 1.0381978785991668
  batch 450 loss: 1.0383570408821106
  batch 500 loss: 1.0373147869110106
  batch 550 loss: 1.0124988853931427
  batch 600 loss: 1.0364516055583954
  batch 650 loss: 1.0689851999282838
  batch 700 loss: 1.0069432032108308
  batch 750 loss: 0.9836380958557129
  batch 800 loss: 1.0318437612056732
  batch 850 loss: 1.0215011417865754
  batch 900 loss: 1.027563773393631
LOSS train 1.02756 valid 1.11763, valid PER 35.54%
EPOCH 20:
  batch 50 loss: 1.0102485275268556
  batch 100 loss: 1.0020087480545044
  batch 150 loss: 0.9880876278877259
  batch 200 loss: 1.0161144387722016
  batch 250 loss: 0.9934910809993744
  batch 300 loss: 1.0298084950447082
  batch 350 loss: 0.986834192276001
  batch 400 loss: 0.9875366139411926
  batch 450 loss: 0.9909882247447968
  batch 500 loss: 0.969211882352829
  batch 550 loss: 1.0473227322101593
  batch 600 loss: 0.9788644254207611
  batch 650 loss: 1.0203821170330047
  batch 700 loss: 1.0073468458652497
  batch 750 loss: 1.0040473091602324
  batch 800 loss: 1.0434322345256806
  batch 850 loss: 1.0119931149482726
  batch 900 loss: 1.034682902097702
LOSS train 1.03468 valid 1.09740, valid PER 32.75%
train_loss
[2.9420603704452515, 2.4555327796936037, 2.1508420872688294, 1.9865574431419373, 1.8195029091835022, 1.6869939136505128, 1.6113255882263184, 1.4651502895355224, 1.3610431933403015, 1.3324903297424315, 1.2796725845336914, 1.235576354265213, 1.1913356411457061, 1.1444493198394776, 1.1118699944019317, 1.080599366426468, 1.0437922382354736, 1.062519392967224, 1.027563773393631, 1.034682902097702]
valid_loss
[2.9109628200531006, 2.440114974975586, 2.1675758361816406, 1.9805641174316406, 1.827834129333496, 1.7519558668136597, 1.5736002922058105, 1.5005903244018555, 1.4056627750396729, 1.3600842952728271, 1.2873344421386719, 1.243381142616272, 1.2218207120895386, 1.1943618059158325, 1.1986252069473267, 1.155838966369629, 1.118990421295166, 1.1240086555480957, 1.1176302433013916, 1.0973955392837524]
valid_per
[88.17491001199839, 81.52912944940674, 77.60298626849753, 73.30355952539661, 68.74416744434075, 67.27769630715905, 57.39901346487135, 51.04652712971603, 47.14704706039195, 46.13384882015731, 41.27449673376883, 39.92800959872017, 38.894814024796695, 38.241567790961206, 36.95507265697907, 34.79536061858418, 35.1686441807759, 35.695240634582056, 35.53526196507132, 32.748966804426075]
Training finished in 3.0 minutes.
Model saved to checkpoints/20231208_015457/model_20
Loading model from checkpoints/20231208_015457/model_20
SUB: 18.20%, DEL: 13.99%, INS: 2.27%, COR: 67.82%, PER: 34.45%
