Namespace(seed=123, train_json='/rds/user/yo279/hpc-work/MLMI2/fbanks/train_fbank.json', val_json='/rds/user/yo279/hpc-work/MLMI2/fbanks/dev_fbank.json', test_json='/rds/user/yo279/hpc-work/MLMI2/fbanks/test_fbank.json', batch_size=4, num_layers=4, fbank_dims=23, model_dims=128, concat=1, lr=0.5, vocab='vocab_39.txt', report_interval=50, num_epochs=20, dropout_rate=0.8, clip_max_norm=1)
Total number of model parameters is 1352744
EPOCH 1:
  batch 50 loss: 4.158827838897705
  batch 100 loss: 3.3318955373764036
  batch 150 loss: 3.3113361120224
  batch 200 loss: 3.2827624702453613
  batch 250 loss: 3.2805639839172365
  batch 300 loss: 3.241428437232971
  batch 350 loss: 3.231333518028259
  batch 400 loss: 3.2158979988098144
  batch 450 loss: 3.166433253288269
  batch 500 loss: 3.0835487270355224
  batch 550 loss: 2.9828616046905516
  batch 600 loss: 2.8911364221572877
  batch 650 loss: 2.8017915201187136
  batch 700 loss: 2.779625196456909
  batch 750 loss: 2.695034999847412
  batch 800 loss: 2.6545715618133543
  batch 850 loss: 2.593087115287781
  batch 900 loss: 2.5285355949401858
LOSS train 2.52854 valid 2.44999, valid PER 73.58%
EPOCH 2:
  batch 50 loss: 2.47038405418396
  batch 100 loss: 2.4082552337646486
  batch 150 loss: 2.3148961567878725
  batch 200 loss: 2.3294144964218138
  batch 250 loss: 2.2831641578674318
  batch 300 loss: 2.207625143527985
  batch 350 loss: 2.1060537910461425
  batch 400 loss: 2.0855684947967528
  batch 450 loss: 2.0116822028160097
  batch 500 loss: 2.0106122970581053
  batch 550 loss: 2.011006245613098
  batch 600 loss: 1.93296843290329
  batch 650 loss: 1.9608421349525451
  batch 700 loss: 1.9026363897323608
  batch 750 loss: 1.8876978135108948
  batch 800 loss: 1.820166347026825
  batch 850 loss: 1.8254907727241516
  batch 900 loss: 1.826580822467804
LOSS train 1.82658 valid 1.74405, valid PER 52.24%
EPOCH 3:
  batch 50 loss: 1.7879449248313903
  batch 100 loss: 1.7375978660583495
  batch 150 loss: 1.7324126553535462
  batch 200 loss: 1.718693838119507
  batch 250 loss: 1.6767986845970153
  batch 300 loss: 1.6681366515159608
  batch 350 loss: 1.6744033765792847
  batch 400 loss: 1.6744857335090637
  batch 450 loss: 1.6202392768859863
  batch 500 loss: 1.6254135251045227
  batch 550 loss: 1.6189450359344482
  batch 600 loss: 1.5845435452461243
  batch 650 loss: 1.5323523569107056
  batch 700 loss: 1.5916618990898133
  batch 750 loss: 1.6148290133476257
  batch 800 loss: 1.5339616751670837
  batch 850 loss: 1.5531000018119812
  batch 900 loss: 1.4861767745018006
LOSS train 1.48618 valid 1.44329, valid PER 42.81%
EPOCH 4:
  batch 50 loss: 1.534256181716919
  batch 100 loss: 1.4798030424118043
  batch 150 loss: 1.460740921497345
  batch 200 loss: 1.4616257524490357
  batch 250 loss: 1.485013120174408
  batch 300 loss: 1.486501805782318
  batch 350 loss: 1.3840584611892701
  batch 400 loss: 1.4590634632110595
  batch 450 loss: 1.437475130558014
  batch 500 loss: 1.4130169558525085
  batch 550 loss: 1.4289411091804505
  batch 600 loss: 1.4444849324226379
  batch 650 loss: 1.456829707622528
  batch 700 loss: 1.3745679211616517
  batch 750 loss: 1.3561701083183288
  batch 800 loss: 1.3647756397724151
  batch 850 loss: 1.3707003831863402
  batch 900 loss: 1.3924048507213593
LOSS train 1.39240 valid 1.22884, valid PER 37.35%
EPOCH 5:
  batch 50 loss: 1.3594859528541565
  batch 100 loss: 1.3250435972213745
  batch 150 loss: 1.369302089214325
  batch 200 loss: 1.2934765315055847
  batch 250 loss: 1.3352510666847228
  batch 300 loss: 1.3192156744003296
  batch 350 loss: 1.3319039595127107
  batch 400 loss: 1.3273225927352905
  batch 450 loss: 1.301961611509323
  batch 500 loss: 1.3382900404930114
  batch 550 loss: 1.2354905331134796
  batch 600 loss: 1.3472643530368804
  batch 650 loss: 1.3088607907295227
  batch 700 loss: 1.3196584105491638
  batch 750 loss: 1.2560535645484925
  batch 800 loss: 1.3025934934616088
  batch 850 loss: 1.2820782721042634
  batch 900 loss: 1.2906287908554077
LOSS train 1.29063 valid 1.15511, valid PER 34.52%
EPOCH 6:
  batch 50 loss: 1.30737322807312
  batch 100 loss: 1.2590577268600465
  batch 150 loss: 1.2404124188423156
  batch 200 loss: 1.2433398365974426
  batch 250 loss: 1.2595505702495575
  batch 300 loss: 1.2473005270957946
  batch 350 loss: 1.218123071193695
  batch 400 loss: 1.2092834579944611
  batch 450 loss: 1.262573035955429
  batch 500 loss: 1.2276474332809448
  batch 550 loss: 1.2505036962032319
  batch 600 loss: 1.2467495000362396
  batch 650 loss: 1.2432323837280272
  batch 700 loss: 1.224940176010132
  batch 750 loss: 1.216641399860382
  batch 800 loss: 1.2179759967327117
  batch 850 loss: 1.1799350214004516
  batch 900 loss: 1.200116354227066
LOSS train 1.20012 valid 1.11485, valid PER 33.82%
EPOCH 7:
  batch 50 loss: 1.2179345667362214
  batch 100 loss: 1.2351113045215607
  batch 150 loss: 1.204527326822281
  batch 200 loss: 1.1761058032512666
  batch 250 loss: 1.1749773705005646
  batch 300 loss: 1.1387707316875457
  batch 350 loss: 1.167563807964325
  batch 400 loss: 1.1578734564781188
  batch 450 loss: 1.1746275699138642
  batch 500 loss: 1.1905170941352845
  batch 550 loss: 1.1540198171138762
  batch 600 loss: 1.182576060295105
  batch 650 loss: 1.1656072425842285
  batch 700 loss: 1.2089243936538696
  batch 750 loss: 1.1532352590560913
  batch 800 loss: 1.137527972459793
  batch 850 loss: 1.1651691102981567
  batch 900 loss: 1.2019242310523988
LOSS train 1.20192 valid 1.04694, valid PER 32.54%
EPOCH 8:
  batch 50 loss: 1.1586625623703002
  batch 100 loss: 1.1671035254001618
  batch 150 loss: 1.1377021670341492
  batch 200 loss: 1.1278322732448578
  batch 250 loss: 1.1605153262615204
  batch 300 loss: 1.098884961605072
  batch 350 loss: 1.2008387625217438
  batch 400 loss: 1.1511810719966888
  batch 450 loss: 1.1671545374393464
  batch 500 loss: 1.1887567150592804
  batch 550 loss: 1.149206919670105
  batch 600 loss: 1.1790012836456298
  batch 650 loss: 1.1767455756664276
  batch 700 loss: 1.185497772693634
  batch 750 loss: 1.1555388522148133
  batch 800 loss: 1.1530989241600036
  batch 850 loss: 1.1351102030277251
  batch 900 loss: 1.1180802309513092
LOSS train 1.11808 valid 1.03507, valid PER 31.83%
EPOCH 9:
  batch 50 loss: 1.0800779366493225
  batch 100 loss: 1.1476828694343566
  batch 150 loss: 1.1508372962474822
  batch 200 loss: 1.1121367239952087
  batch 250 loss: 1.1334300541877746
  batch 300 loss: 1.1214123177528381
  batch 350 loss: 1.1143998217582702
  batch 400 loss: 1.111472030878067
  batch 450 loss: 1.1202597522735596
  batch 500 loss: 1.0547401845455169
  batch 550 loss: 1.0934175229072571
  batch 600 loss: 1.124769948720932
  batch 650 loss: 1.0874455559253693
  batch 700 loss: 1.0757397878170014
  batch 750 loss: 1.0813022530078888
  batch 800 loss: 1.09977330327034
  batch 850 loss: 1.0963286769390106
  batch 900 loss: 1.064112823009491
LOSS train 1.06411 valid 0.99767, valid PER 30.05%
EPOCH 10:
  batch 50 loss: 1.0448960888385772
  batch 100 loss: 1.0877677309513092
  batch 150 loss: 1.0892111301422118
  batch 200 loss: 1.108179577589035
  batch 250 loss: 1.1031647276878358
  batch 300 loss: 1.0295176231861114
  batch 350 loss: 1.071688219308853
  batch 400 loss: 1.0494927632808686
  batch 450 loss: 1.0412567687034606
  batch 500 loss: 1.090576356649399
  batch 550 loss: 1.0858046793937683
  batch 600 loss: 1.0587807059288026
  batch 650 loss: 1.0570382940769196
  batch 700 loss: 1.0616625618934632
  batch 750 loss: 1.032909895181656
  batch 800 loss: 1.088033939599991
  batch 850 loss: 1.0665448701381683
  batch 900 loss: 1.0500086534023285
LOSS train 1.05001 valid 0.99663, valid PER 30.98%
EPOCH 11:
  batch 50 loss: 1.0237825083732606
  batch 100 loss: 1.0199888741970062
  batch 150 loss: 1.0151518821716308
  batch 200 loss: 1.0918707823753357
  batch 250 loss: 1.0851984298229218
  batch 300 loss: 1.0679098045825959
  batch 350 loss: 1.0663959288597107
  batch 400 loss: 1.0890919482707977
  batch 450 loss: 1.0591519451141358
  batch 500 loss: 1.0380991113185882
  batch 550 loss: 1.036426671743393
  batch 600 loss: 1.0099097418785095
  batch 650 loss: 1.0967308616638183
  batch 700 loss: 1.033955566883087
  batch 750 loss: 1.0417738020420075
  batch 800 loss: 1.069380589723587
  batch 850 loss: 1.0971638202667235
  batch 900 loss: 1.0905225801467895
LOSS train 1.09052 valid 0.97941, valid PER 30.31%
EPOCH 12:
  batch 50 loss: 1.0538265085220337
  batch 100 loss: 1.0439154481887818
  batch 150 loss: 1.025609267950058
  batch 200 loss: 1.0135691213607787
  batch 250 loss: 1.066396151781082
  batch 300 loss: 1.011152789592743
  batch 350 loss: 1.0021284008026123
  batch 400 loss: 1.0597526335716247
  batch 450 loss: 1.0687246644496917
  batch 500 loss: 1.0634158658981323
  batch 550 loss: 0.9727770924568176
  batch 600 loss: 1.02480806350708
  batch 650 loss: 1.0633040285110473
  batch 700 loss: 1.071735875606537
  batch 750 loss: 1.0205512189865111
  batch 800 loss: 1.0335032546520233
  batch 850 loss: 1.0631782817840576
  batch 900 loss: 1.053270218372345
LOSS train 1.05327 valid 0.94880, valid PER 29.38%
EPOCH 13:
  batch 50 loss: 1.026256387233734
  batch 100 loss: 1.0289916324615478
  batch 150 loss: 0.9966221809387207
  batch 200 loss: 1.0709833443164825
  batch 250 loss: 1.009843086004257
  batch 300 loss: 1.0096058118343354
  batch 350 loss: 1.0322667348384857
  batch 400 loss: 1.0480628383159638
  batch 450 loss: 1.0628495037555694
  batch 500 loss: 1.0167297399044037
  batch 550 loss: 1.0510211193561554
  batch 600 loss: 1.0217685556411744
  batch 650 loss: 1.0372717201709747
  batch 700 loss: 1.0322859859466553
  batch 750 loss: 0.9952107810974121
  batch 800 loss: 1.00459277510643
  batch 850 loss: 1.0589186680316924
  batch 900 loss: 1.0445578372478486
LOSS train 1.04456 valid 0.93610, valid PER 28.84%
EPOCH 14:
  batch 50 loss: 1.0065471744537353
  batch 100 loss: 1.0180961894989013
  batch 150 loss: 0.9893032479286193
  batch 200 loss: 1.0343771719932555
  batch 250 loss: 1.0338024175167084
  batch 300 loss: 1.0998451507091522
  batch 350 loss: 1.0201010310649872
  batch 400 loss: 1.0352607035636903
  batch 450 loss: 1.033449900150299
  batch 500 loss: 1.0478150343894959
  batch 550 loss: 1.0449073219299316
  batch 600 loss: 1.018000123500824
  batch 650 loss: 1.0604445719718933
  batch 700 loss: 1.0897997164726256
  batch 750 loss: 1.0240336406230925
  batch 800 loss: 1.0098239016532897
  batch 850 loss: 1.055948395729065
  batch 900 loss: 1.068541295528412
LOSS train 1.06854 valid 0.97007, valid PER 29.70%
EPOCH 15:
  batch 50 loss: 1.039071124792099
  batch 100 loss: 1.013775349855423
  batch 150 loss: 1.0012599909305573
  batch 200 loss: 1.0617039227485656
  batch 250 loss: 1.0142710709571838
  batch 300 loss: 1.016003623008728
  batch 350 loss: 1.0120003747940063
  batch 400 loss: 0.9711538326740264
  batch 450 loss: 1.0050216436386108
  batch 500 loss: 0.9615267086029052
  batch 550 loss: 1.0295706284046173
  batch 600 loss: 1.0415232169628144
  batch 650 loss: 1.0371826219558715
  batch 700 loss: 1.0436689972877502
  batch 750 loss: 1.0253368508815766
  batch 800 loss: 0.9904194188117981
  batch 850 loss: 0.9855007994174957
  batch 900 loss: 1.0241880166530608
LOSS train 1.02419 valid 0.99005, valid PER 29.90%
EPOCH 16:
  batch 50 loss: 1.0336897885799408
  batch 100 loss: 0.9954782390594482
  batch 150 loss: 0.9994661152362824
  batch 200 loss: 1.0092237615585327
  batch 250 loss: 1.0409730303287505
  batch 300 loss: 1.0131266856193542
  batch 350 loss: 1.0421871256828308
  batch 400 loss: 0.9936135196685791
  batch 450 loss: 1.016226636171341
  batch 500 loss: 0.9724970722198486
  batch 550 loss: 1.0218934154510497
  batch 600 loss: 1.007907282114029
  batch 650 loss: 1.0270291769504547
  batch 700 loss: 0.9667116594314575
  batch 750 loss: 1.0136055302619935
  batch 800 loss: 1.019176265001297
  batch 850 loss: 0.9945571839809417
  batch 900 loss: 1.0132708263397217
LOSS train 1.01327 valid 0.95433, valid PER 28.87%
EPOCH 17:
  batch 50 loss: 1.0033992636203766
  batch 100 loss: 1.0271439933776856
  batch 150 loss: 0.9858598816394806
  batch 200 loss: 1.000160301923752
  batch 250 loss: 1.0081904518604279
  batch 300 loss: 1.036069233417511
  batch 350 loss: 1.0004300808906554
  batch 400 loss: 1.058199898004532
  batch 450 loss: 1.0506899464130401
  batch 500 loss: 1.0154014492034913
  batch 550 loss: 1.014552286863327
  batch 600 loss: 1.0907902669906617
  batch 650 loss: 1.0098928117752075
  batch 700 loss: 1.0409522092342376
  batch 750 loss: 1.0092019581794738
  batch 800 loss: 1.1178414702415467
  batch 850 loss: 1.0619737946987151
  batch 900 loss: 1.0347285175323486
LOSS train 1.03473 valid 0.96169, valid PER 29.44%
EPOCH 18:
  batch 50 loss: 1.0231436455249787
  batch 100 loss: 1.0247061574459075
  batch 150 loss: 1.053658003807068
  batch 200 loss: 1.0004129481315613
  batch 250 loss: 1.0238989984989166
  batch 300 loss: 1.0071586418151854
  batch 350 loss: 1.036676653623581
  batch 400 loss: 1.000997793674469
  batch 450 loss: 0.9999739301204681
  batch 500 loss: 0.9933359718322754
  batch 550 loss: 1.0039789843559266
  batch 600 loss: 1.040406744480133
  batch 650 loss: 1.016773875951767
  batch 700 loss: 1.0327160739898682
  batch 750 loss: 1.0126905131340027
  batch 800 loss: 1.0105048131942749
  batch 850 loss: 1.0088943874835967
  batch 900 loss: 1.0242052376270294
LOSS train 1.02421 valid 0.94768, valid PER 28.90%
EPOCH 19:
  batch 50 loss: 0.9254570913314819
  batch 100 loss: 0.9866226601600647
  batch 150 loss: 1.0074332880973815
  batch 200 loss: 1.0117564070224763
  batch 250 loss: 1.014241831302643
  batch 300 loss: 1.0066100943088532
  batch 350 loss: 1.0137021470069885
  batch 400 loss: 0.997703263759613
  batch 450 loss: 0.9719861137866974
  batch 500 loss: 1.0250243294239043
  batch 550 loss: 0.9885827207565308
  batch 600 loss: 1.0110603702068328
  batch 650 loss: 1.0295581328868866
  batch 700 loss: 0.9904540145397186
  batch 750 loss: 0.9601361131668091
  batch 800 loss: 0.9893961656093597
  batch 850 loss: 0.9947362768650055
  batch 900 loss: 0.9877576053142547
LOSS train 0.98776 valid 0.92680, valid PER 28.28%
EPOCH 20:
  batch 50 loss: 0.9535498225688934
  batch 100 loss: 0.9665764188766479
  batch 150 loss: 0.9589930891990661
  batch 200 loss: 0.9877515518665314
  batch 250 loss: 0.9702064168453216
  batch 300 loss: 0.9941162705421448
  batch 350 loss: 0.9630422329902649
  batch 400 loss: 0.9625947833061218
  batch 450 loss: 0.9795683228969574
  batch 500 loss: 0.9539653503894806
  batch 550 loss: 1.042680503129959
  batch 600 loss: 0.9709083390235901
  batch 650 loss: 0.9773882365226746
  batch 700 loss: 1.013886787891388
  batch 750 loss: 0.9768324410915374
  batch 800 loss: 1.0235367691516877
  batch 850 loss: 1.0075718545913697
  batch 900 loss: 1.0017765700817107
LOSS train 1.00178 valid 0.92627, valid PER 28.20%
train_loss
[2.5285355949401858, 1.826580822467804, 1.4861767745018006, 1.3924048507213593, 1.2906287908554077, 1.200116354227066, 1.2019242310523988, 1.1180802309513092, 1.064112823009491, 1.0500086534023285, 1.0905225801467895, 1.053270218372345, 1.0445578372478486, 1.068541295528412, 1.0241880166530608, 1.0132708263397217, 1.0347285175323486, 1.0242052376270294, 0.9877576053142547, 1.0017765700817107]
valid_loss
[2.4499940872192383, 1.7440471649169922, 1.44328773021698, 1.2288414239883423, 1.1551055908203125, 1.1148463487625122, 1.0469352006912231, 1.0350673198699951, 0.9976731538772583, 0.9966344237327576, 0.9794095158576965, 0.9488005042076111, 0.9361026287078857, 0.9700674414634705, 0.9900519847869873, 0.954329788684845, 0.9616853594779968, 0.9476796984672546, 0.9268016815185547, 0.9262702465057373]
valid_per
[73.5835221970404, 52.23970137315025, 42.80762564991335, 37.34835355285962, 34.51539794694041, 33.82215704572724, 32.54232768964138, 31.829089454739368, 30.049326756432475, 30.975869884015466, 30.309292094387413, 29.382748966804428, 28.842820957205706, 29.696040527929608, 29.896013864818023, 28.86948406879083, 29.44274096787095, 28.90281295827223, 28.276229836021866, 28.202906279162775]
Training finished in 7.0 minutes.
Model saved to checkpoints/20231207_234749/model_20
Loading model from checkpoints/20231207_234749/model_20
SUB: 17.68%, DEL: 8.79%, INS: 2.29%, COR: 73.53%, PER: 28.76%
