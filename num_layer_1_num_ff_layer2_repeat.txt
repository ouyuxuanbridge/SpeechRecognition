Namespace(seed=123, train_json='/rds/user/yo279/hpc-work/MLMI2/fbanks/train_fbank.json', val_json='/rds/user/yo279/hpc-work/MLMI2/fbanks/dev_fbank.json', test_json='/rds/user/yo279/hpc-work/MLMI2/fbanks/test_fbank.json', batch_size=4, num_layers=1, fbank_dims=23, model_dims=128, concat=1, lr=0.5, vocab='vocab_39.txt', report_interval=50, num_epochs=20, dropout_rate=0.5, clip_max_norm=1, num_ff_layers=2)
Total number of model parameters is 205008
EPOCH 1:
  batch 50 loss: 4.1228733205795285
  batch 100 loss: 3.255114779472351
  batch 150 loss: 3.2010316896438598
  batch 200 loss: 3.123450927734375
  batch 250 loss: 2.9741282081604004
  batch 300 loss: 2.764422769546509
  batch 350 loss: 2.6556328201293944
  batch 400 loss: 2.5112241744995116
  batch 450 loss: 2.455158715248108
  batch 500 loss: 2.3327768635749817
  batch 550 loss: 2.278698937892914
  batch 600 loss: 2.2003418016433716
  batch 650 loss: 2.1251221537590026
  batch 700 loss: 2.1147420763969422
  batch 750 loss: 2.016529388427734
  batch 800 loss: 2.0386518907546995
  batch 850 loss: 1.9517559504508972
  batch 900 loss: 1.9725256371498108
LOSS train 1.97253 valid 1.80846, valid PER 65.14%
EPOCH 2:
  batch 50 loss: 1.8374179053306579
  batch 100 loss: 1.80894052028656
  batch 150 loss: 1.7352272796630859
  batch 200 loss: 1.7382672762870788
  batch 250 loss: 1.7492590403556825
  batch 300 loss: 1.683076593875885
  batch 350 loss: 1.6154132580757141
  batch 400 loss: 1.6045380449295044
  batch 450 loss: 1.565980293750763
  batch 500 loss: 1.5404335427284241
  batch 550 loss: 1.5381686735153197
  batch 600 loss: 1.5041899943351746
  batch 650 loss: 1.5425353932380677
  batch 700 loss: 1.47338139295578
  batch 750 loss: 1.463669695854187
  batch 800 loss: 1.4105642938613892
  batch 850 loss: 1.4295848298072815
  batch 900 loss: 1.4582900834083556
LOSS train 1.45829 valid 1.44057, valid PER 42.97%
EPOCH 3:
  batch 50 loss: 1.3754146075248719
  batch 100 loss: 1.348217978477478
  batch 150 loss: 1.3427393245697021
  batch 200 loss: 1.3175139737129211
  batch 250 loss: 1.3202764010429382
  batch 300 loss: 1.3206009781360626
  batch 350 loss: 1.3615851712226867
  batch 400 loss: 1.3214637362957
  batch 450 loss: 1.3174978852272035
  batch 500 loss: 1.2686486482620238
  batch 550 loss: 1.2918323826789857
  batch 600 loss: 1.2491183352470399
  batch 650 loss: 1.2338648450374603
  batch 700 loss: 1.241963175535202
  batch 750 loss: 1.3174241507053375
  batch 800 loss: 1.223318603038788
  batch 850 loss: 1.2531667292118072
  batch 900 loss: 1.1806548976898192
LOSS train 1.18065 valid 1.27972, valid PER 39.59%
EPOCH 4:
  batch 50 loss: 1.171686028242111
  batch 100 loss: 1.171832311153412
  batch 150 loss: 1.1362399578094482
  batch 200 loss: 1.1767559516429902
  batch 250 loss: 1.1884288120269775
  batch 300 loss: 1.2073827254772187
  batch 350 loss: 1.1029625296592713
  batch 400 loss: 1.1823162698745728
  batch 450 loss: 1.1314907550811768
  batch 500 loss: 1.13955215215683
  batch 550 loss: 1.1564825236797334
  batch 600 loss: 1.1766955637931824
  batch 650 loss: 1.162142448425293
  batch 700 loss: 1.1214001059532166
  batch 750 loss: 1.0855784475803376
  batch 800 loss: 1.0820892918109895
  batch 850 loss: 1.104748764038086
  batch 900 loss: 1.154289299249649
LOSS train 1.15429 valid 1.09554, valid PER 34.32%
EPOCH 5:
  batch 50 loss: 1.0584912109375
  batch 100 loss: 1.0582166337966918
  batch 150 loss: 1.08384401679039
  batch 200 loss: 1.039291628599167
  batch 250 loss: 1.0556983077526092
  batch 300 loss: 1.0513597989082337
  batch 350 loss: 1.0671792471408843
  batch 400 loss: 1.0684865891933442
  batch 450 loss: 1.0428996646404267
  batch 500 loss: 1.0475770294666291
  batch 550 loss: 1.0030294644832611
  batch 600 loss: 1.0902329087257385
  batch 650 loss: 1.0422575223445891
  batch 700 loss: 1.090087329149246
  batch 750 loss: 1.0044739055633545
  batch 800 loss: 1.0566023480892182
  batch 850 loss: 1.0413222539424896
  batch 900 loss: 1.0544361174106598
LOSS train 1.05444 valid 1.10078, valid PER 34.41%
EPOCH 6:
  batch 50 loss: 1.0169016671180726
  batch 100 loss: 0.9648129057884216
  batch 150 loss: 0.9699476659297943
  batch 200 loss: 0.9830949127674102
  batch 250 loss: 1.0151757764816285
  batch 300 loss: 0.9840067791938781
  batch 350 loss: 0.9862327742576599
  batch 400 loss: 0.9936733198165894
  batch 450 loss: 1.0187568390369415
  batch 500 loss: 0.9814568364620209
  batch 550 loss: 1.0181183767318727
  batch 600 loss: 0.9631828951835633
  batch 650 loss: 1.0037511682510376
  batch 700 loss: 0.9770260834693909
  batch 750 loss: 0.9643839800357819
  batch 800 loss: 0.9631914567947387
  batch 850 loss: 0.936995964050293
  batch 900 loss: 0.9712547791004181
LOSS train 0.97125 valid 1.04747, valid PER 31.39%
EPOCH 7:
  batch 50 loss: 0.9414885413646698
  batch 100 loss: 0.9538273549079895
  batch 150 loss: 0.9292633020877838
  batch 200 loss: 0.9207591235637664
  batch 250 loss: 0.9248557150363922
  batch 300 loss: 0.921144243478775
  batch 350 loss: 0.9318618321418762
  batch 400 loss: 0.9267473065853119
  batch 450 loss: 0.9093205535411835
  batch 500 loss: 0.9391056513786316
  batch 550 loss: 0.9557191050052642
  batch 600 loss: 0.9628584229946137
  batch 650 loss: 0.9583144617080689
  batch 700 loss: 0.9579983544349671
  batch 750 loss: 0.9292770040035248
  batch 800 loss: 0.9342539846897125
  batch 850 loss: 0.9634104669094086
  batch 900 loss: 0.9797376537322998
LOSS train 0.97974 valid 1.01073, valid PER 31.36%
EPOCH 8:
  batch 50 loss: 0.9244132423400879
  batch 100 loss: 0.8903339231014251
  batch 150 loss: 0.891614727973938
  batch 200 loss: 0.8488960123062134
  batch 250 loss: 0.9160653328895569
  batch 300 loss: 0.8935948729515075
  batch 350 loss: 0.9286782550811767
  batch 400 loss: 0.9066472542285919
  batch 450 loss: 0.913714063167572
  batch 500 loss: 0.9327447605133057
  batch 550 loss: 0.8717656898498535
  batch 600 loss: 0.9051663374900818
  batch 650 loss: 0.9161551117897033
  batch 700 loss: 0.8777144104242325
  batch 750 loss: 0.8849326241016388
  batch 800 loss: 0.9024747914075851
  batch 850 loss: 0.9071580481529236
  batch 900 loss: 0.9057669496536255
LOSS train 0.90577 valid 1.01102, valid PER 31.79%
EPOCH 9:
  batch 50 loss: 0.8279851222038269
  batch 100 loss: 0.8629206120967865
  batch 150 loss: 0.9064450585842132
  batch 200 loss: 0.8357746350765228
  batch 250 loss: 0.8905804276466369
  batch 300 loss: 0.8604696452617645
  batch 350 loss: 0.903979400396347
  batch 400 loss: 0.8781547462940216
  batch 450 loss: 0.8666535627841949
  batch 500 loss: 0.8328048419952393
  batch 550 loss: 0.8722628509998321
  batch 600 loss: 0.9326833391189575
  batch 650 loss: 0.8905410397052765
  batch 700 loss: 0.8541132009029389
  batch 750 loss: 0.9009369730949401
  batch 800 loss: 0.90361563205719
  batch 850 loss: 0.9240920817852021
  batch 900 loss: 0.8474111640453339
LOSS train 0.84741 valid 0.99771, valid PER 30.24%
EPOCH 10:
  batch 50 loss: 0.8216204190254212
  batch 100 loss: 0.8306680965423584
  batch 150 loss: 0.8921456527709961
  batch 200 loss: 0.8709427440166473
  batch 250 loss: 0.8886632335186004
  batch 300 loss: 0.8610864269733429
  batch 350 loss: 0.8722131383419037
  batch 400 loss: 0.831753917336464
  batch 450 loss: 0.8284512734413148
  batch 500 loss: 0.8941307401657105
  batch 550 loss: 0.8769049835205078
  batch 600 loss: 0.9307836210727691
  batch 650 loss: 0.89542524933815
  batch 700 loss: 0.9185824882984162
  batch 750 loss: 0.8834910464286804
  batch 800 loss: 0.9246191692352295
  batch 850 loss: 0.9448072695732117
  batch 900 loss: 0.9748655426502227
LOSS train 0.97487 valid 1.01694, valid PER 31.83%
EPOCH 11:
  batch 50 loss: 0.8485492181777954
  batch 100 loss: 0.820611628293991
  batch 150 loss: 0.8276778149604798
  batch 200 loss: 0.8939319694042206
  batch 250 loss: 0.8917300426959991
  batch 300 loss: 0.8484177029132843
  batch 350 loss: 0.8487153398990631
  batch 400 loss: 0.8808078444004059
  batch 450 loss: 0.9676743543148041
  batch 500 loss: 0.8373047232627868
  batch 550 loss: 0.8939454829692841
  batch 600 loss: 0.9130613434314728
  batch 650 loss: 0.9416260516643524
  batch 700 loss: 0.8613484525680541
  batch 750 loss: 0.8687417721748352
  batch 800 loss: 0.8959432029724121
  batch 850 loss: 0.9025949585437775
  batch 900 loss: 0.8935827922821045
LOSS train 0.89358 valid 1.03636, valid PER 30.87%
EPOCH 12:
  batch 50 loss: 0.9061684226989746
  batch 100 loss: 0.8634519362449646
  batch 150 loss: 0.8554824674129486
  batch 200 loss: 0.8413026994466781
  batch 250 loss: 0.8876808571815491
  batch 300 loss: 0.858442474603653
  batch 350 loss: 0.8465900361537934
  batch 400 loss: 0.9644994413852692
  batch 450 loss: 0.8940973341464996
  batch 500 loss: 0.8740331292152405
  batch 550 loss: 0.8281598508358001
  batch 600 loss: 0.8099180030822754
  batch 650 loss: 0.8625523149967194
  batch 700 loss: 0.8506462740898132
  batch 750 loss: 0.8427337610721588
  batch 800 loss: 0.8326007330417633
  batch 850 loss: 0.9202548706531525
  batch 900 loss: 0.8976286721229553
LOSS train 0.89763 valid 0.96984, valid PER 29.72%
EPOCH 13:
  batch 50 loss: 0.7957581722736359
  batch 100 loss: 0.8239399456977844
  batch 150 loss: 0.8268624711036682
  batch 200 loss: 0.8231060695648194
  batch 250 loss: 0.8208839643001556
  batch 300 loss: 0.8115423321723938
  batch 350 loss: 0.874215737581253
  batch 400 loss: 0.8497342360019684
  batch 450 loss: 0.8292905855178833
  batch 500 loss: 0.8045495593547821
  batch 550 loss: 0.8591709184646606
  batch 600 loss: 0.8207103180885315
  batch 650 loss: 0.854838434457779
  batch 700 loss: 0.8535008275508881
  batch 750 loss: 0.7866957628726959
  batch 800 loss: 0.8300646781921387
  batch 850 loss: 0.8743822586536407
  batch 900 loss: 0.8601409983634949
LOSS train 0.86014 valid 1.42559, valid PER 37.99%
EPOCH 14:
  batch 50 loss: 0.9467106151580811
  batch 100 loss: 0.8320024311542511
  batch 150 loss: 0.8190518021583557
  batch 200 loss: 0.8373590064048767
  batch 250 loss: 0.8501471924781799
  batch 300 loss: 0.8823044395446777
  batch 350 loss: 0.8325548589229583
  batch 400 loss: 0.861777263879776
  batch 450 loss: 0.8492839503288269
  batch 500 loss: 0.8575034773349762
  batch 550 loss: 0.8814539468288421
  batch 600 loss: 0.8128801661729813
  batch 650 loss: 0.8464552354812622
  batch 700 loss: 0.8541323828697205
  batch 750 loss: 0.8377352809906006
  batch 800 loss: 0.822875167131424
  batch 850 loss: 0.8572899222373962
  batch 900 loss: 0.8330782508850098
LOSS train 0.83308 valid 1.00308, valid PER 31.00%
EPOCH 15:
  batch 50 loss: 0.7872589612007141
  batch 100 loss: 0.7731189984083175
  batch 150 loss: 0.7939936876296997
  batch 200 loss: 0.8804719245433807
  batch 250 loss: 0.886493433713913
  batch 300 loss: 0.8661602282524109
  batch 350 loss: 0.8114146733283997
  batch 400 loss: 0.7986187767982483
  batch 450 loss: 0.8523325562477112
  batch 500 loss: 0.8091304075717926
  batch 550 loss: 0.843639874458313
  batch 600 loss: 0.8816717600822449
  batch 650 loss: 0.8875223255157471
  batch 700 loss: 0.8786977362632752
  batch 750 loss: 0.925776994228363
  batch 800 loss: 0.8743580067157746
  batch 850 loss: 0.8710060954093933
  batch 900 loss: 0.889225766658783
LOSS train 0.88923 valid 1.02619, valid PER 30.47%
EPOCH 16:
  batch 50 loss: 0.8492910945415497
  batch 100 loss: 0.813332793712616
  batch 150 loss: 0.7796917617321014
  batch 200 loss: 0.7969992899894714
  batch 250 loss: 0.8163267862796784
  batch 300 loss: 0.7869287645816803
  batch 350 loss: 0.8793524396419525
  batch 400 loss: 0.8374515891075134
  batch 450 loss: 0.8495911180973053
  batch 500 loss: 1.1343919503688813
  batch 550 loss: 1.1054955625534058
  batch 600 loss: 0.9952572667598725
  batch 650 loss: 0.9896415591239929
  batch 700 loss: 0.9160767829418183
  batch 750 loss: 0.92388720870018
  batch 800 loss: 0.9158842730522155
  batch 850 loss: 0.8947094440460205
  batch 900 loss: 0.8955817651748658
LOSS train 0.89558 valid 1.03904, valid PER 30.66%
EPOCH 17:
  batch 50 loss: 0.8179370158910751
  batch 100 loss: 0.8377710843086242
  batch 150 loss: 0.8121618509292603
  batch 200 loss: 0.8181302511692047
  batch 250 loss: 0.8188937020301819
  batch 300 loss: 0.8417966878414154
  batch 350 loss: 0.8100130331516265
  batch 400 loss: 0.8659286665916442
  batch 450 loss: 0.8343212568759918
  batch 500 loss: 0.8205467689037323
  batch 550 loss: 0.8617727208137512
  batch 600 loss: 0.9148214840888977
  batch 650 loss: 0.886120582818985
  batch 700 loss: 0.871794456243515
  batch 750 loss: 0.8182835257053376
  batch 800 loss: 0.8549202728271484
  batch 850 loss: 0.8534037649631501
  batch 900 loss: 0.8251492798328399
LOSS train 0.82515 valid 0.99112, valid PER 29.68%
EPOCH 18:
  batch 50 loss: 0.7993601763248444
  batch 100 loss: 0.7997757077217102
  batch 150 loss: 0.8358851528167724
  batch 200 loss: 0.8142752981185913
  batch 250 loss: 0.8607361137866973
  batch 300 loss: 0.8397121787071228
  batch 350 loss: 0.8763612711429596
  batch 400 loss: 0.8107181477546692
  batch 450 loss: 0.9851873874664306
  batch 500 loss: 0.9428056120872498
  batch 550 loss: 0.916419129371643
  batch 600 loss: 0.8966415536403656
  batch 650 loss: 0.8737784934043884
  batch 700 loss: 0.9330100965499878
  batch 750 loss: 0.8808476638793945
  batch 800 loss: 0.920814733505249
  batch 850 loss: 0.8898317503929138
  batch 900 loss: 0.90511927485466
LOSS train 0.90512 valid 1.23304, valid PER 37.02%
EPOCH 19:
  batch 50 loss: 0.8889532256126403
  batch 100 loss: 0.8797168159484863
  batch 150 loss: 0.8380221927165985
  batch 200 loss: 0.8844316351413727
  batch 250 loss: 0.9127995383739471
  batch 300 loss: 0.9112786936759949
  batch 350 loss: 0.8446701765060425
  batch 400 loss: 0.8497716355323791
  batch 450 loss: 0.8532750558853149
  batch 500 loss: 1.1440767920017243
  batch 550 loss: 1.141254299879074
  batch 600 loss: 1.0763216137886047
  batch 650 loss: 1.1233911740779876
  batch 700 loss: 1.0179500043392182
  batch 750 loss: 0.9723390769958496
  batch 800 loss: 0.9824925589561463
  batch 850 loss: 1.0138010859489441
  batch 900 loss: 0.9879307639598847
LOSS train 0.98793 valid 1.12770, valid PER 35.01%
EPOCH 20:
  batch 50 loss: 0.9186349856853485
  batch 100 loss: 0.8845475816726684
  batch 150 loss: 0.8840619969367981
  batch 200 loss: 0.9155627357959747
  batch 250 loss: 0.8855769443511963
  batch 300 loss: 0.9158285403251648
  batch 350 loss: 0.8769826257228851
  batch 400 loss: 0.8820865654945373
  batch 450 loss: 0.878920247554779
  batch 500 loss: 0.8479629290103913
  batch 550 loss: 0.933747011423111
  batch 600 loss: 0.8583966970443726
  batch 650 loss: 0.8773391425609589
  batch 700 loss: 0.9021602916717529
  batch 750 loss: 0.8446184647083282
  batch 800 loss: 0.8818555331230163
  batch 850 loss: 0.8750871169567108
  batch 900 loss: 0.8942744100093841
LOSS train 0.89427 valid 1.04299, valid PER 31.56%
train_loss
[1.9725256371498108, 1.4582900834083556, 1.1806548976898192, 1.154289299249649, 1.0544361174106598, 0.9712547791004181, 0.9797376537322998, 0.9057669496536255, 0.8474111640453339, 0.9748655426502227, 0.8935827922821045, 0.8976286721229553, 0.8601409983634949, 0.8330782508850098, 0.889225766658783, 0.8955817651748658, 0.8251492798328399, 0.90511927485466, 0.9879307639598847, 0.8942744100093841]
valid_loss
[1.8084592819213867, 1.4405659437179565, 1.2797222137451172, 1.0955357551574707, 1.1007835865020752, 1.0474716424942017, 1.0107293128967285, 1.0110186338424683, 0.9977094531059265, 1.0169373750686646, 1.0363550186157227, 0.9698395133018494, 1.4255861043930054, 1.0030848979949951, 1.026192545890808, 1.0390441417694092, 0.9911235570907593, 1.2330361604690552, 1.1277034282684326, 1.042987585067749]
valid_per
[65.13798160245301, 42.97427009732036, 39.58805492600986, 34.32209038794827, 34.40874550059992, 31.389148113584852, 31.355819224103453, 31.789094787361684, 30.23596853752833, 31.829089454739368, 30.86921743767498, 29.72270363951473, 37.98826823090254, 31.002532995600586, 30.469270763898148, 30.655912544994003, 29.68270897213705, 37.021730435941876, 35.008665511265164, 31.56245833888815]
Training finished in 3.0 minutes.
Model saved to checkpoints/20231208_144944/model_12
Loading model from checkpoints/20231208_144944/model_12
SUB: 16.88%, DEL: 12.16%, INS: 2.25%, COR: 70.96%, PER: 31.29%
