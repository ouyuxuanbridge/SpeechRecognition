Namespace(seed=123, train_json='/rds/user/yo279/hpc-work/MLMI2/fbanks/train_fbank.json', val_json='/rds/user/yo279/hpc-work/MLMI2/fbanks/dev_fbank.json', test_json='/rds/user/yo279/hpc-work/MLMI2/fbanks/test_fbank.json', batch_size=4, num_layers=1, fbank_dims=23, model_dims=128, concat=1, lr=0.3, vocab='vocab_39.txt', report_interval=50, num_epochs=20, dropout_rate=0.5, clip_max_norm=1)
Total number of model parameters is 166952
EPOCH 1:
  batch 50 loss: 4.111374053955078
  batch 100 loss: 3.2322315788269043
  batch 150 loss: 3.154557456970215
  batch 200 loss: 3.0160369348526
  batch 250 loss: 2.7974999809265135
  batch 300 loss: 2.6085858058929445
  batch 350 loss: 2.5013112115859983
  batch 400 loss: 2.4414639329910277
  batch 450 loss: 2.3670832109451294
  batch 500 loss: 2.2581214904785156
  batch 550 loss: 2.198872830867767
  batch 600 loss: 2.162199926376343
  batch 650 loss: 2.1077637791633608
  batch 700 loss: 2.085834250450134
  batch 750 loss: 2.0317633032798765
  batch 800 loss: 1.9899023079872131
  batch 850 loss: 1.9499431705474854
  batch 900 loss: 1.9303727626800538
LOSS train 1.93037 valid 1.86934, valid PER 70.89%
EPOCH 2:
  batch 50 loss: 1.891920747756958
  batch 100 loss: 1.8148627662658692
  batch 150 loss: 1.7911987113952637
  batch 200 loss: 1.7995149111747741
  batch 250 loss: 1.7863645911216737
  batch 300 loss: 1.7562882256507875
  batch 350 loss: 1.6637381720542908
  batch 400 loss: 1.6783226919174195
  batch 450 loss: 1.6458540868759155
  batch 500 loss: 1.6715447854995729
  batch 550 loss: 1.6605012965202333
  batch 600 loss: 1.6023429799079896
  batch 650 loss: 1.633615825176239
  batch 700 loss: 1.6093069291114808
  batch 750 loss: 1.5919665837287902
  batch 800 loss: 1.5319362592697143
  batch 850 loss: 1.5307314443588256
  batch 900 loss: 1.550739607810974
LOSS train 1.55074 valid 1.51599, valid PER 54.18%
EPOCH 3:
  batch 50 loss: 1.500983009338379
  batch 100 loss: 1.4697128701210023
  batch 150 loss: 1.4783700370788575
  batch 200 loss: 1.4441714215278625
  batch 250 loss: 1.431977880001068
  batch 300 loss: 1.421961088180542
  batch 350 loss: 1.470138566493988
  batch 400 loss: 1.443091025352478
  batch 450 loss: 1.3833017086982726
  batch 500 loss: 1.3694182324409485
  batch 550 loss: 1.3880154657363892
  batch 600 loss: 1.3599830090999603
  batch 650 loss: 1.3074354207515717
  batch 700 loss: 1.3482495868206024
  batch 750 loss: 1.3916931676864623
  batch 800 loss: 1.3062434911727905
  batch 850 loss: 1.3356454062461853
  batch 900 loss: 1.2625060975551605
LOSS train 1.26251 valid 1.34683, valid PER 41.25%
EPOCH 4:
  batch 50 loss: 1.2576044023036956
  batch 100 loss: 1.2755777966976165
  batch 150 loss: 1.2307439732551575
  batch 200 loss: 1.2656732392311096
  batch 250 loss: 1.2548937487602234
  batch 300 loss: 1.2692409265041351
  batch 350 loss: 1.1761337065696715
  batch 400 loss: 1.2329344296455382
  batch 450 loss: 1.2151688599586488
  batch 500 loss: 1.197928626537323
  batch 550 loss: 1.2195314407348632
  batch 600 loss: 1.2305466508865357
  batch 650 loss: 1.222101820707321
  batch 700 loss: 1.1852695739269257
  batch 750 loss: 1.1753770005702973
  batch 800 loss: 1.1321869194507599
  batch 850 loss: 1.1825866222381591
  batch 900 loss: 1.2231319105625154
LOSS train 1.22313 valid 1.17943, valid PER 36.69%
EPOCH 5:
  batch 50 loss: 1.1474226486682892
  batch 100 loss: 1.1278820133209229
  batch 150 loss: 1.1756824827194214
  batch 200 loss: 1.0996005785465242
  batch 250 loss: 1.1298918652534484
  batch 300 loss: 1.1366041469573975
  batch 350 loss: 1.124661955833435
  batch 400 loss: 1.111872695684433
  batch 450 loss: 1.1110135531425476
  batch 500 loss: 1.1176244878768922
  batch 550 loss: 1.0684922230243683
  batch 600 loss: 1.173160227537155
  batch 650 loss: 1.0956209051609038
  batch 700 loss: 1.150947812795639
  batch 750 loss: 1.0628729701042174
  batch 800 loss: 1.115463182926178
  batch 850 loss: 1.0898149633407592
  batch 900 loss: 1.1078204774856568
LOSS train 1.10782 valid 1.11376, valid PER 34.03%
EPOCH 6:
  batch 50 loss: 1.1218694758415222
  batch 100 loss: 1.050147875547409
  batch 150 loss: 1.0474804866313934
  batch 200 loss: 1.052495572566986
  batch 250 loss: 1.0834343087673188
  batch 300 loss: 1.0590424060821533
  batch 350 loss: 1.0647550463676452
  batch 400 loss: 1.047471523284912
  batch 450 loss: 1.0915609884262085
  batch 500 loss: 1.0622389090061188
  batch 550 loss: 1.0742631614208222
  batch 600 loss: 1.0459886300563812
  batch 650 loss: 1.059069139957428
  batch 700 loss: 1.0600193107128144
  batch 750 loss: 1.0346941041946411
  batch 800 loss: 1.038244000673294
  batch 850 loss: 1.0273919820785522
  batch 900 loss: 1.0468603086471557
LOSS train 1.04686 valid 1.10569, valid PER 33.77%
EPOCH 7:
  batch 50 loss: 1.0207418990135193
  batch 100 loss: 1.0411937427520752
  batch 150 loss: 1.014675452709198
  batch 200 loss: 1.0105320036411285
  batch 250 loss: 1.0083258938789368
  batch 300 loss: 0.9864441955089569
  batch 350 loss: 1.0020102965831756
  batch 400 loss: 1.0021715557575226
  batch 450 loss: 0.9958704555034638
  batch 500 loss: 0.9904893100261688
  batch 550 loss: 1.0184087646007538
  batch 600 loss: 1.0115397953987122
  batch 650 loss: 1.0008107233047485
  batch 700 loss: 1.006619349718094
  batch 750 loss: 0.9968583333492279
  batch 800 loss: 0.9989259624481202
  batch 850 loss: 1.006249487400055
  batch 900 loss: 1.0525704872608186
LOSS train 1.05257 valid 1.05016, valid PER 33.10%
EPOCH 8:
  batch 50 loss: 0.9713313901424407
  batch 100 loss: 0.9618810105323792
  batch 150 loss: 0.945034670829773
  batch 200 loss: 0.9438126516342163
  batch 250 loss: 0.9805654942989349
  batch 300 loss: 0.9046337604522705
  batch 350 loss: 0.9766476106643677
  batch 400 loss: 0.9563752770423889
  batch 450 loss: 0.9742247831821441
  batch 500 loss: 0.9962119531631469
  batch 550 loss: 0.9421412372589111
  batch 600 loss: 0.9779888415336608
  batch 650 loss: 0.9967716193199158
  batch 700 loss: 0.9462147510051727
  batch 750 loss: 0.9485581874847412
  batch 800 loss: 0.976131625175476
  batch 850 loss: 0.9614614903926849
  batch 900 loss: 0.9788692486286164
LOSS train 0.97887 valid 1.03393, valid PER 31.47%
EPOCH 9:
  batch 50 loss: 0.8954510390758514
  batch 100 loss: 0.9337309193611145
  batch 150 loss: 0.9205306196212768
  batch 200 loss: 0.9013784515857697
  batch 250 loss: 0.9423125958442689
  batch 300 loss: 0.9428240692615509
  batch 350 loss: 0.9810162520408631
  batch 400 loss: 0.9251563465595245
  batch 450 loss: 0.9454693675041199
  batch 500 loss: 0.8998959219455719
  batch 550 loss: 0.9389590871334076
  batch 600 loss: 0.9330801498889923
  batch 650 loss: 0.9079215371608734
  batch 700 loss: 0.8995951056480408
  batch 750 loss: 0.9210957026481629
  batch 800 loss: 0.9455359768867493
  batch 850 loss: 0.9465413439273834
  batch 900 loss: 0.902289193868637
LOSS train 0.90229 valid 1.00620, valid PER 31.12%
EPOCH 10:
  batch 50 loss: 0.8543512904644013
  batch 100 loss: 0.90038902759552
  batch 150 loss: 0.9042899143695832
  batch 200 loss: 0.9161584484577179
  batch 250 loss: 0.9153823983669281
  batch 300 loss: 0.8731092751026154
  batch 350 loss: 0.9003214597702026
  batch 400 loss: 0.8585145747661591
  batch 450 loss: 0.8617583382129669
  batch 500 loss: 0.902544857263565
  batch 550 loss: 0.9239502322673797
  batch 600 loss: 0.8948560702800751
  batch 650 loss: 0.8644835948944092
  batch 700 loss: 0.9050562381744385
  batch 750 loss: 0.8911832964420319
  batch 800 loss: 0.8869341063499451
  batch 850 loss: 0.9061764228343964
  batch 900 loss: 0.9121777379512787
LOSS train 0.91218 valid 0.98712, valid PER 31.64%
EPOCH 11:
  batch 50 loss: 0.8413124918937683
  batch 100 loss: 0.8261870503425598
  batch 150 loss: 0.8369556522369385
  batch 200 loss: 0.8995068764686585
  batch 250 loss: 0.8587825167179107
  batch 300 loss: 0.8366606795787811
  batch 350 loss: 0.8467785120010376
  batch 400 loss: 0.8745804679393768
  batch 450 loss: 0.8792452156543732
  batch 500 loss: 0.8544913685321808
  batch 550 loss: 0.8506355345249176
  batch 600 loss: 0.8455930936336518
  batch 650 loss: 0.9132006740570069
  batch 700 loss: 0.8368726181983948
  batch 750 loss: 0.8507295882701874
  batch 800 loss: 0.8828449845314026
  batch 850 loss: 0.9089074158668518
  batch 900 loss: 0.8942625761032105
LOSS train 0.89426 valid 0.96660, valid PER 29.80%
EPOCH 12:
  batch 50 loss: 0.8483576953411103
  batch 100 loss: 0.8362661993503571
  batch 150 loss: 0.8242241895198822
  batch 200 loss: 0.8185268771648407
  batch 250 loss: 0.8425877010822296
  batch 300 loss: 0.8387173235416412
  batch 350 loss: 0.8201861214637757
  batch 400 loss: 0.8493798625469208
  batch 450 loss: 0.8432434689998627
  batch 500 loss: 0.8565337252616883
  batch 550 loss: 0.7821459710597992
  batch 600 loss: 0.8167912793159485
  batch 650 loss: 0.8611630308628082
  batch 700 loss: 0.8630789363384247
  batch 750 loss: 0.8409370100498199
  batch 800 loss: 0.8156644076108932
  batch 850 loss: 0.89134157538414
  batch 900 loss: 0.8684525406360626
LOSS train 0.86845 valid 0.94160, valid PER 29.71%
EPOCH 13:
  batch 50 loss: 0.7775066304206848
  batch 100 loss: 0.8161927008628845
  batch 150 loss: 0.7883533960580826
  batch 200 loss: 0.8095857882499695
  batch 250 loss: 0.7911694079637528
  batch 300 loss: 0.7915155839920044
  batch 350 loss: 0.8018572795391082
  batch 400 loss: 0.8164297473430634
  batch 450 loss: 0.8207476711273194
  batch 500 loss: 0.8026688730716706
  batch 550 loss: 0.8417266166210174
  batch 600 loss: 0.8102530896663666
  batch 650 loss: 0.8302598661184311
  batch 700 loss: 0.8379757750034332
  batch 750 loss: 0.7918935990333558
  batch 800 loss: 0.7981258666515351
  batch 850 loss: 0.8454861283302307
  batch 900 loss: 0.824636687040329
LOSS train 0.82464 valid 0.94994, valid PER 29.36%
EPOCH 14:
  batch 50 loss: 0.7887165987491608
  batch 100 loss: 0.8024014902114868
  batch 150 loss: 0.7813051581382752
  batch 200 loss: 0.7860406643152237
  batch 250 loss: 0.7855828785896302
  batch 300 loss: 0.8239035373926162
  batch 350 loss: 0.7746214497089386
  batch 400 loss: 0.7698955154418945
  batch 450 loss: 0.7733075964450836
  batch 500 loss: 0.7950181496143341
  batch 550 loss: 0.8102437889575959
  batch 600 loss: 0.7583886349201202
  batch 650 loss: 0.8044624853134156
  batch 700 loss: 0.8358940410614014
  batch 750 loss: 0.7946828496456146
  batch 800 loss: 0.7519301962852478
  batch 850 loss: 0.7930168986320496
  batch 900 loss: 0.789150801897049
LOSS train 0.78915 valid 0.94884, valid PER 29.52%
EPOCH 15:
  batch 50 loss: 0.7621307110786438
  batch 100 loss: 0.7512677544355393
  batch 150 loss: 0.7637318563461304
  batch 200 loss: 0.7801846754550934
  batch 250 loss: 0.7909236371517181
  batch 300 loss: 0.7590349316596985
  batch 350 loss: 0.7828916138410569
  batch 400 loss: 0.7682424175739289
  batch 450 loss: 0.777904771566391
  batch 500 loss: 0.7494874048233032
  batch 550 loss: 0.7604088044166565
  batch 600 loss: 0.7885162520408631
  batch 650 loss: 0.806890938282013
  batch 700 loss: 0.8092728424072265
  batch 750 loss: 0.8047879588603973
  batch 800 loss: 0.764871426820755
  batch 850 loss: 0.7487259042263031
  batch 900 loss: 0.7557825702428818
LOSS train 0.75578 valid 0.95219, valid PER 29.12%
EPOCH 16:
  batch 50 loss: 0.7655809342861175
  batch 100 loss: 0.7217423242330551
  batch 150 loss: 0.7470411384105682
  batch 200 loss: 0.7471715545654297
  batch 250 loss: 0.7589190661907196
  batch 300 loss: 0.7613118308782577
  batch 350 loss: 0.7504167586565018
  batch 400 loss: 0.7615963244438171
  batch 450 loss: 0.7809787726402283
  batch 500 loss: 0.7348233771324157
  batch 550 loss: 0.7643582940101623
  batch 600 loss: 0.755976095199585
  batch 650 loss: 0.7566962385177612
  batch 700 loss: 0.7487228357791901
  batch 750 loss: 0.7580945241451263
  batch 800 loss: 0.7713568043708802
  batch 850 loss: 0.7401437383890151
  batch 900 loss: 0.7372604620456695
LOSS train 0.73726 valid 0.92010, valid PER 28.48%
EPOCH 17:
  batch 50 loss: 0.7182261645793915
  batch 100 loss: 0.7216128051280976
  batch 150 loss: 0.7108823150396347
  batch 200 loss: 0.7084390443563461
  batch 250 loss: 0.7895786356925965
  batch 300 loss: 0.7788508087396622
  batch 350 loss: 0.7218073672056198
  batch 400 loss: 0.7778591299057007
  batch 450 loss: 0.7716219162940979
  batch 500 loss: 0.7305334460735321
  batch 550 loss: 0.7476310247182846
  batch 600 loss: 0.776483777165413
  batch 650 loss: 0.7417481851577759
  batch 700 loss: 0.7380303877592087
  batch 750 loss: 0.7034959405660629
  batch 800 loss: 0.7255028057098388
  batch 850 loss: 0.7423897439241409
  batch 900 loss: 0.7330318814516068
LOSS train 0.73303 valid 0.94915, valid PER 28.57%
EPOCH 18:
  batch 50 loss: 0.7306767416000366
  batch 100 loss: 0.7267857921123505
  batch 150 loss: 0.7323367077112198
  batch 200 loss: 0.7303392863273621
  batch 250 loss: 0.7188730978965759
  batch 300 loss: 0.7055027604103088
  batch 350 loss: 0.7283593219518661
  batch 400 loss: 0.696026102900505
  batch 450 loss: 0.7419168603420258
  batch 500 loss: 0.7266485834121704
  batch 550 loss: 0.765227227807045
  batch 600 loss: 0.7207325685024262
  batch 650 loss: 0.7156098926067352
  batch 700 loss: 0.7542549347877503
  batch 750 loss: 0.719370403289795
  batch 800 loss: 0.7080957698822021
  batch 850 loss: 0.7063917893171311
  batch 900 loss: 0.7384853684902191
LOSS train 0.73849 valid 0.93207, valid PER 28.63%
EPOCH 19:
  batch 50 loss: 0.655540441274643
  batch 100 loss: 0.657784942984581
  batch 150 loss: 0.6816122210025788
  batch 200 loss: 0.6840726900100708
  batch 250 loss: 0.7023683649301529
  batch 300 loss: 0.6995059806108475
  batch 350 loss: 0.6904124480485916
  batch 400 loss: 0.7051790469884872
  batch 450 loss: 0.7188986563682556
  batch 500 loss: 0.7050649464130402
  batch 550 loss: 0.6988114893436432
  batch 600 loss: 0.7029577267169952
  batch 650 loss: 0.7693463772535324
  batch 700 loss: 0.706824569106102
  batch 750 loss: 0.6889292877912522
  batch 800 loss: 0.7197165024280548
  batch 850 loss: 0.7417292892932892
  batch 900 loss: 0.7303150904178619
LOSS train 0.73032 valid 0.94454, valid PER 28.63%
EPOCH 20:
  batch 50 loss: 0.6661671525239945
  batch 100 loss: 0.6692186236381531
  batch 150 loss: 0.6708173102140427
  batch 200 loss: 0.7015918815135955
  batch 250 loss: 0.6767358380556107
  batch 300 loss: 0.7103799676895142
  batch 350 loss: 0.6787285923957824
  batch 400 loss: 0.6752781093120575
  batch 450 loss: 0.7041605365276337
  batch 500 loss: 0.6486263567209244
  batch 550 loss: 0.7310080277919769
  batch 600 loss: 0.664885521531105
  batch 650 loss: 0.7273416811227799
  batch 700 loss: 0.7320068168640137
  batch 750 loss: 0.703622824549675
  batch 800 loss: 0.7399892187118531
  batch 850 loss: 0.7388123428821564
  batch 900 loss: 0.7702309864759446
LOSS train 0.77023 valid 0.95331, valid PER 28.61%
train_loss
[1.9303727626800538, 1.550739607810974, 1.2625060975551605, 1.2231319105625154, 1.1078204774856568, 1.0468603086471557, 1.0525704872608186, 0.9788692486286164, 0.902289193868637, 0.9121777379512787, 0.8942625761032105, 0.8684525406360626, 0.824636687040329, 0.789150801897049, 0.7557825702428818, 0.7372604620456695, 0.7330318814516068, 0.7384853684902191, 0.7303150904178619, 0.7702309864759446]
valid_loss
[1.8693374395370483, 1.5159882307052612, 1.346825361251831, 1.1794277429580688, 1.113760232925415, 1.1056941747665405, 1.050160527229309, 1.0339322090148926, 1.0062015056610107, 0.987122118473053, 0.966601550579071, 0.9416027069091797, 0.9499381184577942, 0.9488427639007568, 0.952192485332489, 0.9200978875160217, 0.9491516947746277, 0.9320732951164246, 0.9445444941520691, 0.9533118009567261]
valid_per
[70.89054792694307, 54.17944274096788, 41.24783362218371, 36.688441541127844, 34.02879616051193, 33.76883082255699, 33.10225303292894, 31.469137448340224, 31.122516997733634, 31.635781895747233, 29.802692974270094, 29.70937208372217, 29.356085855219305, 29.522730302626314, 29.122783628849486, 28.47620317291028, 28.569524063458207, 28.62951606452473, 28.62951606452473, 28.609518730835887]
Training finished in 3.0 minutes.
Model saved to checkpoints/20231208_021147/model_16
Loading model from checkpoints/20231208_021147/model_16
SUB: 17.03%, DEL: 10.27%, INS: 2.68%, COR: 72.70%, PER: 29.99%
