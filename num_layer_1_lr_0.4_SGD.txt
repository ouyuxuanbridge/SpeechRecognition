Namespace(seed=123, train_json='/rds/user/yo279/hpc-work/MLMI2/fbanks/train_fbank.json', val_json='/rds/user/yo279/hpc-work/MLMI2/fbanks/dev_fbank.json', test_json='/rds/user/yo279/hpc-work/MLMI2/fbanks/test_fbank.json', batch_size=4, num_layers=1, fbank_dims=23, model_dims=128, concat=1, lr=0.4, vocab='vocab_39.txt', report_interval=50, num_epochs=20, dropout_rate=0.5, clip_max_norm=1)
Total number of model parameters is 166952
EPOCH 1:
  batch 50 loss: 4.170553030967713
  batch 100 loss: 3.2205390405654906
  batch 150 loss: 3.030926170349121
  batch 200 loss: 2.8348594760894774
  batch 250 loss: 2.6975295639038084
  batch 300 loss: 2.5676553201675416
  batch 350 loss: 2.4622318172454833
  batch 400 loss: 2.4198222160339355
  batch 450 loss: 2.3718531370162963
  batch 500 loss: 2.2405295848846434
  batch 550 loss: 2.188969111442566
  batch 600 loss: 2.132873065471649
  batch 650 loss: 2.0403476214408873
  batch 700 loss: 2.049694633483887
  batch 750 loss: 1.9523091745376586
  batch 800 loss: 1.9378783893585205
  batch 850 loss: 1.8800512742996216
  batch 900 loss: 1.8731363773345948
LOSS train 1.87314 valid 1.79414, valid PER 68.25%
EPOCH 2:
  batch 50 loss: 1.799919216632843
  batch 100 loss: 1.7463003945350648
  batch 150 loss: 1.7279840064048768
  batch 200 loss: 1.7355619072914124
  batch 250 loss: 1.732412233352661
  batch 300 loss: 1.6921377301216125
  batch 350 loss: 1.5954937744140625
  batch 400 loss: 1.6129087591171265
  batch 450 loss: 1.5691999173164368
  batch 500 loss: 1.5920649838447571
  batch 550 loss: 1.5978166937828064
  batch 600 loss: 1.5455345678329468
  batch 650 loss: 1.5707431960105895
  batch 700 loss: 1.5567670607566833
  batch 750 loss: 1.5256528067588806
  batch 800 loss: 1.4476013493537903
  batch 850 loss: 1.4679353284835814
  batch 900 loss: 1.4829378199577332
LOSS train 1.48294 valid 1.44914, valid PER 46.47%
EPOCH 3:
  batch 50 loss: 1.4412681698799132
  batch 100 loss: 1.3911711049079896
  batch 150 loss: 1.389112756252289
  batch 200 loss: 1.3574462056159973
  batch 250 loss: 1.3523484063148499
  batch 300 loss: 1.3428880453109742
  batch 350 loss: 1.3883341431617737
  batch 400 loss: 1.3512741541862487
  batch 450 loss: 1.3407003211975097
  batch 500 loss: 1.3230779075622559
  batch 550 loss: 1.329051170349121
  batch 600 loss: 1.2851338732242583
  batch 650 loss: 1.247414653301239
  batch 700 loss: 1.2841932916641234
  batch 750 loss: 1.3456453227996825
  batch 800 loss: 1.262346053123474
  batch 850 loss: 1.285298970937729
  batch 900 loss: 1.2323620915412903
LOSS train 1.23236 valid 1.27401, valid PER 39.07%
EPOCH 4:
  batch 50 loss: 1.2107283282279968
  batch 100 loss: 1.2214360022544861
  batch 150 loss: 1.1759064280986786
  batch 200 loss: 1.2083165657520294
  batch 250 loss: 1.2421681666374207
  batch 300 loss: 1.2150895416736602
  batch 350 loss: 1.1506140971183776
  batch 400 loss: 1.1946050453186035
  batch 450 loss: 1.1810048305988312
  batch 500 loss: 1.1752565491199494
  batch 550 loss: 1.2005942583084106
  batch 600 loss: 1.2185519099235536
  batch 650 loss: 1.1847512102127076
  batch 700 loss: 1.1577098858356476
  batch 750 loss: 1.144683529138565
  batch 800 loss: 1.1255279505252838
  batch 850 loss: 1.1650018191337586
  batch 900 loss: 1.1981160581111907
LOSS train 1.19812 valid 1.16805, valid PER 35.76%
EPOCH 5:
  batch 50 loss: 1.1008903658390046
  batch 100 loss: 1.1205140626430512
  batch 150 loss: 1.161530978679657
  batch 200 loss: 1.069210066795349
  batch 250 loss: 1.1061724591255189
  batch 300 loss: 1.110255481004715
  batch 350 loss: 1.1114488458633422
  batch 400 loss: 1.102779847383499
  batch 450 loss: 1.084615296125412
  batch 500 loss: 1.1089004862308502
  batch 550 loss: 1.0616651439666749
  batch 600 loss: 1.1470833837985992
  batch 650 loss: 1.0911572551727295
  batch 700 loss: 1.1127702808380127
  batch 750 loss: 1.0539624774456025
  batch 800 loss: 1.0919994950294494
  batch 850 loss: 1.0816577911376952
  batch 900 loss: 1.0934457099437713
LOSS train 1.09345 valid 1.11778, valid PER 35.01%
EPOCH 6:
  batch 50 loss: 1.0674891209602355
  batch 100 loss: 1.0311124205589295
  batch 150 loss: 1.0081012272834777
  batch 200 loss: 1.0323800039291382
  batch 250 loss: 1.073714131116867
  batch 300 loss: 1.0267719376087188
  batch 350 loss: 1.0423959648609162
  batch 400 loss: 1.020827786922455
  batch 450 loss: 1.0705944311618805
  batch 500 loss: 1.0295284736156463
  batch 550 loss: 1.0597672367095947
  batch 600 loss: 1.0065054404735565
  batch 650 loss: 1.0299986338615417
  batch 700 loss: 1.0328310596942902
  batch 750 loss: 1.0258397698402404
  batch 800 loss: 1.0077131700515747
  batch 850 loss: 1.0124011623859406
  batch 900 loss: 1.0502864336967468
LOSS train 1.05029 valid 1.13162, valid PER 36.30%
EPOCH 7:
  batch 50 loss: 0.9999269318580627
  batch 100 loss: 1.0170587396621704
  batch 150 loss: 1.0041191792488098
  batch 200 loss: 0.9854142916202545
  batch 250 loss: 1.0008938145637511
  batch 300 loss: 0.9719774830341339
  batch 350 loss: 0.9747610712051391
  batch 400 loss: 0.9831493985652924
  batch 450 loss: 0.9948294496536255
  batch 500 loss: 0.9852430593967437
  batch 550 loss: 0.9920260179042816
  batch 600 loss: 0.9843702101707459
  batch 650 loss: 0.9832456350326538
  batch 700 loss: 1.0078021359443665
  batch 750 loss: 0.9648163783550262
  batch 800 loss: 0.984690432548523
  batch 850 loss: 1.0093775713443756
  batch 900 loss: 1.0709785842895507
LOSS train 1.07098 valid 1.07462, valid PER 33.62%
EPOCH 8:
  batch 50 loss: 0.9610790061950684
  batch 100 loss: 0.9548876535892487
  batch 150 loss: 0.937030314207077
  batch 200 loss: 0.9140935575962067
  batch 250 loss: 0.9297104454040528
  batch 300 loss: 0.8848582077026367
  batch 350 loss: 0.9651463663578034
  batch 400 loss: 0.9236177110671997
  batch 450 loss: 0.9661183512210846
  batch 500 loss: 0.9693053209781647
  batch 550 loss: 0.930419282913208
  batch 600 loss: 0.9538641750812531
  batch 650 loss: 0.9805307173728943
  batch 700 loss: 0.9339465725421906
  batch 750 loss: 0.9635327553749085
  batch 800 loss: 0.9554034173488617
  batch 850 loss: 0.9352253603935242
  batch 900 loss: 0.9237043929100036
LOSS train 0.92370 valid 1.02574, valid PER 31.77%
EPOCH 9:
  batch 50 loss: 0.8656435751914978
  batch 100 loss: 0.9024061143398285
  batch 150 loss: 0.9290291607379914
  batch 200 loss: 0.8715954399108887
  batch 250 loss: 0.9136136317253113
  batch 300 loss: 0.9170709347724915
  batch 350 loss: 0.9313087892532349
  batch 400 loss: 0.9026745915412903
  batch 450 loss: 0.904928069114685
  batch 500 loss: 0.9454080307483673
  batch 550 loss: 0.9458928370475769
  batch 600 loss: 0.9431512975692748
  batch 650 loss: 0.9480282413959503
  batch 700 loss: 0.9449098801612854
  batch 750 loss: 0.9276560509204864
  batch 800 loss: 0.9553464448451996
  batch 850 loss: 0.9526770484447479
  batch 900 loss: 0.9000137996673584
LOSS train 0.90001 valid 1.07112, valid PER 32.21%
EPOCH 10:
  batch 50 loss: 0.886785296201706
  batch 100 loss: 0.8940741550922394
  batch 150 loss: 0.9264901006221771
  batch 200 loss: 0.9243061590194702
  batch 250 loss: 0.9042280375957489
  batch 300 loss: 0.8593631994724273
  batch 350 loss: 0.8724482870101928
  batch 400 loss: 0.8345317971706391
  batch 450 loss: 0.8445977187156677
  batch 500 loss: 0.9118803691864014
  batch 550 loss: 0.9295051550865173
  batch 600 loss: 0.8978711688518524
  batch 650 loss: 0.8825414562225342
  batch 700 loss: 0.902765634059906
  batch 750 loss: 0.9440465009212494
  batch 800 loss: 0.9270064473152161
  batch 850 loss: 0.9164420354366303
  batch 900 loss: 0.8993563282489777
LOSS train 0.89936 valid 1.00320, valid PER 31.84%
EPOCH 11:
  batch 50 loss: 0.8348207211494446
  batch 100 loss: 0.8151465570926666
  batch 150 loss: 0.8368434727191925
  batch 200 loss: 0.8749330472946167
  batch 250 loss: 0.8618447983264923
  batch 300 loss: 0.8319437408447266
  batch 350 loss: 0.8922210860252381
  batch 400 loss: 0.8829176557064057
  batch 450 loss: 0.8559926688671112
  batch 500 loss: 0.8587250149250031
  batch 550 loss: 0.8657473993301391
  batch 600 loss: 0.8283634626865387
  batch 650 loss: 0.8822578620910645
  batch 700 loss: 0.8348769652843475
  batch 750 loss: 0.8506387841701507
  batch 800 loss: 0.8764238739013672
  batch 850 loss: 0.9472486603260041
  batch 900 loss: 0.9131957948207855
LOSS train 0.91320 valid 0.98531, valid PER 30.96%
EPOCH 12:
  batch 50 loss: 0.8514831447601319
  batch 100 loss: 0.8341395294666291
  batch 150 loss: 0.8217151069641113
  batch 200 loss: 0.8509037852287292
  batch 250 loss: 0.8820815443992615
  batch 300 loss: 0.8496614611148834
  batch 350 loss: 0.8352987253665924
  batch 400 loss: 0.857674446105957
  batch 450 loss: 0.8670692086219788
  batch 500 loss: 0.8909165048599244
  batch 550 loss: 0.823594777584076
  batch 600 loss: 0.8577869868278504
  batch 650 loss: 0.8672645115852355
  batch 700 loss: 0.8640267586708069
  batch 750 loss: 0.8228244733810425
  batch 800 loss: 0.841182701587677
  batch 850 loss: 0.8780221581459046
  batch 900 loss: 0.8738839447498321
LOSS train 0.87388 valid 0.97047, valid PER 30.46%
EPOCH 13:
  batch 50 loss: 0.805901769399643
  batch 100 loss: 0.8181151688098908
  batch 150 loss: 0.7788180649280548
  batch 200 loss: 0.8177144920825958
  batch 250 loss: 0.8290778291225434
  batch 300 loss: 0.8111223691701889
  batch 350 loss: 0.8068709754943848
  batch 400 loss: 0.8331938004493713
  batch 450 loss: 0.8283568835258484
  batch 500 loss: 0.7971650695800782
  batch 550 loss: 0.8240558385849
  batch 600 loss: 0.8177351343631745
  batch 650 loss: 0.8173567360639572
  batch 700 loss: 0.8098875439167023
  batch 750 loss: 0.7956506419181824
  batch 800 loss: 0.8204884374141693
  batch 850 loss: 0.8443493127822876
  batch 900 loss: 0.83492192029953
LOSS train 0.83492 valid 0.96024, valid PER 30.05%
EPOCH 14:
  batch 50 loss: 0.7775716197490692
  batch 100 loss: 0.7775166606903077
  batch 150 loss: 0.7987020766735077
  batch 200 loss: 0.7944305580854416
  batch 250 loss: 0.7887181925773621
  batch 300 loss: 0.8285498011112213
  batch 350 loss: 0.77920863032341
  batch 400 loss: 0.7796967422962189
  batch 450 loss: 0.7902605199813842
  batch 500 loss: 0.8011008203029633
  batch 550 loss: 0.8153092813491821
  batch 600 loss: 0.7702491819858551
  batch 650 loss: 0.8118189954757691
  batch 700 loss: 0.8227968871593475
  batch 750 loss: 0.7788799977302552
  batch 800 loss: 0.7618735599517822
  batch 850 loss: 0.8266931283473968
  batch 900 loss: 0.8327742648124695
LOSS train 0.83277 valid 0.96911, valid PER 30.38%
EPOCH 15:
  batch 50 loss: 0.7707784390449524
  batch 100 loss: 0.7739213025569915
  batch 150 loss: 0.7767473423480987
  batch 200 loss: 0.8266454350948333
  batch 250 loss: 0.7803315114974976
  batch 300 loss: 0.7899973857402801
  batch 350 loss: 0.7638772058486939
  batch 400 loss: 0.8212672924995422
  batch 450 loss: 0.8062197840213776
  batch 500 loss: 0.7536047172546386
  batch 550 loss: 0.7971095156669616
  batch 600 loss: 0.8285428845882415
  batch 650 loss: 0.8295313513278961
  batch 700 loss: 0.826786241531372
  batch 750 loss: 0.8094588673114776
  batch 800 loss: 0.7994903302192689
  batch 850 loss: 0.8117867684364319
  batch 900 loss: 0.7960457921028137
LOSS train 0.79605 valid 0.97432, valid PER 29.42%
EPOCH 16:
  batch 50 loss: 0.7817856788635253
  batch 100 loss: 0.7514827764034271
  batch 150 loss: 0.7652336603403092
  batch 200 loss: 0.7408375459909439
  batch 250 loss: 0.7615755498409271
  batch 300 loss: 0.7671662330627441
  batch 350 loss: 0.7924239432811737
  batch 400 loss: 0.8027032315731049
  batch 450 loss: 0.8183093047142029
  batch 500 loss: 0.730915242433548
  batch 550 loss: 0.770982735157013
  batch 600 loss: 0.7613907408714294
  batch 650 loss: 0.7816034162044525
  batch 700 loss: 0.7592504000663758
  batch 750 loss: 0.771090704202652
  batch 800 loss: 0.7975129795074463
  batch 850 loss: 0.764796006679535
  batch 900 loss: 0.7579215681552887
LOSS train 0.75792 valid 0.95344, valid PER 28.59%
EPOCH 17:
  batch 50 loss: 0.7467621326446533
  batch 100 loss: 0.7661834388971329
  batch 150 loss: 0.7293819087743759
  batch 200 loss: 0.732088810801506
  batch 250 loss: 0.763843914270401
  batch 300 loss: 0.7472913384437561
  batch 350 loss: 0.735280499458313
  batch 400 loss: 0.7813696694374085
  batch 450 loss: 0.7920704877376556
  batch 500 loss: 0.7655783438682556
  batch 550 loss: 0.8174179691076279
  batch 600 loss: 0.8168321406841278
  batch 650 loss: 0.7450135385990143
  batch 700 loss: 0.7520105648040771
  batch 750 loss: 0.7358395159244537
  batch 800 loss: 0.744226883649826
  batch 850 loss: 0.7678619736433029
  batch 900 loss: 0.7333381992578506
LOSS train 0.73334 valid 0.94592, valid PER 28.77%
EPOCH 18:
  batch 50 loss: 0.708101921081543
  batch 100 loss: 0.7236826205253601
  batch 150 loss: 0.7670597696304321
  batch 200 loss: 0.7473741972446442
  batch 250 loss: 0.747618465423584
  batch 300 loss: 0.7391204941272735
  batch 350 loss: 0.7512257432937622
  batch 400 loss: 0.7361942988634109
  batch 450 loss: 0.7885774910449982
  batch 500 loss: 0.758601405620575
  batch 550 loss: 0.7433549046516419
  batch 600 loss: 0.7457691240310669
  batch 650 loss: 0.7406532466411591
  batch 700 loss: 0.7686374700069427
  batch 750 loss: 0.7464541983604431
  batch 800 loss: 0.7448439681529999
  batch 850 loss: 0.7513892859220505
  batch 900 loss: 0.8286593651771545
LOSS train 0.82866 valid 0.96691, valid PER 29.92%
EPOCH 19:
  batch 50 loss: 0.6858844059705734
  batch 100 loss: 0.6822451829910279
  batch 150 loss: 0.7089720642566681
  batch 200 loss: 0.7109987938404083
  batch 250 loss: 0.7309834533929824
  batch 300 loss: 0.7081746137142182
  batch 350 loss: 0.7202307319641114
  batch 400 loss: 0.7124138331413269
  batch 450 loss: 0.7327523505687714
  batch 500 loss: 0.7281382858753205
  batch 550 loss: 0.7174360847473145
  batch 600 loss: 0.7386837375164031
  batch 650 loss: 0.7968944001197815
  batch 700 loss: 0.7321381914615631
  batch 750 loss: 0.7106263899803161
  batch 800 loss: 0.7398273068666458
  batch 850 loss: 0.7299660992622375
  batch 900 loss: 0.7338264393806457
LOSS train 0.73383 valid 0.96370, valid PER 28.78%
EPOCH 20:
  batch 50 loss: 0.6954474246501923
  batch 100 loss: 0.6775048208236695
  batch 150 loss: 0.6770999813079834
  batch 200 loss: 0.6849195027351379
  batch 250 loss: 0.7055046266317367
  batch 300 loss: 0.73018026471138
  batch 350 loss: 0.6990798062086105
  batch 400 loss: 0.7086918222904205
  batch 450 loss: 0.7266019141674042
  batch 500 loss: 0.7049327939748764
  batch 550 loss: 0.7663361704349518
  batch 600 loss: 0.692295590043068
  batch 650 loss: 0.7343863791227341
  batch 700 loss: 0.740960990190506
  batch 750 loss: 0.7186044245958328
  batch 800 loss: 0.7491276848316193
  batch 850 loss: 0.7543033826351165
  batch 900 loss: 0.7316350519657135
LOSS train 0.73164 valid 0.94719, valid PER 28.62%
train_loss
[1.8731363773345948, 1.4829378199577332, 1.2323620915412903, 1.1981160581111907, 1.0934457099437713, 1.0502864336967468, 1.0709785842895507, 0.9237043929100036, 0.9000137996673584, 0.8993563282489777, 0.9131957948207855, 0.8738839447498321, 0.83492192029953, 0.8327742648124695, 0.7960457921028137, 0.7579215681552887, 0.7333381992578506, 0.8286593651771545, 0.7338264393806457, 0.7316350519657135]
valid_loss
[1.7941447496414185, 1.4491422176361084, 1.2740106582641602, 1.1680498123168945, 1.1177786588668823, 1.1316168308258057, 1.0746159553527832, 1.0257402658462524, 1.071117877960205, 1.0031996965408325, 0.9853118658065796, 0.9704680442810059, 0.9602375626564026, 0.9691059589385986, 0.9743185043334961, 0.9534367918968201, 0.9459232091903687, 0.9669101238250732, 0.9636989235877991, 0.9471920728683472]
valid_per
[68.25089988001601, 46.46713771497134, 39.074790027996265, 35.76189841354486, 35.008665511265164, 36.2951606452473, 33.61551793094254, 31.769097453672845, 32.20903879482736, 31.84242101053193, 30.955872550326625, 30.462604986001868, 30.049326756432475, 30.375949873350223, 29.42274363418211, 28.58952139714705, 28.769497400346623, 29.916011198506865, 28.776163178242903, 28.62285028662845]
Training finished in 5.0 minutes.
Model saved to checkpoints/20231208_090400/model_17
Loading model from checkpoints/20231208_090400/model_17
SUB: 17.45%, DEL: 11.26%, INS: 2.43%, COR: 71.29%, PER: 31.14%
