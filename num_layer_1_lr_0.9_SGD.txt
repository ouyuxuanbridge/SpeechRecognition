Namespace(seed=123, train_json='/rds/user/yo279/hpc-work/MLMI2/fbanks/train_fbank.json', val_json='/rds/user/yo279/hpc-work/MLMI2/fbanks/dev_fbank.json', test_json='/rds/user/yo279/hpc-work/MLMI2/fbanks/test_fbank.json', batch_size=4, num_layers=1, fbank_dims=23, model_dims=128, concat=1, lr=0.9, vocab='vocab_39.txt', report_interval=50, num_epochs=20, dropout_rate=0.5, clip_max_norm=1)
Total number of model parameters is 166952
EPOCH 1:
  batch 50 loss: 5.043356819152832
  batch 100 loss: 6.10394033908844
  batch 150 loss: 3.5344152450561523
  batch 200 loss: 3.424666314125061
  batch 250 loss: 3.400718216896057
  batch 300 loss: 3.34156596660614
  batch 350 loss: 3.269897184371948
  batch 400 loss: 3.230901894569397
  batch 450 loss: 3.171109118461609
  batch 500 loss: 3.0908636903762816
  batch 550 loss: 3.0064232873916628
  batch 600 loss: 2.8530527687072755
  batch 650 loss: 2.754441051483154
  batch 700 loss: 2.739475564956665
  batch 750 loss: 2.5702887535095216
  batch 800 loss: 2.5029872989654542
  batch 850 loss: 2.450147714614868
  batch 900 loss: 2.3148459577560425
LOSS train 2.31485 valid 2.13489, valid PER 62.52%
EPOCH 2:
  batch 50 loss: 2.1658445763587952
  batch 100 loss: 2.0861864352226256
  batch 150 loss: 2.006710374355316
  batch 200 loss: 1.9997286462783814
  batch 250 loss: 1.9651929831504822
  batch 300 loss: 1.8792434573173522
  batch 350 loss: 1.769665491580963
  batch 400 loss: 1.7394238495826722
  batch 450 loss: 1.695006730556488
  batch 500 loss: 1.7054423356056214
  batch 550 loss: 1.7163688802719117
  batch 600 loss: 1.5884262037277221
  batch 650 loss: 1.6552582454681397
  batch 700 loss: 1.6289244103431701
  batch 750 loss: 1.6079648113250733
  batch 800 loss: 1.5164912152290344
  batch 850 loss: 1.4974291324615479
  batch 900 loss: 1.519371907711029
LOSS train 1.51937 valid 1.50312, valid PER 45.10%
EPOCH 3:
  batch 50 loss: 1.4939114451408386
  batch 100 loss: 1.4345629048347472
  batch 150 loss: 1.4311761903762816
  batch 200 loss: 1.4164029550552368
  batch 250 loss: 1.3998474979400635
  batch 300 loss: 1.3710520386695861
  batch 350 loss: 1.4176154375076293
  batch 400 loss: 1.406529767513275
  batch 450 loss: 1.3620664811134338
  batch 500 loss: 1.3430736470222473
  batch 550 loss: 1.3699785304069518
  batch 600 loss: 1.3233285665512085
  batch 650 loss: 1.2804476976394654
  batch 700 loss: 1.3177381241321564
  batch 750 loss: 1.3830943870544434
  batch 800 loss: 1.2943131613731385
  batch 850 loss: 1.331323630809784
  batch 900 loss: 1.2503362154960633
LOSS train 1.25034 valid 1.34096, valid PER 41.27%
EPOCH 4:
  batch 50 loss: 1.2163934516906738
  batch 100 loss: 1.2503934717178344
  batch 150 loss: 1.1950714600086212
  batch 200 loss: 1.226781815290451
  batch 250 loss: 1.23659343957901
  batch 300 loss: 1.2750479471683502
  batch 350 loss: 1.1864984786510469
  batch 400 loss: 1.2168471813201904
  batch 450 loss: 1.2039998149871827
  batch 500 loss: 1.2077411901950836
  batch 550 loss: 1.1960006535053254
  batch 600 loss: 1.2407761299610138
  batch 650 loss: 1.2264047825336457
  batch 700 loss: 1.1737396764755248
  batch 750 loss: 1.1395920705795288
  batch 800 loss: 1.1471607351303101
  batch 850 loss: 1.1709136509895324
  batch 900 loss: 1.2123164880275725
LOSS train 1.21232 valid 1.19738, valid PER 36.46%
EPOCH 5:
  batch 50 loss: 1.109381742477417
  batch 100 loss: 1.117025089263916
  batch 150 loss: 1.166847027540207
  batch 200 loss: 1.0792694914340972
  batch 250 loss: 1.1281739246845246
  batch 300 loss: 1.1187782025337218
  batch 350 loss: 1.1237705361843109
  batch 400 loss: 1.1295669162273407
  batch 450 loss: 1.1085499274730681
  batch 500 loss: 1.1291839063167572
  batch 550 loss: 1.0541820454597473
  batch 600 loss: 1.1429578351974488
  batch 650 loss: 1.0909144830703736
  batch 700 loss: 1.1313830244541168
  batch 750 loss: 1.0790659773349762
  batch 800 loss: 1.1017747378349305
  batch 850 loss: 1.1071709179878235
  batch 900 loss: 1.1063639664649962
LOSS train 1.10636 valid 1.13283, valid PER 34.85%
EPOCH 6:
  batch 50 loss: 1.084536384344101
  batch 100 loss: 1.0280816268920898
  batch 150 loss: 1.0297422850131988
  batch 200 loss: 1.0503971707820892
  batch 250 loss: 1.0753528153896332
  batch 300 loss: 1.0483918333053588
  batch 350 loss: 1.050573674440384
  batch 400 loss: 1.0350699973106385
  batch 450 loss: 1.0840017294883728
  batch 500 loss: 1.0717619490623473
  batch 550 loss: 1.0889930057525634
  batch 600 loss: 1.0272196114063263
  batch 650 loss: 1.0600982868671418
  batch 700 loss: 1.0543428993225097
  batch 750 loss: 1.0397705006599427
  batch 800 loss: 1.0340411925315858
  batch 850 loss: 1.0112107717990875
  batch 900 loss: 1.0928611993789672
LOSS train 1.09286 valid 1.14933, valid PER 34.80%
EPOCH 7:
  batch 50 loss: 1.0177260327339173
  batch 100 loss: 1.017335683107376
  batch 150 loss: 1.010490379333496
  batch 200 loss: 0.9919494819641114
  batch 250 loss: 0.998750103712082
  batch 300 loss: 0.9708698618412018
  batch 350 loss: 1.0142520093917846
  batch 400 loss: 1.0010903930664063
  batch 450 loss: 0.9887842774391175
  batch 500 loss: 0.9969059956073761
  batch 550 loss: 1.0055951642990113
  batch 600 loss: 1.0130158603191375
  batch 650 loss: 1.015176886320114
  batch 700 loss: 1.031482892036438
  batch 750 loss: 0.9983589696884155
  batch 800 loss: 0.9959338617324829
  batch 850 loss: 1.0211229372024535
  batch 900 loss: 1.0846422290802002
LOSS train 1.08464 valid 1.10843, valid PER 33.98%
EPOCH 8:
  batch 50 loss: 0.9669006609916687
  batch 100 loss: 0.9569721126556396
  batch 150 loss: 0.9725667762756348
  batch 200 loss: 0.9334949612617492
  batch 250 loss: 0.9638213241100311
  batch 300 loss: 0.919028137922287
  batch 350 loss: 0.9873600566387176
  batch 400 loss: 0.9583333516120911
  batch 450 loss: 0.9914872789382935
  batch 500 loss: 1.016177304983139
  batch 550 loss: 0.9410008716583252
  batch 600 loss: 0.9783870041370392
  batch 650 loss: 0.9957870411872863
  batch 700 loss: 0.9657622218132019
  batch 750 loss: 0.9609812033176423
  batch 800 loss: 0.9831009364128113
  batch 850 loss: 0.9789307618141174
  batch 900 loss: 0.9750366091728211
LOSS train 0.97504 valid 1.07226, valid PER 33.41%
EPOCH 9:
  batch 50 loss: 0.880324296951294
  batch 100 loss: 0.9354307329654694
  batch 150 loss: 0.9353133118152619
  batch 200 loss: 0.9086639988422394
  batch 250 loss: 0.9416630089282989
  batch 300 loss: 0.9520722258090973
  batch 350 loss: 0.976346994638443
  batch 400 loss: 0.9326998376846314
  batch 450 loss: 0.9392137587070465
  batch 500 loss: 0.9078792905807496
  batch 550 loss: 0.9678725707530975
  batch 600 loss: 0.9712837076187134
  batch 650 loss: 0.9244651591777802
  batch 700 loss: 0.918202315568924
  batch 750 loss: 0.9417109775543213
  batch 800 loss: 0.9593567609786987
  batch 850 loss: 0.960044811964035
  batch 900 loss: 0.9230919528007507
LOSS train 0.92309 valid 1.04216, valid PER 31.50%
EPOCH 10:
  batch 50 loss: 0.8629207146167756
  batch 100 loss: 0.8990854692459106
  batch 150 loss: 0.919125097990036
  batch 200 loss: 0.9512715840339661
  batch 250 loss: 0.9474777674674988
  batch 300 loss: 0.8998904752731324
  batch 350 loss: 0.955548255443573
  batch 400 loss: 0.899669543504715
  batch 450 loss: 0.894845780134201
  batch 500 loss: 0.9325110232830047
  batch 550 loss: 0.9424939346313477
  batch 600 loss: 0.9201713681221009
  batch 650 loss: 0.909539840221405
  batch 700 loss: 0.9448093950748444
  batch 750 loss: 0.8992014598846435
  batch 800 loss: 0.9349884474277497
  batch 850 loss: 0.9459847688674927
  batch 900 loss: 0.9173023915290832
LOSS train 0.91730 valid 1.11692, valid PER 34.66%
EPOCH 11:
  batch 50 loss: 0.8790327739715577
  batch 100 loss: 0.8742428398132325
  batch 150 loss: 0.8871277678012848
  batch 200 loss: 0.914370402097702
  batch 250 loss: 0.8855887579917908
  batch 300 loss: 0.8690683662891387
  batch 350 loss: 0.8759183239936829
  batch 400 loss: 0.9164592301845551
  batch 450 loss: 0.9029461812973022
  batch 500 loss: 0.8816292691230774
  batch 550 loss: 0.8605610084533691
  batch 600 loss: 0.8572125613689423
  batch 650 loss: 0.9237582850456237
  batch 700 loss: 0.8668886911869049
  batch 750 loss: 0.8935257792472839
  batch 800 loss: 0.9243839144706726
  batch 850 loss: 0.9373089015483856
  batch 900 loss: 0.8973223960399628
LOSS train 0.89732 valid 1.01897, valid PER 32.03%
EPOCH 12:
  batch 50 loss: 0.848521054983139
  batch 100 loss: 0.8217381250858307
  batch 150 loss: 0.8119089108705521
  batch 200 loss: 0.8390339463949203
  batch 250 loss: 0.9186703658103943
  batch 300 loss: 0.8909939777851105
  batch 350 loss: 0.8801905131340027
  batch 400 loss: 0.8879072725772857
  batch 450 loss: 0.916783664226532
  batch 500 loss: 0.9273119628429413
  batch 550 loss: 0.8500043761730194
  batch 600 loss: 0.8617786121368408
  batch 650 loss: 0.9045269560813903
  batch 700 loss: 0.894079065322876
  batch 750 loss: 0.8465827441215515
  batch 800 loss: 0.8692655301094055
  batch 850 loss: 0.9014506804943084
  batch 900 loss: 0.938509087562561
LOSS train 0.93851 valid 0.98424, valid PER 30.82%
EPOCH 13:
  batch 50 loss: 0.8072769284248352
  batch 100 loss: 0.8349413204193116
  batch 150 loss: 0.8376169139146805
  batch 200 loss: 0.9011665546894073
  batch 250 loss: 0.8687277221679688
  batch 300 loss: 0.8763958406448364
  batch 350 loss: 0.8695873665809631
  batch 400 loss: 0.8603358936309814
  batch 450 loss: 0.8615560889244079
  batch 500 loss: 0.8362271648645401
  batch 550 loss: 0.8505176770687103
  batch 600 loss: 0.8991101491451263
  batch 650 loss: 0.908608283996582
  batch 700 loss: 0.8928581404685975
  batch 750 loss: 0.8214338088035583
  batch 800 loss: 0.863827098608017
  batch 850 loss: 0.8918194627761841
  batch 900 loss: 0.8863657140731811
LOSS train 0.88637 valid 1.02932, valid PER 31.17%
EPOCH 14:
  batch 50 loss: 0.8577796041965484
  batch 100 loss: 0.8364543056488037
  batch 150 loss: 0.8464536368846893
  batch 200 loss: 0.8685678100585937
  batch 250 loss: 0.8481782436370849
  batch 300 loss: 0.8618290030956268
  batch 350 loss: 0.8179485130310059
  batch 400 loss: 0.8191130566596985
  batch 450 loss: 0.8285985195636749
  batch 500 loss: 0.8974177074432373
  batch 550 loss: 0.8914729261398315
  batch 600 loss: 0.8371883749961853
  batch 650 loss: 0.8829771852493287
  batch 700 loss: 0.9176142477989196
  batch 750 loss: 0.8820351815223694
  batch 800 loss: 0.9353363800048828
  batch 850 loss: 0.9408066964149475
  batch 900 loss: 0.9197363972663879
LOSS train 0.91974 valid 1.11688, valid PER 34.15%
EPOCH 15:
  batch 50 loss: 0.8815104496479035
  batch 100 loss: 0.8607320123910904
  batch 150 loss: 0.8549719250202179
  batch 200 loss: 0.8852032685279846
  batch 250 loss: 0.8926371884346008
  batch 300 loss: 0.8705785834789276
  batch 350 loss: 0.9113630712032318
  batch 400 loss: 0.8743101978302001
  batch 450 loss: 0.8768420207500458
  batch 500 loss: 0.8438533353805542
  batch 550 loss: 0.8783306419849396
  batch 600 loss: 0.874081814289093
  batch 650 loss: 0.865292546749115
  batch 700 loss: 0.8849647605419159
  batch 750 loss: 0.9280756151676178
  batch 800 loss: 0.9539948880672455
  batch 850 loss: 0.8873598718643189
  batch 900 loss: 0.9362041020393371
LOSS train 0.93620 valid 1.06631, valid PER 32.00%
EPOCH 16:
  batch 50 loss: 0.891500586271286
  batch 100 loss: 0.8550365018844605
  batch 150 loss: 0.8425098168849945
  batch 200 loss: 0.8483182847499847
  batch 250 loss: 0.9612261760234833
  batch 300 loss: 0.9049967670440674
  batch 350 loss: 0.9114234137535095
  batch 400 loss: 0.9068377327919006
  batch 450 loss: 0.8980228126049041
  batch 500 loss: 0.8416335451602935
  batch 550 loss: 0.8788177299499512
  batch 600 loss: 0.8586373221874237
  batch 650 loss: 0.879174599647522
  batch 700 loss: 0.8894774901866913
  batch 750 loss: 0.8672972452640534
  batch 800 loss: 0.8615698331594467
  batch 850 loss: 0.8272857224941254
  batch 900 loss: 0.845421952009201
LOSS train 0.84542 valid 1.01225, valid PER 30.96%
EPOCH 17:
  batch 50 loss: 0.8069279593229294
  batch 100 loss: 0.805558055639267
  batch 150 loss: 0.8085670018196106
  batch 200 loss: 0.8186106276512146
  batch 250 loss: 0.8235155999660492
  batch 300 loss: 0.8533297085762024
  batch 350 loss: 0.8070455765724183
  batch 400 loss: 0.8586936867237092
  batch 450 loss: 0.9564877891540527
  batch 500 loss: 0.9930156183242798
  batch 550 loss: 0.9285058188438415
  batch 600 loss: 0.9568795347213745
  batch 650 loss: 0.8977528274059295
  batch 700 loss: 0.8635432147979736
  batch 750 loss: 0.8495354247093201
  batch 800 loss: 0.9019871151447296
  batch 850 loss: 0.8823989963531494
  batch 900 loss: 0.8601545524597168
LOSS train 0.86015 valid 1.05562, valid PER 31.68%
EPOCH 18:
  batch 50 loss: 0.8310185766220093
  batch 100 loss: 0.8358441936969757
  batch 150 loss: 0.8628104305267335
  batch 200 loss: 0.8376890838146209
  batch 250 loss: 0.8316231119632721
  batch 300 loss: 0.801144163608551
  batch 350 loss: 0.8386289119720459
  batch 400 loss: 0.7984420788288117
  batch 450 loss: 0.8816187810897828
  batch 500 loss: 0.8455132627487183
  batch 550 loss: 0.8233845460414887
  batch 600 loss: 0.8045813000202179
  batch 650 loss: 0.8325938737392425
  batch 700 loss: 0.8522332322597503
  batch 750 loss: 0.8047280216217041
  batch 800 loss: 0.8191138589382172
  batch 850 loss: 0.8253577816486358
  batch 900 loss: 0.866806173324585
LOSS train 0.86681 valid 1.03466, valid PER 31.72%
EPOCH 19:
  batch 50 loss: 0.7792551058530808
  batch 100 loss: 0.7616724324226379
  batch 150 loss: 0.7924771022796631
  batch 200 loss: 0.7914017212390899
  batch 250 loss: 0.8382813316583634
  batch 300 loss: 0.8623887401819229
  batch 350 loss: 0.8604090988636017
  batch 400 loss: 0.9568441045284272
  batch 450 loss: 0.8792158675193786
  batch 500 loss: 0.8731602513790131
  batch 550 loss: 0.8325442922115326
  batch 600 loss: 0.8106531512737274
  batch 650 loss: 0.9076838231086731
  batch 700 loss: 0.8211020612716675
  batch 750 loss: 0.7925454342365265
  batch 800 loss: 0.8694110405445099
  batch 850 loss: 0.8359425818920135
  batch 900 loss: 0.8619858527183533
LOSS train 0.86199 valid 1.05603, valid PER 32.07%
EPOCH 20:
  batch 50 loss: 0.812549569606781
  batch 100 loss: 0.8295359277725219
  batch 150 loss: 0.7816407954692841
  batch 200 loss: 0.7931387042999267
  batch 250 loss: 0.8540255463123322
  batch 300 loss: 0.8693266260623932
  batch 350 loss: 0.7959008914232254
  batch 400 loss: 0.821995884180069
  batch 450 loss: 0.7988110876083374
  batch 500 loss: 0.813230299949646
  batch 550 loss: 0.878619384765625
  batch 600 loss: 0.7852198338508606
  batch 650 loss: 0.8314949440956115
  batch 700 loss: 0.8393546569347382
  batch 750 loss: 0.8284826934337616
  batch 800 loss: 0.9085333478450776
  batch 850 loss: 0.8733025074005127
  batch 900 loss: 0.8462097573280335
LOSS train 0.84621 valid 1.03315, valid PER 31.16%
train_loss
[2.3148459577560425, 1.519371907711029, 1.2503362154960633, 1.2123164880275725, 1.1063639664649962, 1.0928611993789672, 1.0846422290802002, 0.9750366091728211, 0.9230919528007507, 0.9173023915290832, 0.8973223960399628, 0.938509087562561, 0.8863657140731811, 0.9197363972663879, 0.9362041020393371, 0.845421952009201, 0.8601545524597168, 0.866806173324585, 0.8619858527183533, 0.8462097573280335]
valid_loss
[2.1348936557769775, 1.5031170845031738, 1.3409574031829834, 1.197377324104309, 1.1328282356262207, 1.149334192276001, 1.1084269285202026, 1.0722638368606567, 1.04216468334198, 1.1169227361679077, 1.0189743041992188, 0.9842448234558105, 1.0293240547180176, 1.1168805360794067, 1.0663089752197266, 1.0122519731521606, 1.055617094039917, 1.0346567630767822, 1.0560294389724731, 1.0331469774246216]
valid_per
[62.524996667111054, 45.10065324623383, 41.27449673376883, 36.455139314758036, 34.848686841754436, 34.79536061858418, 33.982135715237966, 33.408878816157845, 31.502466337821623, 34.66204506065858, 32.02906279162778, 30.81589121450473, 31.1691774430076, 34.14878016264498, 32.00239968004266, 30.962538328222905, 31.67577656312492, 31.715771230502597, 32.06905745900547, 31.15584588721504]
Training finished in 3.0 minutes.
Model saved to checkpoints/20231208_091553/model_12
Loading model from checkpoints/20231208_091553/model_12
SUB: 18.02%, DEL: 11.90%, INS: 2.29%, COR: 70.08%, PER: 32.21%
