Namespace(seed=123, train_json='/rds/user/yo279/hpc-work/MLMI2/fbanks/train_fbank.json', val_json='/rds/user/yo279/hpc-work/MLMI2/fbanks/dev_fbank.json', test_json='/rds/user/yo279/hpc-work/MLMI2/fbanks/test_fbank.json', batch_size=4, num_layers=1, fbank_dims=23, model_dims=128, concat=1, lr=1.0, vocab='vocab_39.txt', report_interval=50, num_epochs=20, dropout_rate=0.5, clip_max_norm=1)
Total number of model parameters is 166952
EPOCH 1:
  batch 50 loss: 4.5858080959320064
  batch 100 loss: 3.328640856742859
  batch 150 loss: 3.131796860694885
  batch 200 loss: 3.065879578590393
  batch 250 loss: 3.0273595666885376
  batch 300 loss: 2.7573150777816773
  batch 350 loss: 2.5399242496490477
  batch 400 loss: 2.4101261067390443
  batch 450 loss: 2.2939324879646303
  batch 500 loss: 2.1221396708488465
  batch 550 loss: 2.038216645717621
  batch 600 loss: 2.058494625091553
  batch 650 loss: 1.8478786754608154
  batch 700 loss: 1.8000936937332153
  batch 750 loss: 1.7233107352256776
  batch 800 loss: 1.7140431833267211
  batch 850 loss: 1.6779140305519105
  batch 900 loss: 1.6035831761360169
LOSS train 1.60358 valid 1.61444, valid PER 50.83%
EPOCH 2:
  batch 50 loss: 1.5486936616897582
  batch 100 loss: 1.4911112141609193
  batch 150 loss: 1.4889985704421997
  batch 200 loss: 1.5023298096656799
  batch 250 loss: 1.523310146331787
  batch 300 loss: 1.4613548851013183
  batch 350 loss: 1.3853329563140868
  batch 400 loss: 1.396638071537018
  batch 450 loss: 1.3221102476119995
  batch 500 loss: 1.3965197563171388
  batch 550 loss: 1.3784530925750733
  batch 600 loss: 1.3368856954574584
  batch 650 loss: 1.3786463832855225
  batch 700 loss: 1.3757473731040955
  batch 750 loss: 1.331855478286743
  batch 800 loss: 1.2597903800010681
  batch 850 loss: 1.2825585675239564
  batch 900 loss: 1.3090831685066222
LOSS train 1.30908 valid 1.32652, valid PER 39.75%
EPOCH 3:
  batch 50 loss: 1.2526789093017578
  batch 100 loss: 1.265047265291214
  batch 150 loss: 1.2268118000030517
  batch 200 loss: 1.2250976157188416
  batch 250 loss: 1.1852426040172577
  batch 300 loss: 1.2049528551101685
  batch 350 loss: 1.2908300757408142
  batch 400 loss: 1.2491422975063324
  batch 450 loss: 1.231518384218216
  batch 500 loss: 1.2009342646598815
  batch 550 loss: 1.2498781287670135
  batch 600 loss: 1.2485718858242034
  batch 650 loss: 1.184286140203476
  batch 700 loss: 1.2137524926662444
  batch 750 loss: 1.2669528377056123
  batch 800 loss: 1.1801831388473512
  batch 850 loss: 1.2375531017780304
  batch 900 loss: 1.1612199413776398
LOSS train 1.16122 valid 1.22272, valid PER 36.66%
EPOCH 4:
  batch 50 loss: 1.1458202600479126
  batch 100 loss: 1.181331466436386
  batch 150 loss: 1.1720982897281647
  batch 200 loss: 1.1519847548007964
  batch 250 loss: 1.1981069481372832
  batch 300 loss: 1.1872082543373108
  batch 350 loss: 1.1297519671916962
  batch 400 loss: 1.164774100780487
  batch 450 loss: 1.1328408420085907
  batch 500 loss: 1.1392782199382783
  batch 550 loss: 1.2489178705215453
  batch 600 loss: 1.211621344089508
  batch 650 loss: 1.2119471001625062
  batch 700 loss: 1.183125275373459
  batch 750 loss: 1.1078778612613678
  batch 800 loss: 1.066476492881775
  batch 850 loss: 1.113909707069397
  batch 900 loss: 1.1621313679218293
LOSS train 1.16213 valid 1.12190, valid PER 34.28%
EPOCH 5:
  batch 50 loss: 1.0924966049194336
  batch 100 loss: 1.0701624989509582
  batch 150 loss: 1.1365088486671449
  batch 200 loss: 1.0599211478233337
  batch 250 loss: 1.0896467542648316
  batch 300 loss: 1.0948542332649231
  batch 350 loss: 1.1288614499568939
  batch 400 loss: 1.0942963230609895
  batch 450 loss: 1.0714032340049744
  batch 500 loss: 1.1144577872753143
  batch 550 loss: 1.0515643501281737
  batch 600 loss: 1.1465946209430695
  batch 650 loss: 1.0836145079135895
  batch 700 loss: 1.1251465725898742
  batch 750 loss: 1.073328037261963
  batch 800 loss: 1.1390235710144043
  batch 850 loss: 1.1676901435852052
  batch 900 loss: 1.1206407690048217
LOSS train 1.12064 valid 1.13070, valid PER 33.82%
EPOCH 6:
  batch 50 loss: 1.0907880651950836
  batch 100 loss: 1.046064976453781
  batch 150 loss: 1.0606985795497894
  batch 200 loss: 1.0914439833164216
  batch 250 loss: 1.0994339382648468
  batch 300 loss: 1.0515448176860809
  batch 350 loss: 1.052256054878235
  batch 400 loss: 1.0657499408721924
  batch 450 loss: 1.0894074988365174
  batch 500 loss: 1.0710576164722443
  batch 550 loss: 1.089984667301178
  batch 600 loss: 1.0446912598609925
  batch 650 loss: 1.092086945772171
  batch 700 loss: 1.1064690244197846
  batch 750 loss: 1.2492101311683654
  batch 800 loss: 1.1598722922801972
  batch 850 loss: 1.1358372581005096
  batch 900 loss: 1.0988469922542572
LOSS train 1.09885 valid 1.16466, valid PER 34.82%
EPOCH 7:
  batch 50 loss: 1.0569509959220886
  batch 100 loss: 1.0932904982566833
  batch 150 loss: 1.028121154308319
  batch 200 loss: 1.040655312538147
  batch 250 loss: 1.0270448517799378
  batch 300 loss: 1.0135091948509216
  batch 350 loss: 1.0215854942798615
  batch 400 loss: 1.0251271116733551
  batch 450 loss: 1.0171157109737397
  batch 500 loss: 1.0164930844306945
  batch 550 loss: 1.0103352987766265
  batch 600 loss: 1.2745838963985443
  batch 650 loss: 1.2079703390598298
  batch 700 loss: 1.1533846437931061
  batch 750 loss: 1.1499990737438202
  batch 800 loss: 1.1264877831935882
  batch 850 loss: 1.1359353542327881
  batch 900 loss: 1.127030324935913
LOSS train 1.12703 valid 1.16317, valid PER 35.45%
EPOCH 8:
  batch 50 loss: 1.0575127005577087
  batch 100 loss: 1.0574868404865265
  batch 150 loss: 1.0883850502967833
  batch 200 loss: 1.048104591369629
  batch 250 loss: 1.0715286421775818
  batch 300 loss: 1.0643816351890565
  batch 350 loss: 1.1081664276123047
  batch 400 loss: 1.0787811136245729
  batch 450 loss: 1.0851295065879822
  batch 500 loss: 1.0955904972553254
  batch 550 loss: 1.0177265989780426
  batch 600 loss: 1.0741635060310364
  batch 650 loss: 1.1224359464645386
  batch 700 loss: 1.0560725331306458
  batch 750 loss: 1.035947642326355
  batch 800 loss: 1.0485287261009217
  batch 850 loss: 1.155172039270401
  batch 900 loss: 1.1271152710914611
LOSS train 1.12712 valid 1.18446, valid PER 35.92%
EPOCH 9:
  batch 50 loss: 1.0193999183177949
  batch 100 loss: 1.0643946051597595
  batch 150 loss: 1.0639856803417205
  batch 200 loss: 1.0146995878219605
  batch 250 loss: 1.0680898070335387
  batch 300 loss: 1.0745843899250032
  batch 350 loss: 1.0634955811500548
  batch 400 loss: 1.0878616392612457
  batch 450 loss: 1.081365909576416
  batch 500 loss: 0.9989247310161591
  batch 550 loss: 1.0843095481395721
  batch 600 loss: 1.0841099727153778
  batch 650 loss: 1.0418093335628509
  batch 700 loss: 1.02581822514534
  batch 750 loss: 1.0966875445842743
  batch 800 loss: 1.0774252796173096
  batch 850 loss: 1.0773409271240235
  batch 900 loss: 1.0252902066707612
LOSS train 1.02529 valid 1.12251, valid PER 34.11%
EPOCH 10:
  batch 50 loss: 0.9699659323692322
  batch 100 loss: 0.9801577174663544
  batch 150 loss: 1.077826155424118
  batch 200 loss: 1.0441202008724213
  batch 250 loss: 1.0268412482738496
  batch 300 loss: 0.9668010807037354
  batch 350 loss: 1.0222324299812318
  batch 400 loss: 0.9698993456363678
  batch 450 loss: 0.997844797372818
  batch 500 loss: 1.050442280769348
  batch 550 loss: 1.0723797166347504
  batch 600 loss: 1.1162403631210327
  batch 650 loss: 1.1175589835643769
  batch 700 loss: 1.1210057473182677
  batch 750 loss: 1.083052203655243
  batch 800 loss: 1.1165976023674011
  batch 850 loss: 1.0855228412151336
  batch 900 loss: 1.0730700623989105
LOSS train 1.07307 valid 1.17401, valid PER 36.49%
EPOCH 11:
  batch 50 loss: 0.9829917883872986
  batch 100 loss: 1.0229598128795623
  batch 150 loss: 1.0059709286689758
  batch 200 loss: 1.0344679474830627
  batch 250 loss: 1.0405065059661864
  batch 300 loss: 0.9925995218753815
  batch 350 loss: 1.097486013174057
  batch 400 loss: 1.0382055282592773
  batch 450 loss: 1.0243861985206604
  batch 500 loss: 1.293633199930191
  batch 550 loss: 1.1227413666248323
  batch 600 loss: 1.0771410810947417
  batch 650 loss: 1.1144738376140595
  batch 700 loss: 1.0464279901981355
  batch 750 loss: 1.0406651937961577
  batch 800 loss: 1.0639617085456847
  batch 850 loss: 1.1208375430107116
  batch 900 loss: 1.0637895226478578
LOSS train 1.06379 valid 1.12717, valid PER 34.15%
EPOCH 12:
  batch 50 loss: 1.0292230641841889
  batch 100 loss: 1.0256605887413024
  batch 150 loss: 1.0311958980560303
  batch 200 loss: 1.0087388062477112
  batch 250 loss: 1.047304207086563
  batch 300 loss: 1.0255121994018555
  batch 350 loss: 0.9815731465816497
  batch 400 loss: 1.0309901809692383
  batch 450 loss: 1.0448559617996216
  batch 500 loss: 1.0499290525913239
  batch 550 loss: 0.995439944267273
  batch 600 loss: 1.0165982246398926
  batch 650 loss: 1.0416301953792573
  batch 700 loss: 1.003842889070511
  batch 750 loss: 1.0404319047927857
  batch 800 loss: 1.0006953394412994
  batch 850 loss: 1.0556872284412384
  batch 900 loss: 1.0489417803287506
LOSS train 1.04894 valid 1.12547, valid PER 34.06%
EPOCH 13:
  batch 50 loss: 1.021850494146347
  batch 100 loss: 1.0271117556095124
  batch 150 loss: 0.9863543927669525
  batch 200 loss: 0.9769599521160126
  batch 250 loss: 0.985939290523529
  batch 300 loss: 0.9650262856483459
  batch 350 loss: 1.0495793306827546
  batch 400 loss: 1.0521872127056122
  batch 450 loss: 1.0146435356140138
  batch 500 loss: 0.9760332369804382
  batch 550 loss: 0.9837860238552093
  batch 600 loss: 0.9934870326519012
  batch 650 loss: 1.048549817800522
  batch 700 loss: 1.1558487045764922
  batch 750 loss: 1.0521673130989075
  batch 800 loss: 1.106752016544342
  batch 850 loss: 1.107519167661667
  batch 900 loss: 1.0690351438522339
LOSS train 1.06904 valid 1.15804, valid PER 34.52%
EPOCH 14:
  batch 50 loss: 1.043310353755951
  batch 100 loss: 1.0261814105510711
  batch 150 loss: 1.0575192272663116
  batch 200 loss: 1.1034523284435271
  batch 250 loss: 1.0425111532211304
  batch 300 loss: 1.0694013965129852
  batch 350 loss: 1.057439422607422
  batch 400 loss: 1.0702759194374085
  batch 450 loss: 1.1044624853134155
  batch 500 loss: 1.0949138760566712
  batch 550 loss: 1.074144380092621
  batch 600 loss: 1.0311545240879059
  batch 650 loss: 1.160365833044052
  batch 700 loss: 1.1314574456214905
  batch 750 loss: 1.0799022710323334
  batch 800 loss: 1.0684235978126526
  batch 850 loss: 1.1050642836093902
  batch 900 loss: 1.0910448932647705
LOSS train 1.09104 valid 1.16422, valid PER 35.47%
EPOCH 15:
  batch 50 loss: 1.0206006646156311
  batch 100 loss: 1.0003230357170105
  batch 150 loss: 1.001645919084549
  batch 200 loss: 1.0560582768917084
  batch 250 loss: 1.02666867852211
  batch 300 loss: 1.0756291437149048
  batch 350 loss: 1.0716158306598664
  batch 400 loss: 1.0529887068271637
  batch 450 loss: 1.072691972255707
  batch 500 loss: 1.0436229288578034
  batch 550 loss: 1.062396421432495
  batch 600 loss: 1.0457446098327636
  batch 650 loss: 1.0649001610279083
  batch 700 loss: 1.034465321302414
  batch 750 loss: 1.013650871515274
  batch 800 loss: 1.032504630088806
  batch 850 loss: 1.0149751496315003
  batch 900 loss: 1.0194671630859375
LOSS train 1.01947 valid 1.16149, valid PER 35.38%
EPOCH 16:
  batch 50 loss: 0.9988608705997467
  batch 100 loss: 0.9428729474544525
  batch 150 loss: 0.93551096200943
  batch 200 loss: 0.9481698882579803
  batch 250 loss: 0.9712534701824188
  batch 300 loss: 0.9713675177097321
  batch 350 loss: 1.0691861283779145
  batch 400 loss: 1.066293067932129
  batch 450 loss: 1.0416572141647338
  batch 500 loss: 0.977699464559555
  batch 550 loss: 1.0149045419692992
  batch 600 loss: 1.0137461984157563
  batch 650 loss: 1.0484256029129029
  batch 700 loss: 1.0097504222393037
  batch 750 loss: 1.0007770383358001
  batch 800 loss: 1.0501289248466492
  batch 850 loss: 1.0573578810691833
  batch 900 loss: 1.0309138977527619
LOSS train 1.03091 valid 1.17072, valid PER 34.46%
EPOCH 17:
  batch 50 loss: 0.9953342914581299
  batch 100 loss: 0.994271092414856
  batch 150 loss: 0.9893379056453705
  batch 200 loss: 0.9604027187824249
  batch 250 loss: 0.9815710699558258
  batch 300 loss: 0.9833063161373139
  batch 350 loss: 0.9392611610889435
  batch 400 loss: 1.015388240814209
  batch 450 loss: 1.0247813439369202
  batch 500 loss: 0.9912272274494172
  batch 550 loss: 0.9897604262828827
  batch 600 loss: 0.9939432394504547
  batch 650 loss: 0.9645678520202636
  batch 700 loss: 0.9693086576461792
  batch 750 loss: 0.9408343315124512
  batch 800 loss: 0.9789781022071838
  batch 850 loss: 0.965176066160202
  batch 900 loss: 0.9291019678115845
LOSS train 0.92910 valid 1.17243, valid PER 34.54%
EPOCH 18:
  batch 50 loss: 0.9956018626689911
  batch 100 loss: 1.0570759570598602
  batch 150 loss: 1.0683861994743347
  batch 200 loss: 1.0637256109714508
  batch 250 loss: 1.0258393740653993
  batch 300 loss: 0.9619004797935485
  batch 350 loss: 0.9891635572910309
  batch 400 loss: 0.94939915060997
  batch 450 loss: 1.027890807390213
  batch 500 loss: 1.00514417886734
  batch 550 loss: 0.9950496804714203
  batch 600 loss: 0.9880253052711487
  batch 650 loss: 0.9822452199459076
  batch 700 loss: 1.0096700286865234
  batch 750 loss: 0.983992931842804
  batch 800 loss: 1.0051832163333894
  batch 850 loss: 1.0041021120548248
  batch 900 loss: 1.007429838180542
LOSS train 1.00743 valid 1.13726, valid PER 34.48%
EPOCH 19:
  batch 50 loss: 0.883071494102478
  batch 100 loss: 0.9107006692886352
  batch 150 loss: 0.9500781679153443
  batch 200 loss: 0.9523355972766876
  batch 250 loss: 1.0822239100933075
  batch 300 loss: 1.087217197418213
  batch 350 loss: 1.0413106083869934
  batch 400 loss: 1.0403497552871703
  batch 450 loss: 1.064339644908905
  batch 500 loss: 1.0138126587867737
  batch 550 loss: 0.976222892999649
  batch 600 loss: 0.983182611465454
  batch 650 loss: 1.0920537400245667
  batch 700 loss: 1.0274286317825316
  batch 750 loss: 0.9993925225734711
  batch 800 loss: 1.0499465954303742
  batch 850 loss: 1.0356656527519226
  batch 900 loss: 1.0049409985542297
LOSS train 1.00494 valid 1.14791, valid PER 35.58%
EPOCH 20:
  batch 50 loss: 0.9826123309135437
  batch 100 loss: 0.9591848647594452
  batch 150 loss: 0.9625942027568817
  batch 200 loss: 1.051138608455658
  batch 250 loss: 1.0707701361179351
  batch 300 loss: 1.0870550346374512
  batch 350 loss: 1.0514866960048677
  batch 400 loss: 1.0559162819385528
  batch 450 loss: 1.0481366217136383
  batch 500 loss: 1.0191088914871216
  batch 550 loss: 1.043526201248169
  batch 600 loss: 1.0023540568351745
  batch 650 loss: 1.0314732933044433
  batch 700 loss: 1.0880320274829864
  batch 750 loss: 1.03402911901474
  batch 800 loss: 1.0456440258026123
  batch 850 loss: 1.0918616878986358
  batch 900 loss: 1.0340107214450835
LOSS train 1.03401 valid 1.22171, valid PER 36.28%
train_loss
[1.6035831761360169, 1.3090831685066222, 1.1612199413776398, 1.1621313679218293, 1.1206407690048217, 1.0988469922542572, 1.127030324935913, 1.1271152710914611, 1.0252902066707612, 1.0730700623989105, 1.0637895226478578, 1.0489417803287506, 1.0690351438522339, 1.0910448932647705, 1.0194671630859375, 1.0309138977527619, 0.9291019678115845, 1.007429838180542, 1.0049409985542297, 1.0340107214450835]
valid_loss
[1.614442229270935, 1.326524019241333, 1.2227249145507812, 1.1218998432159424, 1.1307048797607422, 1.1646599769592285, 1.163170576095581, 1.1844635009765625, 1.1225125789642334, 1.1740102767944336, 1.1271716356277466, 1.125473141670227, 1.1580355167388916, 1.1642247438430786, 1.1614927053451538, 1.1707162857055664, 1.1724269390106201, 1.137261152267456, 1.147910714149475, 1.2217060327529907]
valid_per
[50.83322223703506, 39.75469937341688, 36.66177842954273, 34.275429942674315, 33.82215704572724, 34.82202373016931, 35.44860685241968, 35.92187708305559, 34.1087854952673, 36.48846820423943, 34.14878016264498, 34.05545927209705, 34.51539794694041, 35.46860418610852, 35.37528329556059, 34.455405945873885, 34.54206105852553, 34.47540327956273, 35.57525663244901, 36.28182908945474]
Training finished in 3.0 minutes.
Model saved to checkpoints/20231208_020504/model_4
Loading model from checkpoints/20231208_020504/model_4
SUB: 19.23%, DEL: 14.63%, INS: 1.95%, COR: 66.15%, PER: 35.80%
