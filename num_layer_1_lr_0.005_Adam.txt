Namespace(seed=123, train_json='/rds/user/yo279/hpc-work/MLMI2/fbanks/train_fbank.json', val_json='/rds/user/yo279/hpc-work/MLMI2/fbanks/dev_fbank.json', test_json='/rds/user/yo279/hpc-work/MLMI2/fbanks/test_fbank.json', batch_size=4, num_layers=1, fbank_dims=23, model_dims=128, concat=1, lr=0.005, vocab='vocab_39.txt', report_interval=50, num_epochs=20, dropout_rate=0.5, clip_max_norm=1)
Total number of model parameters is 166952
EPOCH 1:
  batch 50 loss: 5.418990225791931
  batch 100 loss: 3.4232932043075563
  batch 150 loss: 3.4108921670913697
  batch 200 loss: 3.2987319135665896
  batch 250 loss: 3.2650264930725097
  batch 300 loss: 2.926224536895752
  batch 350 loss: 2.6317522621154783
  batch 400 loss: 2.4320468425750734
  batch 450 loss: 2.274383568763733
  batch 500 loss: 2.1669830513000488
  batch 550 loss: 2.052413067817688
  batch 600 loss: 1.9736960792541505
  batch 650 loss: 1.944024188518524
  batch 700 loss: 1.8969932222366332
  batch 750 loss: 1.8222946763038634
  batch 800 loss: 1.820368070602417
  batch 850 loss: 1.7697928071022033
  batch 900 loss: 1.723317472934723
LOSS train 1.72332 valid 1.69616, valid PER 53.71%
EPOCH 2:
  batch 50 loss: 1.6678711009025573
  batch 100 loss: 1.6342625331878662
  batch 150 loss: 1.6157947611808776
  batch 200 loss: 1.6217963814735412
  batch 250 loss: 1.6124852180480957
  batch 300 loss: 1.563149666786194
  batch 350 loss: 1.5203920364379884
  batch 400 loss: 1.5408266830444335
  batch 450 loss: 1.460184898376465
  batch 500 loss: 1.5298745226860047
  batch 550 loss: 1.5174395537376404
  batch 600 loss: 1.4360227370262146
  batch 650 loss: 1.511977527141571
  batch 700 loss: 1.457599663734436
  batch 750 loss: 1.4638201379776001
  batch 800 loss: 1.426256880760193
  batch 850 loss: 1.459753656387329
  batch 900 loss: 1.4282252955436707
LOSS train 1.42823 valid 1.44094, valid PER 44.81%
EPOCH 3:
  batch 50 loss: 1.4273350954055786
  batch 100 loss: 1.5006540894508362
  batch 150 loss: 1.4514059734344482
  batch 200 loss: 1.4080933833122253
  batch 250 loss: 1.379460563659668
  batch 300 loss: 1.332234230041504
  batch 350 loss: 1.394844546318054
  batch 400 loss: 1.3650756573677063
  batch 450 loss: 1.3394435119628907
  batch 500 loss: 1.3446769976615907
  batch 550 loss: 1.3151617002487184
  batch 600 loss: 1.2975115299224853
  batch 650 loss: 1.247456294298172
  batch 700 loss: 1.279681978225708
  batch 750 loss: 1.3260854291915893
  batch 800 loss: 1.2725342667102815
  batch 850 loss: 1.2990896582603455
  batch 900 loss: 1.2575288605690003
LOSS train 1.25753 valid 1.32480, valid PER 40.86%
EPOCH 4:
  batch 50 loss: 1.231957278251648
  batch 100 loss: 1.2626720309257506
  batch 150 loss: 1.2480033767223357
  batch 200 loss: 1.2716071224212646
  batch 250 loss: 1.2587364149093627
  batch 300 loss: 1.2499211406707764
  batch 350 loss: 1.172487337589264
  batch 400 loss: 1.2683952736854553
  batch 450 loss: 1.222693395614624
  batch 500 loss: 1.2483353102207184
  batch 550 loss: 1.2394656646251678
  batch 600 loss: 1.244449645280838
  batch 650 loss: 1.2535036861896516
  batch 700 loss: 1.2096415841579438
  batch 750 loss: 1.1850579488277435
  batch 800 loss: 1.1691750478744507
  batch 850 loss: 1.1960565996170045
  batch 900 loss: 1.251344027519226
LOSS train 1.25134 valid 1.24527, valid PER 37.87%
EPOCH 5:
  batch 50 loss: 1.1737505841255187
  batch 100 loss: 1.1943298590183258
  batch 150 loss: 1.2170382642745972
  batch 200 loss: 1.132658509016037
  batch 250 loss: 1.1267813754081726
  batch 300 loss: 1.157708945274353
  batch 350 loss: 1.1660653603076936
  batch 400 loss: 1.174107961654663
  batch 450 loss: 1.1710734272003174
  batch 500 loss: 1.1918580877780913
  batch 550 loss: 1.10072713971138
  batch 600 loss: 1.164725559949875
  batch 650 loss: 1.1629138278961182
  batch 700 loss: 1.153449593782425
  batch 750 loss: 1.1151047384738921
  batch 800 loss: 1.142341858148575
  batch 850 loss: 1.139086856842041
  batch 900 loss: 1.1528104877471923
LOSS train 1.15281 valid 1.17119, valid PER 35.79%
EPOCH 6:
  batch 50 loss: 1.1509034848213195
  batch 100 loss: 1.0478940427303314
  batch 150 loss: 1.071972769498825
  batch 200 loss: 1.0959737503528595
  batch 250 loss: 1.1278214359283447
  batch 300 loss: 1.1152260100841522
  batch 350 loss: 1.0952339732646943
  batch 400 loss: 1.0701763033866882
  batch 450 loss: 1.0959004724025727
  batch 500 loss: 1.0767590022087097
  batch 550 loss: 1.0956256186962128
  batch 600 loss: 1.0623680460453033
  batch 650 loss: 1.097981903553009
  batch 700 loss: 1.112630032300949
  batch 750 loss: 1.0782564961910248
  batch 800 loss: 1.1041470050811768
  batch 850 loss: 1.0827021837234496
  batch 900 loss: 1.1134607768058777
LOSS train 1.11346 valid 1.14776, valid PER 34.84%
EPOCH 7:
  batch 50 loss: 1.0794332242012024
  batch 100 loss: 1.093570877313614
  batch 150 loss: 1.0229767274856567
  batch 200 loss: 1.0504086589813233
  batch 250 loss: 1.0408195757865906
  batch 300 loss: 1.0178815269470214
  batch 350 loss: 1.041642359495163
  batch 400 loss: 1.0536882615089416
  batch 450 loss: 1.0293559908866883
  batch 500 loss: 1.049776257276535
  batch 550 loss: 1.0641349506378175
  batch 600 loss: 1.0803502476215363
  batch 650 loss: 1.0604322671890258
  batch 700 loss: 1.0725870680809022
  batch 750 loss: 1.0463570690155028
  batch 800 loss: 1.0547420024871825
  batch 850 loss: 1.0747500085830688
  batch 900 loss: 1.0883700942993164
LOSS train 1.08837 valid 1.13605, valid PER 34.30%
EPOCH 8:
  batch 50 loss: 1.0157712137699126
  batch 100 loss: 1.0408017206192017
  batch 150 loss: 1.0148301541805267
  batch 200 loss: 0.9885844588279724
  batch 250 loss: 1.0201000440120698
  batch 300 loss: 0.9752571427822113
  batch 350 loss: 1.045045142173767
  batch 400 loss: 0.9893629705905914
  batch 450 loss: 1.0507049238681794
  batch 500 loss: 1.0761055171489715
  batch 550 loss: 0.9933541882038116
  batch 600 loss: 1.0425776648521423
  batch 650 loss: 1.0380618345737458
  batch 700 loss: 0.9956872260570526
  batch 750 loss: 0.9889917612075806
  batch 800 loss: 1.0275493216514588
  batch 850 loss: 1.013896836042404
  batch 900 loss: 1.0396729958057405
LOSS train 1.03967 valid 1.13675, valid PER 34.15%
EPOCH 9:
  batch 50 loss: 0.9571703994274139
  batch 100 loss: 0.9819396603107452
  batch 150 loss: 0.9976390135288239
  batch 200 loss: 0.957252768278122
  batch 250 loss: 0.99548255443573
  batch 300 loss: 1.0080607116222382
  batch 350 loss: 1.005393295288086
  batch 400 loss: 0.975830739736557
  batch 450 loss: 1.0115826296806336
  batch 500 loss: 0.9681648910045624
  batch 550 loss: 0.9911098873615265
  batch 600 loss: 1.0149878191947936
  batch 650 loss: 0.9833598840236664
  batch 700 loss: 0.9853081941604614
  batch 750 loss: 1.0010259020328522
  batch 800 loss: 1.0089652919769287
  batch 850 loss: 1.0218856370449065
  batch 900 loss: 0.9771587431430817
LOSS train 0.97716 valid 1.12910, valid PER 33.37%
EPOCH 10:
  batch 50 loss: 0.9404182195663452
  batch 100 loss: 0.9414166271686554
  batch 150 loss: 0.9717209839820862
  batch 200 loss: 0.9569470107555389
  batch 250 loss: 0.9747040092945098
  batch 300 loss: 0.940809668302536
  batch 350 loss: 0.9884345090389252
  batch 400 loss: 0.9605982267856598
  batch 450 loss: 0.9548633754253387
  batch 500 loss: 1.006832582950592
  batch 550 loss: 1.014297341108322
  batch 600 loss: 1.0004226398468017
  batch 650 loss: 0.970329487323761
  batch 700 loss: 0.992222056388855
  batch 750 loss: 0.9622130107879638
  batch 800 loss: 0.9849550318717957
  batch 850 loss: 0.9822070384025574
  batch 900 loss: 1.002752652168274
LOSS train 1.00275 valid 1.10888, valid PER 33.33%
EPOCH 11:
  batch 50 loss: 0.9223709857463837
  batch 100 loss: 0.896884195804596
  batch 150 loss: 0.9144960355758667
  batch 200 loss: 0.929433091878891
  batch 250 loss: 0.9575483012199402
  batch 300 loss: 0.9073661482334137
  batch 350 loss: 0.9350740611553192
  batch 400 loss: 0.9664321506023407
  batch 450 loss: 0.944810653924942
  batch 500 loss: 0.9678490042686463
  batch 550 loss: 0.9613197624683381
  batch 600 loss: 0.9302046847343445
  batch 650 loss: 0.9802352797985077
  batch 700 loss: 0.9331405210494995
  batch 750 loss: 0.9091154551506042
  batch 800 loss: 0.9700656461715699
  batch 850 loss: 1.0082386875152587
  batch 900 loss: 0.9671422660350799
LOSS train 0.96714 valid 1.07614, valid PER 31.51%
EPOCH 12:
  batch 50 loss: 0.9052233362197876
  batch 100 loss: 0.9091425120830536
  batch 150 loss: 0.8709890508651733
  batch 200 loss: 0.9456829118728638
  batch 250 loss: 0.9522741568088532
  batch 300 loss: 0.9245679605007172
  batch 350 loss: 0.9345166552066803
  batch 400 loss: 0.9327255463600159
  batch 450 loss: 0.9412205719947815
  batch 500 loss: 0.9770103025436402
  batch 550 loss: 0.8885124289989471
  batch 600 loss: 0.9060833549499512
  batch 650 loss: 0.9649926590919494
  batch 700 loss: 0.9275155651569367
  batch 750 loss: 0.9128924012184143
  batch 800 loss: 0.9114649844169617
  batch 850 loss: 0.9313334345817565
  batch 900 loss: 0.9469866085052491
LOSS train 0.94699 valid 1.08422, valid PER 32.66%
EPOCH 13:
  batch 50 loss: 0.8994333529472351
  batch 100 loss: 0.9216207730770111
  batch 150 loss: 0.926256594657898
  batch 200 loss: 0.9176079666614533
  batch 250 loss: 0.8994623970985413
  batch 300 loss: 0.8861502051353455
  batch 350 loss: 0.910598156452179
  batch 400 loss: 0.9605746591091155
  batch 450 loss: 0.9617979252338409
  batch 500 loss: 0.9027942037582397
  batch 550 loss: 0.9494354367256165
  batch 600 loss: 0.9059847021102905
  batch 650 loss: 0.9515872502326965
  batch 700 loss: 0.9500091052055359
  batch 750 loss: 0.9176742148399353
  batch 800 loss: 0.929200679063797
  batch 850 loss: 0.9635802400112152
  batch 900 loss: 0.9818717861175537
LOSS train 0.98187 valid 1.14823, valid PER 33.68%
EPOCH 14:
  batch 50 loss: 0.91246053814888
  batch 100 loss: 0.8829828083515168
  batch 150 loss: 0.9723324739933014
  batch 200 loss: 1.2784557139873505
  batch 250 loss: 1.0949609279632568
  batch 300 loss: 1.0779640853405
  batch 350 loss: 1.0021652114391326
  batch 400 loss: 1.0160466480255126
  batch 450 loss: 1.0017524361610413
  batch 500 loss: 1.0036957275867462
  batch 550 loss: 1.01230513215065
  batch 600 loss: 0.9623348718881607
  batch 650 loss: 1.0055581378936767
  batch 700 loss: 1.013792369365692
  batch 750 loss: 0.9343405294418335
  batch 800 loss: 0.912121684551239
  batch 850 loss: 0.9852224445343017
  batch 900 loss: 0.9645518815517425
LOSS train 0.96455 valid 1.12023, valid PER 33.60%
EPOCH 15:
  batch 50 loss: 0.9026006388664246
  batch 100 loss: 0.9159343624114991
  batch 150 loss: 0.9156863451004028
  batch 200 loss: 0.9638838922977447
  batch 250 loss: 0.9319755697250366
  batch 300 loss: 0.9495769155025482
  batch 350 loss: 0.9403735029697419
  batch 400 loss: 0.9154826867580413
  batch 450 loss: 0.9729040586948394
  batch 500 loss: 0.9214939486980438
  batch 550 loss: 0.9554225409030914
  batch 600 loss: 0.9291035163402558
  batch 650 loss: 0.95956090092659
  batch 700 loss: 0.9577245879173278
  batch 750 loss: 0.9191645300388336
  batch 800 loss: 0.9149938261508942
  batch 850 loss: 0.9136700606346131
  batch 900 loss: 0.9358597040176392
LOSS train 0.93586 valid 1.12058, valid PER 33.16%
EPOCH 16:
  batch 50 loss: 0.9526885366439819
  batch 100 loss: 0.8828679537773132
  batch 150 loss: 0.8706053221225738
  batch 200 loss: 0.9094715762138367
  batch 250 loss: 0.9265921723842621
  batch 300 loss: 0.9166808092594146
  batch 350 loss: 0.9272592830657959
  batch 400 loss: 0.9366844916343688
  batch 450 loss: 0.9240779328346252
  batch 500 loss: 0.8577748358249664
  batch 550 loss: 0.9101851403713226
  batch 600 loss: 0.9413361287117005
  batch 650 loss: 0.9040876483917236
  batch 700 loss: 0.894565361738205
  batch 750 loss: 0.8978591859340668
  batch 800 loss: 0.9180612778663635
  batch 850 loss: 0.9069137501716614
  batch 900 loss: 0.9398499429225922
LOSS train 0.93985 valid 1.15392, valid PER 33.83%
EPOCH 17:
  batch 50 loss: 0.9092454135417938
  batch 100 loss: 0.9054633891582489
  batch 150 loss: 0.9043931460380554
  batch 200 loss: 0.8996088755130768
  batch 250 loss: 0.8887495696544647
  batch 300 loss: 0.9006180953979492
  batch 350 loss: 0.8483247041702271
  batch 400 loss: 0.9260478901863098
  batch 450 loss: 0.9080563795566559
  batch 500 loss: 0.8869757878780365
  batch 550 loss: 0.8887043797969818
  batch 600 loss: 0.9353853046894074
  batch 650 loss: 0.8946260070800781
  batch 700 loss: 0.920861234664917
  batch 750 loss: 0.9552971887588501
  batch 800 loss: 0.9303192877769471
  batch 850 loss: 0.9135910952091217
  batch 900 loss: 0.889640771150589
LOSS train 0.88964 valid 1.11301, valid PER 32.45%
EPOCH 18:
  batch 50 loss: 0.8623212337493896
  batch 100 loss: 0.8834665381908416
  batch 150 loss: 0.8699161350727082
  batch 200 loss: 0.8549456417560577
  batch 250 loss: 0.8645438253879547
  batch 300 loss: 0.8462704288959503
  batch 350 loss: 0.8818319034576416
  batch 400 loss: 0.8416989314556121
  batch 450 loss: 0.8845865416526795
  batch 500 loss: 0.8742930364608764
  batch 550 loss: 0.884153698682785
  batch 600 loss: 0.8623237955570221
  batch 650 loss: 0.8590082430839538
  batch 700 loss: 0.8929972267150879
  batch 750 loss: 0.8662769138813019
  batch 800 loss: 0.8781174516677857
  batch 850 loss: 0.8676865804195404
  batch 900 loss: 0.931437017917633
LOSS train 0.93144 valid 1.07852, valid PER 31.76%
EPOCH 19:
  batch 50 loss: 0.7919117176532745
  batch 100 loss: 0.804935816526413
  batch 150 loss: 0.7968865084648132
  batch 200 loss: 0.8508103680610657
  batch 250 loss: 0.8588329470157623
  batch 300 loss: 0.8996297812461853
  batch 350 loss: 0.8707058084011078
  batch 400 loss: 0.8796084928512573
  batch 450 loss: 0.9131807994842529
  batch 500 loss: 0.9023418629169464
  batch 550 loss: 0.8403529644012451
  batch 600 loss: 0.8708926868438721
  batch 650 loss: 0.9547695136070251
  batch 700 loss: 0.8980892789363861
  batch 750 loss: 0.851148202419281
  batch 800 loss: 0.8925248897075653
  batch 850 loss: 0.8952124285697937
  batch 900 loss: 0.900707894563675
LOSS train 0.90071 valid 1.11464, valid PER 32.82%
EPOCH 20:
  batch 50 loss: 0.8378295350074768
  batch 100 loss: 0.8486269390583039
  batch 150 loss: 0.8150126612186432
  batch 200 loss: 0.8492196488380432
  batch 250 loss: 0.8325766694545745
  batch 300 loss: 0.861780965924263
  batch 350 loss: 0.8109093928337097
  batch 400 loss: 0.8312182605266571
  batch 450 loss: 0.8218402779102325
  batch 500 loss: 0.801565523147583
  batch 550 loss: 0.8925351643562317
  batch 600 loss: 0.8461297082901001
  batch 650 loss: 0.892977374792099
  batch 700 loss: 0.8680426001548767
  batch 750 loss: 0.8529282116889954
  batch 800 loss: 0.9077614188194275
  batch 850 loss: 0.8830248129367828
  batch 900 loss: 0.8936280584335328
LOSS train 0.89363 valid 1.07747, valid PER 31.29%
train_loss
[1.723317472934723, 1.4282252955436707, 1.2575288605690003, 1.251344027519226, 1.1528104877471923, 1.1134607768058777, 1.0883700942993164, 1.0396729958057405, 0.9771587431430817, 1.002752652168274, 0.9671422660350799, 0.9469866085052491, 0.9818717861175537, 0.9645518815517425, 0.9358597040176392, 0.9398499429225922, 0.889640771150589, 0.931437017917633, 0.900707894563675, 0.8936280584335328]
valid_loss
[1.6961607933044434, 1.4409410953521729, 1.3248035907745361, 1.2452657222747803, 1.171194076538086, 1.1477575302124023, 1.136047601699829, 1.1367486715316772, 1.12909734249115, 1.1088800430297852, 1.0761399269104004, 1.0842225551605225, 1.1482276916503906, 1.1202325820922852, 1.1205767393112183, 1.1539199352264404, 1.1130095720291138, 1.078515887260437, 1.1146396398544312, 1.0774699449539185]
valid_per
[53.71283828822824, 44.80735901879749, 40.86121850419944, 37.87495000666578, 35.78856152512998, 34.835355285961874, 34.29542727636315, 34.14878016264498, 33.36888414878017, 33.32888948140248, 31.509132115717904, 32.65564591387815, 33.67550993200907, 33.595520597253696, 33.15557925609919, 33.82882282362352, 32.44900679909345, 31.762431675776565, 32.82229036128516, 31.289161445140646]
Training finished in 3.0 minutes.
Model saved to checkpoints/20231208_004404/model_11
Loading model from checkpoints/20231208_004404/model_11
SUB: 18.62%, DEL: 11.19%, INS: 3.23%, COR: 70.19%, PER: 33.03%
