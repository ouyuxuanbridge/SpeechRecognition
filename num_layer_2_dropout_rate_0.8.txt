Namespace(seed=123, train_json='/rds/user/yo279/hpc-work/MLMI2/fbanks/train_fbank.json', val_json='/rds/user/yo279/hpc-work/MLMI2/fbanks/dev_fbank.json', test_json='/rds/user/yo279/hpc-work/MLMI2/fbanks/test_fbank.json', batch_size=4, num_layers=2, fbank_dims=23, model_dims=128, concat=1, lr=0.5, vocab='vocab_39.txt', report_interval=50, num_epochs=20, dropout_rate=0.8, clip_max_norm=1)
Total number of model parameters is 562216
EPOCH 1:
  batch 50 loss: 4.148863639831543
  batch 100 loss: 3.298736124038696
  batch 150 loss: 3.2486676502227785
  batch 200 loss: 3.1548011255264283
  batch 250 loss: 3.0199102544784546
  batch 300 loss: 2.8197692823410034
  batch 350 loss: 2.710319437980652
  batch 400 loss: 2.6096796655654906
  batch 450 loss: 2.516448431015015
  batch 500 loss: 2.382721347808838
  batch 550 loss: 2.317854061126709
  batch 600 loss: 2.278292889595032
  batch 650 loss: 2.1883721852302553
  batch 700 loss: 2.163086576461792
  batch 750 loss: 2.0908814430236817
  batch 800 loss: 2.0496385145187377
  batch 850 loss: 1.9975146818161011
  batch 900 loss: 1.9841845774650573
LOSS train 1.98418 valid 1.82381, valid PER 68.54%
EPOCH 2:
  batch 50 loss: 1.900434672832489
  batch 100 loss: 1.81615207195282
  batch 150 loss: 1.8131378030776977
  batch 200 loss: 1.7958717584609984
  batch 250 loss: 1.8027958059310913
  batch 300 loss: 1.7399796962738037
  batch 350 loss: 1.6685059762001038
  batch 400 loss: 1.6718298625946044
  batch 450 loss: 1.6029658818244934
  batch 500 loss: 1.6413874888420106
  batch 550 loss: 1.6464360737800598
  batch 600 loss: 1.58463862657547
  batch 650 loss: 1.594584150314331
  batch 700 loss: 1.5442079710960388
  batch 750 loss: 1.5394301366806031
  batch 800 loss: 1.465778934955597
  batch 850 loss: 1.478473563194275
  batch 900 loss: 1.4707902216911315
LOSS train 1.47079 valid 1.33571, valid PER 41.67%
EPOCH 3:
  batch 50 loss: 1.4513717699050903
  batch 100 loss: 1.3969653344154358
  batch 150 loss: 1.4095216035842895
  batch 200 loss: 1.384232108592987
  batch 250 loss: 1.3528490948677063
  batch 300 loss: 1.3791571021080018
  batch 350 loss: 1.4088509750366212
  batch 400 loss: 1.374200234413147
  batch 450 loss: 1.3590734148025512
  batch 500 loss: 1.3414060330390931
  batch 550 loss: 1.3448630928993226
  batch 600 loss: 1.3109887337684631
  batch 650 loss: 1.2825427722930909
  batch 700 loss: 1.3062209165096283
  batch 750 loss: 1.3605037713050843
  batch 800 loss: 1.2838688683509827
  batch 850 loss: 1.3059350109100343
  batch 900 loss: 1.2345874869823457
LOSS train 1.23459 valid 1.24847, valid PER 37.29%
EPOCH 4:
  batch 50 loss: 1.240876715183258
  batch 100 loss: 1.266221432685852
  batch 150 loss: 1.2106085669994355
  batch 200 loss: 1.259971342086792
  batch 250 loss: 1.2400439810752868
  batch 300 loss: 1.2726963543891907
  batch 350 loss: 1.1884086978435517
  batch 400 loss: 1.2407745480537415
  batch 450 loss: 1.2067974185943604
  batch 500 loss: 1.1951554477214814
  batch 550 loss: 1.2116806375980378
  batch 600 loss: 1.2323408603668213
  batch 650 loss: 1.2337618255615235
  batch 700 loss: 1.1916686522960662
  batch 750 loss: 1.1652071356773377
  batch 800 loss: 1.1356194055080413
  batch 850 loss: 1.1590525674819947
  batch 900 loss: 1.226802978515625
LOSS train 1.22680 valid 1.07062, valid PER 32.84%
EPOCH 5:
  batch 50 loss: 1.14675288438797
  batch 100 loss: 1.1431688833236695
  batch 150 loss: 1.1804219734668733
  batch 200 loss: 1.1275817918777467
  batch 250 loss: 1.1445513546466828
  batch 300 loss: 1.148309588432312
  batch 350 loss: 1.136534299850464
  batch 400 loss: 1.15543869972229
  batch 450 loss: 1.1259198486804962
  batch 500 loss: 1.167756301164627
  batch 550 loss: 1.0684014225006104
  batch 600 loss: 1.1770154070854186
  batch 650 loss: 1.1248616182804108
  batch 700 loss: 1.1566276848316193
  batch 750 loss: 1.0918546950817107
  batch 800 loss: 1.1184312009811401
  batch 850 loss: 1.0999893379211425
  batch 900 loss: 1.1321697962284087
LOSS train 1.13217 valid 1.03234, valid PER 31.07%
EPOCH 6:
  batch 50 loss: 1.13523029088974
  batch 100 loss: 1.0630892169475556
  batch 150 loss: 1.0777542662620545
  batch 200 loss: 1.0713160955905914
  batch 250 loss: 1.1186975288391112
  batch 300 loss: 1.0927841019630433
  batch 350 loss: 1.0934144675731658
  batch 400 loss: 1.0752510404586793
  batch 450 loss: 1.1031999826431274
  batch 500 loss: 1.0907237780094148
  batch 550 loss: 1.096103857755661
  batch 600 loss: 1.082899442911148
  batch 650 loss: 1.0793770396709441
  batch 700 loss: 1.0671355545520782
  batch 750 loss: 1.071030876636505
  batch 800 loss: 1.0807283675670625
  batch 850 loss: 1.0454243099689484
  batch 900 loss: 1.0810600113868714
LOSS train 1.08106 valid 0.98388, valid PER 30.42%
EPOCH 7:
  batch 50 loss: 1.0546921157836915
  batch 100 loss: 1.085481232404709
  batch 150 loss: 1.040836637020111
  batch 200 loss: 1.0265846312046052
  batch 250 loss: 1.0170219123363495
  batch 300 loss: 1.025548392534256
  batch 350 loss: 1.0114089941978455
  batch 400 loss: 1.03090851187706
  batch 450 loss: 1.0435677862167358
  batch 500 loss: 1.0340361142158507
  batch 550 loss: 1.0353159022331238
  batch 600 loss: 1.0456238555908204
  batch 650 loss: 1.0318922650814057
  batch 700 loss: 1.0523044514656066
  batch 750 loss: 1.064755573272705
  batch 800 loss: 1.0496529507637025
  batch 850 loss: 1.0457501697540283
  batch 900 loss: 1.0742757630348205
LOSS train 1.07428 valid 1.04571, valid PER 32.16%
EPOCH 8:
  batch 50 loss: 1.0446684336662293
  batch 100 loss: 1.0060069465637207
  batch 150 loss: 0.9954587185382843
  batch 200 loss: 0.9803920567035675
  batch 250 loss: 0.9981843602657318
  batch 300 loss: 0.9830371224880219
  batch 350 loss: 1.0276197409629821
  batch 400 loss: 0.9834567844867707
  batch 450 loss: 0.9972671794891358
  batch 500 loss: 1.0317350327968597
  batch 550 loss: 0.9613841688632965
  batch 600 loss: 1.0396811830997468
  batch 650 loss: 1.0335721004009246
  batch 700 loss: 1.0080409240722656
  batch 750 loss: 0.9947632896900177
  batch 800 loss: 0.9896296274662018
  batch 850 loss: 1.0035467553138733
  batch 900 loss: 1.0018370759487152
LOSS train 1.00184 valid 0.92116, valid PER 28.42%
EPOCH 9:
  batch 50 loss: 0.9540813159942627
  batch 100 loss: 0.9872178542613983
  batch 150 loss: 0.9894899892807006
  batch 200 loss: 0.9815193998813629
  batch 250 loss: 0.9895069491863251
  batch 300 loss: 0.9960137450695038
  batch 350 loss: 0.9954534947872162
  batch 400 loss: 0.9589298868179321
  batch 450 loss: 0.9686190223693848
  batch 500 loss: 0.9547859942913055
  batch 550 loss: 0.9649092674255371
  batch 600 loss: 0.9683893597126008
  batch 650 loss: 0.9548375391960144
  batch 700 loss: 0.9585004043579102
  batch 750 loss: 0.9717777919769287
  batch 800 loss: 0.9897554850578308
  batch 850 loss: 1.0009634268283845
  batch 900 loss: 0.9458421564102173
LOSS train 0.94584 valid 0.94020, valid PER 28.40%
EPOCH 10:
  batch 50 loss: 0.9291680073738098
  batch 100 loss: 0.9458387053012848
  batch 150 loss: 0.9572365796566009
  batch 200 loss: 0.97024080991745
  batch 250 loss: 0.9472046232223511
  batch 300 loss: 0.9253681766986847
  batch 350 loss: 0.9675916063785553
  batch 400 loss: 0.9321275854110718
  batch 450 loss: 0.9377815818786621
  batch 500 loss: 0.9687630581855774
  batch 550 loss: 0.9836417698860168
  batch 600 loss: 0.9420411646366119
  batch 650 loss: 0.9325698554515839
  batch 700 loss: 0.9557156431674957
  batch 750 loss: 0.9253239965438843
  batch 800 loss: 0.9544872534275055
  batch 850 loss: 0.9639389073848724
  batch 900 loss: 0.9771220266819001
LOSS train 0.97712 valid 0.89283, valid PER 27.96%
EPOCH 11:
  batch 50 loss: 0.9038012731075287
  batch 100 loss: 0.8673175585269928
  batch 150 loss: 0.9060768175125122
  batch 200 loss: 0.9782847774028778
  batch 250 loss: 0.9948428666591644
  batch 300 loss: 0.932416090965271
  batch 350 loss: 0.9314837074279785
  batch 400 loss: 0.9456734609603882
  batch 450 loss: 0.9299871051311492
  batch 500 loss: 0.9086449432373047
  batch 550 loss: 0.9225898671150208
  batch 600 loss: 0.9056703341007233
  batch 650 loss: 0.9721631431579589
  batch 700 loss: 0.8911611378192902
  batch 750 loss: 0.9034301567077637
  batch 800 loss: 0.9422520279884339
  batch 850 loss: 0.94947345495224
  batch 900 loss: 0.9400067067146302
LOSS train 0.94001 valid 0.87263, valid PER 27.16%
EPOCH 12:
  batch 50 loss: 0.9098973059654236
  batch 100 loss: 0.8815747117996215
  batch 150 loss: 0.8617197799682618
  batch 200 loss: 0.8983923935890198
  batch 250 loss: 0.919213502407074
  batch 300 loss: 0.888586003780365
  batch 350 loss: 0.8926682031154632
  batch 400 loss: 0.9240089666843414
  batch 450 loss: 0.9232349479198456
  batch 500 loss: 0.9141660296916961
  batch 550 loss: 0.8724820613861084
  batch 600 loss: 0.8920630919933319
  batch 650 loss: 0.9413875150680542
  batch 700 loss: 0.9111263608932495
  batch 750 loss: 0.8898955643177032
  batch 800 loss: 0.8848978984355926
  batch 850 loss: 0.927132009267807
  batch 900 loss: 0.9121456074714661
LOSS train 0.91215 valid 0.87927, valid PER 26.84%
EPOCH 13:
  batch 50 loss: 0.8644442701339722
  batch 100 loss: 0.8837056076526641
  batch 150 loss: 0.8692647457122803
  batch 200 loss: 0.8957670748233795
  batch 250 loss: 0.8844702792167664
  batch 300 loss: 0.8634941136837005
  batch 350 loss: 0.8708214950561524
  batch 400 loss: 0.9031623983383179
  batch 450 loss: 0.9101719796657562
  batch 500 loss: 0.8648697638511658
  batch 550 loss: 0.899133061170578
  batch 600 loss: 0.8907838451862335
  batch 650 loss: 0.9168510854244232
  batch 700 loss: 0.9060325026512146
  batch 750 loss: 0.8659775459766388
  batch 800 loss: 0.8797601413726807
  batch 850 loss: 0.9132395303249359
  batch 900 loss: 0.8968808746337891
LOSS train 0.89688 valid 0.86819, valid PER 26.48%
EPOCH 14:
  batch 50 loss: 0.8551867496967316
  batch 100 loss: 0.8787058782577515
  batch 150 loss: 0.8612730896472931
  batch 200 loss: 0.878892183303833
  batch 250 loss: 0.8627139401435852
  batch 300 loss: 0.9024113643169404
  batch 350 loss: 0.8316843795776367
  batch 400 loss: 0.8694148457050324
  batch 450 loss: 0.8677249264717102
  batch 500 loss: 0.8837713289260865
  batch 550 loss: 0.8891472470760345
  batch 600 loss: 0.8559317851066589
  batch 650 loss: 0.880561295747757
  batch 700 loss: 0.9180061221122742
  batch 750 loss: 0.8917024266719819
  batch 800 loss: 0.8188081967830658
  batch 850 loss: 0.8916684651374817
  batch 900 loss: 0.875062837600708
LOSS train 0.87506 valid 0.84882, valid PER 26.13%
EPOCH 15:
  batch 50 loss: 0.8611205637454986
  batch 100 loss: 0.8483324587345124
  batch 150 loss: 0.8586027634143829
  batch 200 loss: 0.8989596509933472
  batch 250 loss: 0.9119523131847381
  batch 300 loss: 0.8698255383968353
  batch 350 loss: 0.8471060681343079
  batch 400 loss: 0.839093519449234
  batch 450 loss: 0.906115380525589
  batch 500 loss: 0.9308912980556489
  batch 550 loss: 0.9173422110080719
  batch 600 loss: 0.958916745185852
  batch 650 loss: 0.9313526117801666
  batch 700 loss: 0.912915494441986
  batch 750 loss: 0.8831204414367676
  batch 800 loss: 0.8715553271770478
  batch 850 loss: 0.8437306463718415
  batch 900 loss: 0.882345621585846
LOSS train 0.88235 valid 0.86049, valid PER 26.64%
EPOCH 16:
  batch 50 loss: 0.8941825890541076
  batch 100 loss: 0.8241804778575897
  batch 150 loss: 0.8267590343952179
  batch 200 loss: 0.8741504216194153
  batch 250 loss: 0.8826367640495301
  batch 300 loss: 0.8514089095592499
  batch 350 loss: 0.8889343297481537
  batch 400 loss: 0.8797498691082001
  batch 450 loss: 0.878579454421997
  batch 500 loss: 0.8444368219375611
  batch 550 loss: 0.8901296424865722
  batch 600 loss: 0.8512986242771149
  batch 650 loss: 0.8761611557006836
  batch 700 loss: 0.8480281019210816
  batch 750 loss: 0.8378247022628784
  batch 800 loss: 0.8541456866264343
  batch 850 loss: 0.8447499418258667
  batch 900 loss: 0.8224767923355103
LOSS train 0.82248 valid 0.83116, valid PER 25.48%
EPOCH 17:
  batch 50 loss: 0.8389180111885071
  batch 100 loss: 0.8466604661941528
  batch 150 loss: 0.8674605250358581
  batch 200 loss: 0.8443444526195526
  batch 250 loss: 0.8545004904270173
  batch 300 loss: 0.8516464793682098
  batch 350 loss: 0.8341544950008393
  batch 400 loss: 0.8672896289825439
  batch 450 loss: 0.8424180674552918
  batch 500 loss: 0.8232161128520965
  batch 550 loss: 0.8390588724613189
  batch 600 loss: 0.8716467905044556
  batch 650 loss: 0.8140208995342255
  batch 700 loss: 0.8143768632411956
  batch 750 loss: 0.8122960090637207
  batch 800 loss: 0.8071001446247101
  batch 850 loss: 0.8304916954040528
  batch 900 loss: 0.8072602462768554
LOSS train 0.80726 valid 0.82383, valid PER 25.22%
EPOCH 18:
  batch 50 loss: 0.8294881069660187
  batch 100 loss: 0.835749261379242
  batch 150 loss: 0.8390961289405823
  batch 200 loss: 0.7986148047447205
  batch 250 loss: 0.8170367121696472
  batch 300 loss: 0.8028487420082092
  batch 350 loss: 0.8749921953678131
  batch 400 loss: 0.8316591215133667
  batch 450 loss: 0.8639815425872803
  batch 500 loss: 0.8276389586925507
  batch 550 loss: 0.8564301574230194
  batch 600 loss: 0.8111646282672882
  batch 650 loss: 0.7997989141941071
  batch 700 loss: 0.8576368176937104
  batch 750 loss: 0.822974408864975
  batch 800 loss: 0.8101907050609589
  batch 850 loss: 0.813549622297287
  batch 900 loss: 0.8541333174705505
LOSS train 0.85413 valid 0.82387, valid PER 25.38%
EPOCH 19:
  batch 50 loss: 0.7780909717082978
  batch 100 loss: 0.8405974245071411
  batch 150 loss: 0.8391410148143769
  batch 200 loss: 0.8210400545597076
  batch 250 loss: 0.877280306816101
  batch 300 loss: 0.9610372304916381
  batch 350 loss: 0.9131576251983643
  batch 400 loss: 0.8971577918529511
  batch 450 loss: 0.8924933993816375
  batch 500 loss: 0.8679196798801422
  batch 550 loss: 0.8342231678962707
  batch 600 loss: 0.8529866635799408
  batch 650 loss: 0.9316881895065308
  batch 700 loss: 0.9015393841266632
  batch 750 loss: 0.8613283932209015
  batch 800 loss: 0.8710122239589692
  batch 850 loss: 0.877793824672699
  batch 900 loss: 0.8561450111865997
LOSS train 0.85615 valid 0.85215, valid PER 26.36%
EPOCH 20:
  batch 50 loss: 0.813620709180832
  batch 100 loss: 0.8048694801330566
  batch 150 loss: 0.7898679631948471
  batch 200 loss: 0.8390918838977813
  batch 250 loss: 0.8057358574867248
  batch 300 loss: 0.8407883250713348
  batch 350 loss: 0.7792403262853622
  batch 400 loss: 0.8138883596658707
  batch 450 loss: 0.8323837864398956
  batch 500 loss: 0.8093932676315307
  batch 550 loss: 0.8775143802165986
  batch 600 loss: 0.8084524118900299
  batch 650 loss: 0.8522321951389312
  batch 700 loss: 0.8342990720272064
  batch 750 loss: 0.8215140545368195
  batch 800 loss: 0.8692729473114014
  batch 850 loss: 0.8697513973712921
  batch 900 loss: 0.8449564063549042
LOSS train 0.84496 valid 0.82252, valid PER 25.28%
train_loss
[1.9841845774650573, 1.4707902216911315, 1.2345874869823457, 1.226802978515625, 1.1321697962284087, 1.0810600113868714, 1.0742757630348205, 1.0018370759487152, 0.9458421564102173, 0.9771220266819001, 0.9400067067146302, 0.9121456074714661, 0.8968808746337891, 0.875062837600708, 0.882345621585846, 0.8224767923355103, 0.8072602462768554, 0.8541333174705505, 0.8561450111865997, 0.8449564063549042]
valid_loss
[1.8238117694854736, 1.3357069492340088, 1.2484675645828247, 1.0706202983856201, 1.0323421955108643, 0.9838764667510986, 1.0457128286361694, 0.9211612343788147, 0.9401994347572327, 0.8928256034851074, 0.8726274967193604, 0.879271388053894, 0.8681896924972534, 0.8488222360610962, 0.8604873418807983, 0.8311580419540405, 0.8238260746002197, 0.8238717317581177, 0.8521517515182495, 0.8225202560424805]
valid_per
[68.53752832955607, 41.66777762964938, 37.288361551793095, 32.835621917077724, 31.069190774563392, 30.422610318624184, 32.16237834955339, 28.422876949740033, 28.39621383815491, 27.962938274896683, 27.163044927343023, 26.843087588321556, 26.48313558192241, 26.129849353419544, 26.636448473536863, 25.476603119584055, 25.223303559525394, 25.37661645113985, 26.356485801893083, 25.276629782695643]
Training finished in 4.0 minutes.
Model saved to checkpoints/20231208_000538/model_20
Loading model from checkpoints/20231208_000538/model_20
SUB: 16.38%, DEL: 8.31%, INS: 2.59%, COR: 75.31%, PER: 27.28%
