Namespace(seed=123, train_json='/rds/user/yo279/hpc-work/MLMI2/fbanks/train_fbank.json', val_json='/rds/user/yo279/hpc-work/MLMI2/fbanks/dev_fbank.json', test_json='/rds/user/yo279/hpc-work/MLMI2/fbanks/test_fbank.json', batch_size=4, num_layers=1, fbank_dims=23, model_dims=128, concat=1, lr=0.5, vocab='vocab_39.txt', report_interval=50, num_epochs=20, dropout_rate=0.5, clip_max_norm=1, num_ff_layers=2)
Total number of model parameters is 83496
EPOCH 1:
  batch 50 loss: 4.08084267616272
  batch 100 loss: 3.09796199798584
  batch 150 loss: 2.9758445596694947
  batch 200 loss: 2.902302484512329
  batch 250 loss: 2.7492351007461546
  batch 300 loss: 2.564369421005249
  batch 350 loss: 2.4772272729873657
  batch 400 loss: 2.4129678297042845
  batch 450 loss: 2.3620717573165892
  batch 500 loss: 2.2727213406562807
  batch 550 loss: 2.2701185297966004
  batch 600 loss: 2.1635007953643797
  batch 650 loss: 2.0819571375846864
  batch 700 loss: 2.1025990200042726
  batch 750 loss: 2.035246818065643
  batch 800 loss: 1.9984792184829712
  batch 850 loss: 1.964931719303131
  batch 900 loss: 1.9008122420310973
LOSS train 1.90081 valid 1.88195, valid PER 69.11%
EPOCH 2:
  batch 50 loss: 1.9939239311218262
  batch 100 loss: 1.8556923007965087
  batch 150 loss: 1.831762547492981
  batch 200 loss: 1.807273371219635
  batch 250 loss: 1.7999538064002991
  batch 300 loss: 1.748663866519928
  batch 350 loss: 1.6592452096939088
  batch 400 loss: 1.673699278831482
  batch 450 loss: 1.614448640346527
  batch 500 loss: 1.6322017645835876
  batch 550 loss: 1.646941602230072
  batch 600 loss: 1.5774153900146484
  batch 650 loss: 1.5991164660453796
  batch 700 loss: 1.5878537273406983
  batch 750 loss: 1.552067301273346
  batch 800 loss: 1.5018890237808227
  batch 850 loss: 1.4888237285614014
  batch 900 loss: 1.5079028487205506
LOSS train 1.50790 valid 1.49387, valid PER 46.45%
EPOCH 3:
  batch 50 loss: 1.4620486450195314
  batch 100 loss: 1.4762936425209046
  batch 150 loss: 1.470748951435089
  batch 200 loss: 1.4278405141830444
  batch 250 loss: 1.4152731585502625
  batch 300 loss: 1.4213735461235046
  batch 350 loss: 1.4522745394706726
  batch 400 loss: 1.4273774576187135
  batch 450 loss: 1.385049331188202
  batch 500 loss: 1.4425262212753296
  batch 550 loss: 1.3899717950820922
  batch 600 loss: 1.396179007291794
  batch 650 loss: 1.3761101531982423
  batch 700 loss: 1.4425008392333984
  batch 750 loss: 1.4999544048309326
  batch 800 loss: 1.3337036061286927
  batch 850 loss: 1.3942304396629333
  batch 900 loss: 1.345081729888916
LOSS train 1.34508 valid 1.40753, valid PER 43.63%
EPOCH 4:
  batch 50 loss: 1.3442947006225585
  batch 100 loss: 1.3970921182632445
  batch 150 loss: 1.3428995156288146
  batch 200 loss: 1.3825512218475342
  batch 250 loss: 1.359902594089508
  batch 300 loss: 1.3839145278930665
  batch 350 loss: 1.303072919845581
  batch 400 loss: 1.3590461993217469
  batch 450 loss: 1.3262773287296294
  batch 500 loss: 1.2862772393226622
  batch 550 loss: 1.3322143507003785
  batch 600 loss: 1.364737675189972
  batch 650 loss: 1.35261013507843
  batch 700 loss: 1.321053113937378
  batch 750 loss: 1.284989697933197
  batch 800 loss: 1.276339054107666
  batch 850 loss: 1.3355019855499268
  batch 900 loss: 1.3857873260974884
LOSS train 1.38579 valid 1.34193, valid PER 42.30%
EPOCH 5:
  batch 50 loss: 1.2673860096931457
  batch 100 loss: 1.3542558193206786
  batch 150 loss: 1.334010398387909
  batch 200 loss: 1.2932461595535278
  batch 250 loss: 1.366836793422699
  batch 300 loss: 1.4663877630233764
  batch 350 loss: 1.3779661798477172
  batch 400 loss: 1.4911197638511657
  batch 450 loss: 1.3191935956478118
  batch 500 loss: 1.3625723028182983
  batch 550 loss: 1.3031803810596465
  batch 600 loss: 1.332046263217926
  batch 650 loss: 1.2794356417655945
  batch 700 loss: 1.3473824310302733
  batch 750 loss: 1.2851839542388916
  batch 800 loss: 1.316085591316223
  batch 850 loss: 1.3222259449958802
  batch 900 loss: 1.2993795764446259
LOSS train 1.29938 valid 1.38844, valid PER 41.86%
EPOCH 6:
  batch 50 loss: 1.3488476586341858
  batch 100 loss: 1.2818983840942382
  batch 150 loss: 1.2911815524101258
  batch 200 loss: 1.3064732694625854
  batch 250 loss: 1.3164309108257293
  batch 300 loss: 1.268083976507187
  batch 350 loss: 1.2692705929279327
  batch 400 loss: 1.3179968976974488
  batch 450 loss: 1.3433629512786864
  batch 500 loss: 1.2844040393829346
  batch 550 loss: 1.2719913363456725
  batch 600 loss: 1.232748200893402
  batch 650 loss: 1.2715565752983093
  batch 700 loss: 1.249837715625763
  batch 750 loss: 1.2395100951194764
  batch 800 loss: 1.229809614419937
  batch 850 loss: 1.19566281914711
  batch 900 loss: 1.2446889901161193
LOSS train 1.24469 valid 1.25334, valid PER 39.93%
EPOCH 7:
  batch 50 loss: 1.2425003886222838
  batch 100 loss: 1.313439748287201
  batch 150 loss: 1.3486438274383545
  batch 200 loss: 1.2651408767700196
  batch 250 loss: 1.3107196640968324
  batch 300 loss: 1.2776141309738158
  batch 350 loss: 1.358586220741272
  batch 400 loss: 1.375817630290985
  batch 450 loss: 1.3083341884613038
  batch 500 loss: 1.2707910990715028
  batch 550 loss: 1.3049577116966247
  batch 600 loss: 1.3061876320838928
  batch 650 loss: 1.2907940888404845
  batch 700 loss: 1.283102458715439
  batch 750 loss: 1.2313534772396089
  batch 800 loss: 1.2735314846038819
  batch 850 loss: 1.355084354877472
  batch 900 loss: 1.314392488002777
LOSS train 1.31439 valid 1.28440, valid PER 40.53%
EPOCH 8:
  batch 50 loss: 1.2436558496952057
  batch 100 loss: 1.3005771553516388
  batch 150 loss: 1.217357028722763
  batch 200 loss: 1.223990284204483
  batch 250 loss: 1.2613029611110687
  batch 300 loss: 1.2183194518089295
  batch 350 loss: 1.3010886490345002
  batch 400 loss: 1.2252652537822724
  batch 450 loss: 1.2542684435844422
  batch 500 loss: 1.278977290391922
  batch 550 loss: 1.210162091255188
  batch 600 loss: 1.2593636083602906
  batch 650 loss: 1.267293119430542
  batch 700 loss: 1.2507716631889343
  batch 750 loss: 1.2808172929286956
  batch 800 loss: 1.2757302057743072
  batch 850 loss: 1.3374098443984985
  batch 900 loss: 1.3605163025856017
LOSS train 1.36052 valid 1.38154, valid PER 42.45%
EPOCH 9:
  batch 50 loss: 1.2490080499649048
  batch 100 loss: 1.288187186717987
  batch 150 loss: 1.2586349630355835
  batch 200 loss: 1.18665137052536
  batch 250 loss: 1.2199458038806916
  batch 300 loss: 1.3127269625663758
  batch 350 loss: 1.3020345056056977
  batch 400 loss: 1.238680294752121
  batch 450 loss: 1.2895341992378235
  batch 500 loss: 1.195062061548233
  batch 550 loss: 1.257569054365158
  batch 600 loss: 1.2463222360610962
  batch 650 loss: 1.2116905069351196
  batch 700 loss: 1.2115317690372467
  batch 750 loss: 1.2352391123771667
  batch 800 loss: 1.2809383356571198
  batch 850 loss: 1.3164700829982758
  batch 900 loss: 1.2152014875411987
LOSS train 1.21520 valid 1.31601, valid PER 39.58%
EPOCH 10:
  batch 50 loss: 1.2166637086868286
  batch 100 loss: 1.2110189068317414
  batch 150 loss: 1.263310990333557
  batch 200 loss: 1.2010760998725891
  batch 250 loss: 1.2322250163555146
  batch 300 loss: 1.1838641893863677
  batch 350 loss: 1.2427751517295837
  batch 400 loss: 1.3208598732948302
  batch 450 loss: 1.2453318738937378
  batch 500 loss: 1.2712679195404053
  batch 550 loss: 1.2957223057746887
  batch 600 loss: 1.2489003145694733
  batch 650 loss: 1.2456100940704347
  batch 700 loss: 1.3022093427181245
  batch 750 loss: 1.5354935598373414
  batch 800 loss: 1.4192093586921692
  batch 850 loss: 1.5065776848793029
  batch 900 loss: 1.3623595309257508
LOSS train 1.36236 valid 1.37947, valid PER 43.73%
EPOCH 11:
  batch 50 loss: 1.32938826918602
  batch 100 loss: 1.3941404461860656
  batch 150 loss: 1.314472805261612
  batch 200 loss: 1.333479917049408
  batch 250 loss: 1.282505898475647
  batch 300 loss: 1.3104782342910766
  batch 350 loss: 1.3093100500106811
  batch 400 loss: 1.3502792954444884
  batch 450 loss: 1.3093335962295531
  batch 500 loss: 1.3100543594360352
  batch 550 loss: 1.5952654814720153
  batch 600 loss: 1.4063581657409667
  batch 650 loss: 1.3968432450294495
  batch 700 loss: 1.3095014905929565
  batch 750 loss: 1.392930407524109
  batch 800 loss: 1.453518214225769
  batch 850 loss: 1.4819536876678467
  batch 900 loss: 1.5725650358200074
LOSS train 1.57257 valid 1.51440, valid PER 45.11%
EPOCH 12:
  batch 50 loss: 1.4174906182289124
  batch 100 loss: 1.3767846703529358
  batch 150 loss: 1.3762372183799743
  batch 200 loss: 1.3920102787017823
  batch 250 loss: 1.4446059226989747
  batch 300 loss: 1.3787992811203003
  batch 350 loss: 1.3759594917297364
  batch 400 loss: 1.3978178358078004
  batch 450 loss: 1.4065375018119812
  batch 500 loss: 1.4134597659111023
  batch 550 loss: 1.3331515192985535
  batch 600 loss: 1.3645809936523436
  batch 650 loss: 1.4481001329421996
  batch 700 loss: 1.3895958828926087
  batch 750 loss: 1.344333598613739
  batch 800 loss: 1.3144892048835755
  batch 850 loss: 1.3689721608161927
  batch 900 loss: 1.4494823026657104
LOSS train 1.44948 valid 1.41390, valid PER 42.81%
EPOCH 13:
  batch 50 loss: 1.39236159324646
  batch 100 loss: 1.3774283480644227
  batch 150 loss: 1.2999074816703797
  batch 200 loss: 1.3292133951187133
  batch 250 loss: 1.3450027346611022
  batch 300 loss: 1.283861436843872
  batch 350 loss: 1.3087662255764008
  batch 400 loss: 1.3166138982772828
  batch 450 loss: 1.3468189692497254
  batch 500 loss: 1.2696210408210755
  batch 550 loss: 1.3028383898735045
  batch 600 loss: 1.2986840987205506
  batch 650 loss: 1.2685757756233216
  batch 700 loss: 1.3649874532222748
  batch 750 loss: 1.4106157326698303
  batch 800 loss: 1.3187804794311524
  batch 850 loss: 1.3448897862434388
  batch 900 loss: 1.3254782915115357
LOSS train 1.32548 valid 1.42903, valid PER 42.75%
EPOCH 14:
  batch 50 loss: 1.298884859085083
  batch 100 loss: 1.3001347982883453
  batch 150 loss: 1.4228580594062805
  batch 200 loss: 1.5143428778648376
  batch 250 loss: 1.4125283813476563
  batch 300 loss: 1.5325790882110595
  batch 350 loss: 2.306866068840027
  batch 400 loss: 2.1346524286270143
  batch 450 loss: 1.94315260887146
  batch 500 loss: 1.8861525273323059
  batch 550 loss: 2.509016664028168
  batch 600 loss: 2.1125120878219605
  batch 650 loss: 1.9461092042922974
  batch 700 loss: 1.8805413889884948
  batch 750 loss: 1.782674195766449
  batch 800 loss: 1.6723091530799865
  batch 850 loss: 1.9128384923934936
  batch 900 loss: 1.8475441527366638
LOSS train 1.84754 valid 1.96096, valid PER 58.01%
EPOCH 15:
  batch 50 loss: 1.9998638153076171
  batch 100 loss: 1.8523297381401063
  batch 150 loss: 1.7438880467414857
  batch 200 loss: 1.7523715496063232
  batch 250 loss: 1.9080639672279358
  batch 300 loss: 1.6967409396171569
  batch 350 loss: 1.6711455845832826
  batch 400 loss: 1.688686785697937
  batch 450 loss: 1.6567163372039795
  batch 500 loss: 1.7099417924880982
  batch 550 loss: 1.8523834919929505
  batch 600 loss: 1.7733107471466065
  batch 650 loss: 1.7205067873001099
  batch 700 loss: 1.6919992852210999
  batch 750 loss: 1.607738003730774
  batch 800 loss: 1.7396381688117981
  batch 850 loss: 1.785213465690613
  batch 900 loss: 1.6987072396278382
LOSS train 1.69871 valid 1.84328, valid PER 53.39%
EPOCH 16:
  batch 50 loss: 1.6859559559822082
  batch 100 loss: 1.6502937507629394
  batch 150 loss: 1.785190727710724
  batch 200 loss: 1.648251349925995
  batch 250 loss: 1.7307513189315795
  batch 300 loss: 1.6669462299346924
  batch 350 loss: 1.6567730903625488
  batch 400 loss: 1.5967053866386414
  batch 450 loss: 1.6638285946846008
  batch 500 loss: 1.543226490020752
  batch 550 loss: 1.5992626547813416
  batch 600 loss: 1.5367216897010803
  batch 650 loss: 1.5191369891166686
  batch 700 loss: 1.5462063717842103
  batch 750 loss: 1.5506312036514283
  batch 800 loss: 1.522248888015747
  batch 850 loss: 1.5658403134346008
  batch 900 loss: 1.5790904307365417
LOSS train 1.57909 valid 1.60728, valid PER 48.09%
EPOCH 17:
  batch 50 loss: 1.5340778183937074
  batch 100 loss: 1.5299383878707886
  batch 150 loss: 1.4814374089241027
  batch 200 loss: 1.4579061698913574
  batch 250 loss: 1.4817932486534118
  batch 300 loss: 1.4733248257637024
  batch 350 loss: 1.6787143516540528
  batch 400 loss: 1.6858837127685546
  batch 450 loss: 1.8379010725021363
  batch 500 loss: 1.752639443874359
  batch 550 loss: 1.7819530725479127
  batch 600 loss: 1.7304725551605225
  batch 650 loss: 1.6151206755638123
  batch 700 loss: 1.5850022268295287
  batch 750 loss: 1.5926802134513856
  batch 800 loss: 1.5868408703804016
  batch 850 loss: 1.550590260028839
  batch 900 loss: 1.634340407848358
LOSS train 1.63434 valid 2.46971, valid PER 64.00%
EPOCH 18:
  batch 50 loss: 2.0262415480613707
  batch 100 loss: 1.7934316897392273
  batch 150 loss: 1.6851857233047485
  batch 200 loss: 1.6699006295204162
  batch 250 loss: 1.6250878500938415
  batch 300 loss: 1.7943563222885133
  batch 350 loss: 2.0148272252082826
  batch 400 loss: 1.8724350285530091
  batch 450 loss: 1.7697249722480775
  batch 500 loss: 1.7298656439781188
  batch 550 loss: 1.7049414420127869
  batch 600 loss: 1.6916911172866822
  batch 650 loss: 1.6383426475524903
  batch 700 loss: 1.6672559022903441
  batch 750 loss: 1.8833880162239074
  batch 800 loss: 1.994998185634613
  batch 850 loss: 2.0489629244804384
  batch 900 loss: 2.0679502415657045
LOSS train 2.06795 valid 1.94997, valid PER 58.80%
EPOCH 19:
  batch 50 loss: 1.8566195011138915
  batch 100 loss: 1.834182322025299
  batch 150 loss: 1.7999161410331725
  batch 200 loss: 1.7950297713279724
  batch 250 loss: 1.786358139514923
  batch 300 loss: 1.7411628103256225
  batch 350 loss: 1.7002722525596619
  batch 400 loss: 1.7266309690475463
  batch 450 loss: 1.7561764240264892
  batch 500 loss: 1.7412860345840455
  batch 550 loss: 1.7211501049995421
  batch 600 loss: 1.6870785284042358
  batch 650 loss: 1.7499061465263366
  batch 700 loss: 1.651815333366394
  batch 750 loss: 1.6313772201538086
  batch 800 loss: 1.641053855419159
  batch 850 loss: 1.6490749025344849
  batch 900 loss: 1.7444333815574646
LOSS train 1.74443 valid 1.77879, valid PER 54.64%
EPOCH 20:
  batch 50 loss: 1.8214532899856568
  batch 100 loss: 1.9443498396873473
  batch 150 loss: 1.845782024860382
  batch 200 loss: 1.7168702960014344
  batch 250 loss: 1.8622266221046448
  batch 300 loss: 1.945999572277069
  batch 350 loss: 1.8430607771873475
  batch 400 loss: 2.1757729983329774
  batch 450 loss: 2.730422954559326
  batch 500 loss: 2.3357064843177797
  batch 550 loss: 2.3017936825752257
  batch 600 loss: 2.3095037484169008
  batch 650 loss: 2.131935229301453
  batch 700 loss: 2.100185251235962
  batch 750 loss: 1.958543996810913
  batch 800 loss: 1.9724847626686097
  batch 850 loss: 1.9678631138801574
  batch 900 loss: 2.0690956020355227
LOSS train 2.06910 valid 2.25822, valid PER 65.98%
train_loss
[1.9008122420310973, 1.5079028487205506, 1.345081729888916, 1.3857873260974884, 1.2993795764446259, 1.2446889901161193, 1.314392488002777, 1.3605163025856017, 1.2152014875411987, 1.3623595309257508, 1.5725650358200074, 1.4494823026657104, 1.3254782915115357, 1.8475441527366638, 1.6987072396278382, 1.5790904307365417, 1.634340407848358, 2.0679502415657045, 1.7444333815574646, 2.0690956020355227]
valid_loss
[1.8819494247436523, 1.4938682317733765, 1.4075266122817993, 1.3419314622879028, 1.3884435892105103, 1.253342628479004, 1.284399390220642, 1.3815441131591797, 1.3160134553909302, 1.3794747591018677, 1.514398455619812, 1.413901925086975, 1.4290281534194946, 1.9609628915786743, 1.8432775735855103, 1.607280969619751, 2.4697113037109375, 1.9499675035476685, 1.7787895202636719, 2.25822377204895]
valid_per
[69.11078522863619, 46.447140381282495, 43.627516331155846, 42.301026529796026, 41.86108518864152, 39.92800959872017, 40.52792960938542, 42.4476736435142, 39.58138914811358, 43.727502999600055, 45.10731902413011, 42.81429142780963, 42.7542994267431, 58.01226503132916, 53.39288094920678, 48.0869217437675, 64.00479936008531, 58.79882682309025, 54.63938141581123, 65.97786961738434]
Training finished in 2.0 minutes.
Model saved to checkpoints/20231208_151246/model_6
Loading model from checkpoints/20231208_151246/model_6
SUB: 22.04%, DEL: 16.82%, INS: 1.54%, COR: 61.14%, PER: 40.40%
