Namespace(seed=123, train_json='/rds/user/yo279/hpc-work/MLMI2/fbanks/train_fbank.json', val_json='/rds/user/yo279/hpc-work/MLMI2/fbanks/dev_fbank.json', test_json='/rds/user/yo279/hpc-work/MLMI2/fbanks/test_fbank.json', batch_size=4, num_layers=1, fbank_dims=23, model_dims=128, concat=1, lr=0.5, vocab='vocab_39.txt', report_interval=50, num_epochs=20, dropout_rate=0.5, clip_max_norm=1, num_ff_layers=2, scheduler_factor=0.5)
Total number of model parameters is 166952
EPOCH 1:
  batch 50 loss: 4.078929219245911
  batch 100 loss: 3.157144775390625
  batch 150 loss: 3.025901379585266
  batch 200 loss: 2.907838249206543
  batch 250 loss: 2.832769021987915
  batch 300 loss: 2.6986818218231203
  batch 350 loss: 2.5217490577697754
  batch 400 loss: 2.4407590103149412
  batch 450 loss: 2.391272535324097
  batch 500 loss: 2.2115665221214296
  batch 550 loss: 2.12748744726181
  batch 600 loss: 2.0783630990982056
  batch 650 loss: 1.9619475841522216
  batch 700 loss: 1.980461082458496
  batch 750 loss: 1.9020048761367798
  batch 800 loss: 1.8783379697799683
  batch 850 loss: 1.8071829247474671
  batch 900 loss: 1.7989005827903748
LOSS train 1.79890 valid 1.80147, valid PER 68.11%
EPOCH 2:
  batch 50 loss: 1.7458551168441772
  batch 100 loss: 1.697413067817688
  batch 150 loss: 1.6531487536430358
  batch 200 loss: 1.6675652694702148
  batch 250 loss: 1.677320008277893
  batch 300 loss: 1.6238905811309814
  batch 350 loss: 1.5348606252670287
  batch 400 loss: 1.539039843082428
  batch 450 loss: 1.480068485736847
  batch 500 loss: 1.5234814143180848
  batch 550 loss: 1.5274570870399475
  batch 600 loss: 1.4504747605323791
  batch 650 loss: 1.4654192638397217
  batch 700 loss: 1.4510227680206298
  batch 750 loss: 1.4119198751449584
  batch 800 loss: 1.3670988774299622
  batch 850 loss: 1.3777621173858643
  batch 900 loss: 1.391870186328888
LOSS train 1.39187 valid 1.34672, valid PER 43.80%
EPOCH 3:
  batch 50 loss: 1.3275661945343018
  batch 100 loss: 1.3242039740085603
  batch 150 loss: 1.3099125504493714
  batch 200 loss: 1.275825514793396
  batch 250 loss: 1.294313406944275
  batch 300 loss: 1.2720276522636413
  batch 350 loss: 1.3075463914871215
  batch 400 loss: 1.2714587616920472
  batch 450 loss: 1.2442211186885834
  batch 500 loss: 1.2353890883922576
  batch 550 loss: 1.2620038509368896
  batch 600 loss: 1.2340868592262269
  batch 650 loss: 1.1960011160373687
  batch 700 loss: 1.2205280268192291
  batch 750 loss: 1.2710586416721343
  batch 800 loss: 1.1840945732593537
  batch 850 loss: 1.2400492024421692
  batch 900 loss: 1.1547935616970062
LOSS train 1.15479 valid 1.23597, valid PER 37.66%
EPOCH 4:
  batch 50 loss: 1.1492736041545868
  batch 100 loss: 1.1766965794563293
  batch 150 loss: 1.127716064453125
  batch 200 loss: 1.1684097731113434
  batch 250 loss: 1.1826868546009064
  batch 300 loss: 1.2032774317264556
  batch 350 loss: 1.1480454730987548
  batch 400 loss: 1.149882620573044
  batch 450 loss: 1.1290196013450622
  batch 500 loss: 1.1332080507278441
  batch 550 loss: 1.1462021470069885
  batch 600 loss: 1.1536340010166168
  batch 650 loss: 1.12498566031456
  batch 700 loss: 1.127325290441513
  batch 750 loss: 1.0982183802127838
  batch 800 loss: 1.0657652938365936
  batch 850 loss: 1.1009279823303222
  batch 900 loss: 1.1405097043514252
LOSS train 1.14051 valid 1.15170, valid PER 35.58%
EPOCH 5:
  batch 50 loss: 1.0589402437210083
  batch 100 loss: 1.068421175479889
  batch 150 loss: 1.1180320274829865
  batch 200 loss: 1.0307368731498718
  batch 250 loss: 1.060013245344162
  batch 300 loss: 1.0682934272289275
  batch 350 loss: 1.0682050609588623
  batch 400 loss: 1.0892024636268616
  batch 450 loss: 1.0955975484848022
  batch 500 loss: 1.0692516219615937
  batch 550 loss: 1.039537366628647
  batch 600 loss: 1.0961449253559112
  batch 650 loss: 1.050006914138794
  batch 700 loss: 1.0819550454616547
  batch 750 loss: 1.0158819234371186
  batch 800 loss: 1.0696913886070252
  batch 850 loss: 1.060155802965164
  batch 900 loss: 1.0657523941993714
LOSS train 1.06575 valid 1.08455, valid PER 34.29%
EPOCH 6:
  batch 50 loss: 1.0447572982311248
  batch 100 loss: 1.015309246778488
  batch 150 loss: 0.9803458499908447
  batch 200 loss: 1.0110278964042663
  batch 250 loss: 1.028982093334198
  batch 300 loss: 0.9941746866703034
  batch 350 loss: 1.0118814492225647
  batch 400 loss: 0.9883194279670715
  batch 450 loss: 1.0294818305969238
  batch 500 loss: 1.0110280859470366
  batch 550 loss: 1.037051329612732
  batch 600 loss: 0.9831456792354584
  batch 650 loss: 0.9985821759700775
  batch 700 loss: 1.0132709050178528
  batch 750 loss: 1.003455982208252
  batch 800 loss: 0.992248741388321
  batch 850 loss: 0.9720343697071075
  batch 900 loss: 1.0061558592319488
LOSS train 1.00616 valid 1.06397, valid PER 33.14%
EPOCH 7:
  batch 50 loss: 0.9746804559230804
  batch 100 loss: 0.9882596480846405
  batch 150 loss: 0.9747758543491364
  batch 200 loss: 0.9600780737400055
  batch 250 loss: 0.9453979825973511
  batch 300 loss: 0.961773077249527
  batch 350 loss: 0.9550282347202301
  batch 400 loss: 0.9594713985919953
  batch 450 loss: 0.9654904592037201
  batch 500 loss: 0.937330595254898
  batch 550 loss: 0.9436276543140412
  batch 600 loss: 0.9364455735683441
  batch 650 loss: 0.9653472483158112
  batch 700 loss: 0.9726564192771912
  batch 750 loss: 0.9593629992008209
  batch 800 loss: 0.9483962583541871
  batch 850 loss: 0.9714107728004455
  batch 900 loss: 0.9976409995555877
LOSS train 0.99764 valid 1.03612, valid PER 32.45%
EPOCH 8:
  batch 50 loss: 0.9279981136322022
  batch 100 loss: 0.9073311805725097
  batch 150 loss: 0.9066261494159699
  batch 200 loss: 0.894155660867691
  batch 250 loss: 0.9465505564212799
  batch 300 loss: 0.949079020023346
  batch 350 loss: 1.0144345366954803
  batch 400 loss: 0.9482305979728699
  batch 450 loss: 0.9617069602012634
  batch 500 loss: 0.9755543947219849
  batch 550 loss: 0.9131688559055329
  batch 600 loss: 0.9483206677436828
  batch 650 loss: 0.9610672962665557
  batch 700 loss: 0.9277428150177002
  batch 750 loss: 0.9627190136909485
  batch 800 loss: 0.948298876285553
  batch 850 loss: 0.9249961864948273
  batch 900 loss: 0.9126337039470672
LOSS train 0.91263 valid 1.01101, valid PER 30.91%
EPOCH 9:
  batch 50 loss: 0.8438879442214966
  batch 100 loss: 0.879105395078659
  batch 150 loss: 0.8940805602073669
  batch 200 loss: 0.8637819945812225
  batch 250 loss: 0.8858269536495209
  batch 300 loss: 0.8861399638652802
  batch 350 loss: 0.9154610168933869
  batch 400 loss: 0.8971317315101623
  batch 450 loss: 0.9094436073303223
  batch 500 loss: 0.8657076072692871
  batch 550 loss: 0.9267637121677399
  batch 600 loss: 0.9407881569862365
  batch 650 loss: 0.9043142592906952
  batch 700 loss: 0.8843434524536132
  batch 750 loss: 0.8763562035560608
  batch 800 loss: 0.9262622559070587
  batch 850 loss: 0.9117685663700104
  batch 900 loss: 0.8658843159675598
LOSS train 0.86588 valid 0.99709, valid PER 30.94%
EPOCH 10:
  batch 50 loss: 0.8273989927768707
  batch 100 loss: 0.8813249564170837
  batch 150 loss: 0.8978871130943298
  batch 200 loss: 0.8993619835376739
  batch 250 loss: 0.8955040514469147
  batch 300 loss: 0.8547551357746124
  batch 350 loss: 0.8708830094337463
  batch 400 loss: 0.818302675485611
  batch 450 loss: 0.8543628871440887
  batch 500 loss: 0.8673167049884796
  batch 550 loss: 0.8889189851284027
  batch 600 loss: 0.8592464935779571
  batch 650 loss: 0.8539850783348083
  batch 700 loss: 0.8576034903526306
  batch 750 loss: 0.8551397597789765
  batch 800 loss: 0.8899565482139588
  batch 850 loss: 0.8969600558280945
  batch 900 loss: 0.8770409619808197
LOSS train 0.87704 valid 0.99985, valid PER 31.99%
EPOCH 11:
  batch 50 loss: 0.816674634218216
  batch 100 loss: 0.7855248916149139
  batch 150 loss: 0.7984564769268035
  batch 200 loss: 0.8428206253051758
  batch 250 loss: 0.8235152524709701
  batch 300 loss: 0.7976950657367706
  batch 350 loss: 0.8305842483043671
  batch 400 loss: 0.843862054347992
  batch 450 loss: 0.8345550632476807
  batch 500 loss: 0.8150662922859192
  batch 550 loss: 0.8197493720054626
  batch 600 loss: 0.8183213526010513
  batch 650 loss: 0.8835643935203552
  batch 700 loss: 0.8015513110160828
  batch 750 loss: 0.8460678267478943
  batch 800 loss: 0.8678690934181214
  batch 850 loss: 0.8849284672737121
  batch 900 loss: 0.8529199707508087
LOSS train 0.85292 valid 0.97166, valid PER 30.71%
EPOCH 12:
  batch 50 loss: 0.8041568267345428
  batch 100 loss: 0.777171710729599
  batch 150 loss: 0.8005332827568055
  batch 200 loss: 0.8210641640424728
  batch 250 loss: 0.8286351156234741
  batch 300 loss: 0.820507423877716
  batch 350 loss: 0.8086235225200653
  batch 400 loss: 0.8360014975070953
  batch 450 loss: 0.8336611199378967
  batch 500 loss: 0.8474570035934448
  batch 550 loss: 0.7777272236347198
  batch 600 loss: 0.7971171259880065
  batch 650 loss: 0.8420866358280182
  batch 700 loss: 0.8365277302265167
  batch 750 loss: 0.8486984634399414
  batch 800 loss: 0.8555812132358551
  batch 850 loss: 0.859208836555481
  batch 900 loss: 0.8914466845989227
LOSS train 0.89145 valid 0.96467, valid PER 29.64%
EPOCH 13:
  batch 50 loss: 0.8188680267333984
  batch 100 loss: 0.8070326256752014
  batch 150 loss: 0.7676208174228668
  batch 200 loss: 0.7926139485836029
  batch 250 loss: 0.7815644866228104
  batch 300 loss: 0.7744951844215393
  batch 350 loss: 0.7955173206329346
  batch 400 loss: 0.7773387718200684
  batch 450 loss: 0.8127272200584411
  batch 500 loss: 0.7699489200115204
  batch 550 loss: 0.8019241970777512
  batch 600 loss: 0.8039387166500092
  batch 650 loss: 0.7959003031253815
  batch 700 loss: 0.7862800776958465
  batch 750 loss: 0.780527925491333
  batch 800 loss: 0.7846920657157898
  batch 850 loss: 0.829977879524231
  batch 900 loss: 0.8381051361560822
LOSS train 0.83811 valid 0.98088, valid PER 29.84%
EPOCH 14:
  batch 50 loss: 0.7660888350009918
  batch 100 loss: 0.7650166368484497
  batch 150 loss: 0.7695787346363068
  batch 200 loss: 0.769227945804596
  batch 250 loss: 0.7954491668939591
  batch 300 loss: 0.8012283742427826
  batch 350 loss: 0.7590070444345475
  batch 400 loss: 0.7533805125951767
  batch 450 loss: 0.7504399180412292
  batch 500 loss: 0.7830592274665833
  batch 550 loss: 0.7819380676746368
  batch 600 loss: 0.7368188458681106
  batch 650 loss: 0.792468289732933
  batch 700 loss: 0.80403240442276
  batch 750 loss: 0.7712599325180054
  batch 800 loss: 0.7441118562221527
  batch 850 loss: 0.7991127634048462
  batch 900 loss: 0.7669718754291535
LOSS train 0.76697 valid 0.94020, valid PER 29.44%
EPOCH 15:
  batch 50 loss: 0.7317336177825928
  batch 100 loss: 0.7027706116437912
  batch 150 loss: 0.7386888337135314
  batch 200 loss: 0.75402503490448
  batch 250 loss: 0.7922883903980256
  batch 300 loss: 0.7308969825506211
  batch 350 loss: 0.7504491925239563
  batch 400 loss: 0.7399387538433075
  batch 450 loss: 0.7580038940906525
  batch 500 loss: 0.7307238686084747
  batch 550 loss: 0.7577216494083404
  batch 600 loss: 0.7717145049571991
  batch 650 loss: 0.7954676556587219
  batch 700 loss: 0.771139674782753
  batch 750 loss: 0.7815840685367584
  batch 800 loss: 0.7755245113372803
  batch 850 loss: 0.7709808027744294
  batch 900 loss: 0.7814586448669434
LOSS train 0.78146 valid 0.96625, valid PER 30.03%
EPOCH 16:
  batch 50 loss: 0.758081909418106
  batch 100 loss: 0.7262778830528259
  batch 150 loss: 0.7210010468959809
  batch 200 loss: 0.7210873985290527
  batch 250 loss: 0.742905719280243
  batch 300 loss: 0.7353445035219193
  batch 350 loss: 0.7770779228210449
  batch 400 loss: 0.7750895112752915
  batch 450 loss: 0.7792568862438202
  batch 500 loss: 0.7174116432666778
  batch 550 loss: 0.7468971514701843
  batch 600 loss: 0.7442958295345307
  batch 650 loss: 0.7528036975860596
  batch 700 loss: 0.7254058116674423
  batch 750 loss: 0.7648375380039215
  batch 800 loss: 0.761638589501381
  batch 850 loss: 0.7569600403308868
  batch 900 loss: 0.7561768662929534
LOSS train 0.75618 valid 0.97564, valid PER 29.43%
EPOCH 17:
  batch 50 loss: 0.7314452904462815
  batch 100 loss: 0.7153555428981782
  batch 150 loss: 0.7284763216972351
  batch 200 loss: 0.7073418700695038
  batch 250 loss: 0.7317825299501419
  batch 300 loss: 0.73360635638237
  batch 350 loss: 0.7114303255081177
  batch 400 loss: 0.7778417474031448
  batch 450 loss: 0.7605516475439071
  batch 500 loss: 0.7231922698020935
  batch 550 loss: 0.7536888760328293
  batch 600 loss: 0.7790238785743714
  batch 650 loss: 0.7446231770515442
  batch 700 loss: 0.7202950179576874
  batch 750 loss: 0.7002807050943375
  batch 800 loss: 0.7194825118780136
  batch 850 loss: 0.7507002609968185
  batch 900 loss: 0.7401391583681106
LOSS train 0.74014 valid 0.94858, valid PER 28.67%
EPOCH 18:
  batch 50 loss: 0.7049613606929779
  batch 100 loss: 0.7188313710689545
  batch 150 loss: 0.7165758621692657
  batch 200 loss: 0.7015663081407547
  batch 250 loss: 0.7147995293140411
  batch 300 loss: 0.6804542505741119
  batch 350 loss: 0.7189144992828369
  batch 400 loss: 0.6973328500986099
  batch 450 loss: 0.7315593445301056
  batch 500 loss: 0.7390459132194519
  batch 550 loss: 0.7362671327590943
  batch 600 loss: 0.7211324089765548
  batch 650 loss: 0.7291485977172851
  batch 700 loss: 0.7394904470443726
  batch 750 loss: 0.7177118891477585
  batch 800 loss: 0.6991798621416092
  batch 850 loss: 0.7120025134086609
  batch 900 loss: 0.7168586003780365
LOSS train 0.71686 valid 0.93526, valid PER 28.80%
EPOCH 19:
  batch 50 loss: 0.6360996985435485
  batch 100 loss: 0.654478490948677
  batch 150 loss: 0.6866654908657074
  batch 200 loss: 0.6973339879512787
  batch 250 loss: 0.7293050372600556
  batch 300 loss: 0.7113210427761077
  batch 350 loss: 0.7082916474342347
  batch 400 loss: 0.7169472455978394
  batch 450 loss: 0.7184179562330246
  batch 500 loss: 0.7411908292770386
  batch 550 loss: 0.7107131350040435
  batch 600 loss: 0.71328422665596
  batch 650 loss: 0.7872189474105835
  batch 700 loss: 0.70322882771492
  batch 750 loss: 0.6752989500761032
  batch 800 loss: 0.717728179693222
  batch 850 loss: 0.7305190110206604
  batch 900 loss: 0.7138738036155701
LOSS train 0.71387 valid 0.94611, valid PER 28.68%
EPOCH 20:
  batch 50 loss: 0.66059805393219
  batch 100 loss: 0.6536639267206192
  batch 150 loss: 0.6641715544462204
  batch 200 loss: 0.6687109673023224
  batch 250 loss: 0.6847369390726089
  batch 300 loss: 0.6929192334413529
  batch 350 loss: 0.6658608984947204
  batch 400 loss: 0.7058401381969452
  batch 450 loss: 0.6968046128749847
  batch 500 loss: 0.6798317062854767
  batch 550 loss: 0.7810628366470337
  batch 600 loss: 0.703305692076683
  batch 650 loss: 0.7359535443782806
  batch 700 loss: 0.7431485736370087
  batch 750 loss: 0.7052798044681549
  batch 800 loss: 0.727121331691742
  batch 850 loss: 0.7222863495349884
  batch 900 loss: 0.7112574923038483
LOSS train 0.71126 valid 0.92856, valid PER 28.21%
train_loss
[1.7989005827903748, 1.391870186328888, 1.1547935616970062, 1.1405097043514252, 1.0657523941993714, 1.0061558592319488, 0.9976409995555877, 0.9126337039470672, 0.8658843159675598, 0.8770409619808197, 0.8529199707508087, 0.8914466845989227, 0.8381051361560822, 0.7669718754291535, 0.7814586448669434, 0.7561768662929534, 0.7401391583681106, 0.7168586003780365, 0.7138738036155701, 0.7112574923038483]
valid_loss
[1.80147385597229, 1.346718430519104, 1.235965371131897, 1.1517006158828735, 1.0845458507537842, 1.0639739036560059, 1.0361216068267822, 1.0110087394714355, 0.9970901012420654, 0.9998461604118347, 0.9716641306877136, 0.9646703600883484, 0.9808785915374756, 0.9402016401290894, 0.9662501811981201, 0.9756376147270203, 0.9485818147659302, 0.9352577924728394, 0.9461063146591187, 0.9285552501678467]
valid_per
[68.1109185441941, 43.800826556459135, 37.661645113984804, 35.581922410345285, 34.28876149846687, 33.142247700306626, 32.44900679909345, 30.909212105052656, 30.942540994534063, 31.989068124250096, 30.709238768164244, 29.636048526863085, 29.8360218637515, 29.44274096787095, 30.029329422743633, 29.42940941207839, 28.669510731902413, 28.79616051193174, 28.682842287694974, 28.209572057059056]
Training finished in 3.0 minutes.
Model saved to checkpoints/20231208_153601/model_20
Loading model from checkpoints/20231208_153601/model_20
SUB: 17.16%, DEL: 10.32%, INS: 2.75%, COR: 72.53%, PER: 30.22%
