Namespace(seed=123, train_json='/rds/user/yo279/hpc-work/MLMI2/fbanks/train_fbank.json', val_json='/rds/user/yo279/hpc-work/MLMI2/fbanks/dev_fbank.json', test_json='/rds/user/yo279/hpc-work/MLMI2/fbanks/test_fbank.json', batch_size=4, num_layers=2, fbank_dims=23, model_dims=128, concat=1, lr=0.5, vocab='vocab_39.txt', report_interval=50, num_epochs=20, dropout_rate=0.5, clip_max_norm=1, num_ff_layers=2)
Total number of model parameters is 563856
EPOCH 1:
  batch 50 loss: 4.430407400131226
  batch 100 loss: 3.4392370653152464
  batch 150 loss: 3.422000946998596
  batch 200 loss: 3.391784963607788
  batch 250 loss: 3.3730833292007447
  batch 300 loss: 3.3464874982833863
  batch 350 loss: 3.273343744277954
  batch 400 loss: 3.1133304262161254
  batch 450 loss: 3.00471715927124
  batch 500 loss: 2.890685739517212
  batch 550 loss: 2.772868070602417
  batch 600 loss: 2.696077527999878
  batch 650 loss: 2.597487621307373
  batch 700 loss: 2.5520344734191895
  batch 750 loss: 2.4733823680877687
  batch 800 loss: 2.363982501029968
  batch 850 loss: 2.322389869689941
  batch 900 loss: 2.247820191383362
LOSS train 2.24782 valid 2.23335, valid PER 78.53%
EPOCH 2:
  batch 50 loss: 2.1756612920761107
  batch 100 loss: 2.078116934299469
  batch 150 loss: 2.0939081835746767
  batch 200 loss: 1.9997429156303406
  batch 250 loss: 2.001125054359436
  batch 300 loss: 1.8754452514648436
  batch 350 loss: 1.815127682685852
  batch 400 loss: 1.7835314202308654
  batch 450 loss: 1.7325068378448487
  batch 500 loss: 1.7194961905479431
  batch 550 loss: 1.728786540031433
  batch 600 loss: 1.639564266204834
  batch 650 loss: 1.6446682357788085
  batch 700 loss: 1.6402165961265565
  batch 750 loss: 1.5665270042419435
  batch 800 loss: 1.4640318870544433
  batch 850 loss: 1.4837522029876709
  batch 900 loss: 1.495078055858612
LOSS train 1.49508 valid 1.42687, valid PER 43.13%
EPOCH 3:
  batch 50 loss: 1.4239938855171204
  batch 100 loss: 1.3803885293006897
  batch 150 loss: 1.3691792583465576
  batch 200 loss: 1.3415642714500426
  batch 250 loss: 1.3101339197158814
  batch 300 loss: 1.3073340344429016
  batch 350 loss: 1.373681845664978
  batch 400 loss: 1.3088374042510986
  batch 450 loss: 1.2756514251232147
  batch 500 loss: 1.235936051607132
  batch 550 loss: 1.259465571641922
  batch 600 loss: 1.2317816054821014
  batch 650 loss: 1.1889391815662385
  batch 700 loss: 1.213827030658722
  batch 750 loss: 1.2722129178047181
  batch 800 loss: 1.2003233444690704
  batch 850 loss: 1.235587615966797
  batch 900 loss: 1.1393426287174224
LOSS train 1.13934 valid 1.19149, valid PER 36.21%
EPOCH 4:
  batch 50 loss: 1.1146412479877472
  batch 100 loss: 1.17389732837677
  batch 150 loss: 1.0977536380290984
  batch 200 loss: 1.1290145933628082
  batch 250 loss: 1.150478185415268
  batch 300 loss: 1.150700442790985
  batch 350 loss: 1.0581493020057677
  batch 400 loss: 1.1367199409008026
  batch 450 loss: 1.0932329630851745
  batch 500 loss: 1.073478479385376
  batch 550 loss: 1.0943524420261384
  batch 600 loss: 1.1033021020889282
  batch 650 loss: 1.1011845004558563
  batch 700 loss: 1.0613044929504394
  batch 750 loss: 1.0674213838577271
  batch 800 loss: 1.0249609375
  batch 850 loss: 1.0301455163955688
  batch 900 loss: 1.1679994237422944
LOSS train 1.16800 valid 1.06409, valid PER 32.89%
EPOCH 5:
  batch 50 loss: 1.0372042858600616
  batch 100 loss: 1.0124544262886048
  batch 150 loss: 1.0662568187713624
  batch 200 loss: 1.0010024344921111
  batch 250 loss: 1.0026755452156066
  batch 300 loss: 1.0162626314163208
  batch 350 loss: 0.995023934841156
  batch 400 loss: 1.0178149080276488
  batch 450 loss: 0.9727737963199615
  batch 500 loss: 1.0040609312057496
  batch 550 loss: 0.9889109504222869
  batch 600 loss: 1.0413033735752106
  batch 650 loss: 1.0027253806591034
  batch 700 loss: 1.04711230635643
  batch 750 loss: 0.9581628608703613
  batch 800 loss: 1.0250376212596892
  batch 850 loss: 1.0191865038871766
  batch 900 loss: 0.9800182712078095
LOSS train 0.98002 valid 1.00751, valid PER 30.63%
EPOCH 6:
  batch 50 loss: 0.9839934468269348
  batch 100 loss: 0.9698525750637055
  batch 150 loss: 0.9233908009529114
  batch 200 loss: 0.9326304352283478
  batch 250 loss: 0.9695054602622986
  batch 300 loss: 0.9688732087612152
  batch 350 loss: 0.9604107797145843
  batch 400 loss: 0.9243469202518463
  batch 450 loss: 0.9674389600753784
  batch 500 loss: 0.9182475650310516
  batch 550 loss: 0.9695464706420899
  batch 600 loss: 0.9057792258262635
  batch 650 loss: 0.9305525732040405
  batch 700 loss: 0.9142017519474029
  batch 750 loss: 0.898051666021347
  batch 800 loss: 0.9090199172496796
  batch 850 loss: 0.8930975735187531
  batch 900 loss: 0.9513029658794403
LOSS train 0.95130 valid 0.99964, valid PER 30.56%
EPOCH 7:
  batch 50 loss: 0.9113300156593322
  batch 100 loss: 0.9062315118312836
  batch 150 loss: 0.8732372462749481
  batch 200 loss: 0.8735290408134461
  batch 250 loss: 0.8740162968635559
  batch 300 loss: 0.8804373025894165
  batch 350 loss: 0.8827410793304443
  batch 400 loss: 0.8776369869709015
  batch 450 loss: 0.8581619381904602
  batch 500 loss: 0.9191812610626221
  batch 550 loss: 0.8918661093711853
  batch 600 loss: 0.8898566997051239
  batch 650 loss: 0.8483839821815491
  batch 700 loss: 0.8872787249088288
  batch 750 loss: 0.8475174117088318
  batch 800 loss: 0.8467005741596222
  batch 850 loss: 0.8784886074066162
  batch 900 loss: 0.9006990623474121
LOSS train 0.90070 valid 0.93844, valid PER 29.08%
EPOCH 8:
  batch 50 loss: 0.8072607827186584
  batch 100 loss: 0.8176546156406402
  batch 150 loss: 0.8142525982856751
  batch 200 loss: 0.8012608003616333
  batch 250 loss: 0.8565797799825668
  batch 300 loss: 0.7841749608516693
  batch 350 loss: 0.849970326423645
  batch 400 loss: 0.8032890820503235
  batch 450 loss: 0.8510353899002076
  batch 500 loss: 0.8570487153530121
  batch 550 loss: 0.7797112488746643
  batch 600 loss: 0.8457079315185547
  batch 650 loss: 0.871351706981659
  batch 700 loss: 0.810176545381546
  batch 750 loss: 0.8234434080123901
  batch 800 loss: 0.8257923412322998
  batch 850 loss: 0.7939390003681183
  batch 900 loss: 0.8319550716876983
LOSS train 0.83196 valid 0.91957, valid PER 28.09%
EPOCH 9:
  batch 50 loss: 0.7409235990047455
  batch 100 loss: 0.7927192032337189
  batch 150 loss: 0.7765109384059906
  batch 200 loss: 0.7389795380830765
  batch 250 loss: 0.7819079303741455
  batch 300 loss: 0.7794953072071076
  batch 350 loss: 0.8091747426986694
  batch 400 loss: 0.794331648349762
  batch 450 loss: 0.7724239885807037
  batch 500 loss: 0.7477371019124984
  batch 550 loss: 0.7650580072402954
  batch 600 loss: 0.8319724321365356
  batch 650 loss: 0.7975041592121124
  batch 700 loss: 0.7724503958225251
  batch 750 loss: 0.7453057390451431
  batch 800 loss: 0.7877257561683655
  batch 850 loss: 0.8212005043029785
  batch 900 loss: 0.7684554815292358
LOSS train 0.76846 valid 0.89704, valid PER 27.24%
EPOCH 10:
  batch 50 loss: 0.7141843235492706
  batch 100 loss: 0.7187089991569519
  batch 150 loss: 0.7406772446632385
  batch 200 loss: 0.756602571606636
  batch 250 loss: 0.7946439075469971
  batch 300 loss: 0.7172986841201783
  batch 350 loss: 0.7524253273010254
  batch 400 loss: 0.714292705655098
  batch 450 loss: 0.7518263173103332
  batch 500 loss: 0.7993981873989106
  batch 550 loss: 0.7906017172336578
  batch 600 loss: 0.7634370976686478
  batch 650 loss: 0.7479924249649048
  batch 700 loss: 0.7530721569061279
  batch 750 loss: 0.7608033633232116
  batch 800 loss: 0.7555685156583786
  batch 850 loss: 0.7766307985782623
  batch 900 loss: 0.754235816001892
LOSS train 0.75424 valid 0.93685, valid PER 28.76%
EPOCH 11:
  batch 50 loss: 0.6900870275497436
  batch 100 loss: 0.667881326675415
  batch 150 loss: 0.6774370354413987
  batch 200 loss: 0.7395385754108429
  batch 250 loss: 0.7110611343383789
  batch 300 loss: 0.6694001531600953
  batch 350 loss: 0.7135452532768249
  batch 400 loss: 0.7247573238611221
  batch 450 loss: 0.7362987095117569
  batch 500 loss: 0.717724456191063
  batch 550 loss: 0.7282785379886627
  batch 600 loss: 0.6970039421319961
  batch 650 loss: 0.7653027206659317
  batch 700 loss: 0.6898791092634201
  batch 750 loss: 0.7193659549951553
  batch 800 loss: 0.7438814604282379
  batch 850 loss: 0.7610983335971833
  batch 900 loss: 0.7314693391323089
LOSS train 0.73147 valid 0.91547, valid PER 28.00%
EPOCH 12:
  batch 50 loss: 0.6718872529268265
  batch 100 loss: 0.6967714393138885
  batch 150 loss: 0.6708211886882782
  batch 200 loss: 0.7085828030109406
  batch 250 loss: 0.7076135778427124
  batch 300 loss: 0.6896977353096009
  batch 350 loss: 0.6914249259233475
  batch 400 loss: 0.7063361686468125
  batch 450 loss: 0.7100294852256774
  batch 500 loss: 0.7265613949298859
  batch 550 loss: 0.6653911834955215
  batch 600 loss: 0.6907814776897431
  batch 650 loss: 0.7065628027915954
  batch 700 loss: 0.6909539896249771
  batch 750 loss: 0.688130499124527
  batch 800 loss: 0.6692261701822281
  batch 850 loss: 0.719153859615326
  batch 900 loss: 0.7200730335712433
LOSS train 0.72007 valid 0.83098, valid PER 25.56%
EPOCH 13:
  batch 50 loss: 0.6301530909538269
  batch 100 loss: 0.6864742797613144
  batch 150 loss: 0.6660825800895691
  batch 200 loss: 0.6910761320590972
  batch 250 loss: 0.65367571413517
  batch 300 loss: 0.6477043735980987
  batch 350 loss: 0.6535164618492126
  batch 400 loss: 0.6683598625659942
  batch 450 loss: 0.6847651088237763
  batch 500 loss: 0.637302314043045
  batch 550 loss: 0.6704198610782623
  batch 600 loss: 0.6614348787069321
  batch 650 loss: 0.6653714352846145
  batch 700 loss: 0.6802369838953018
  batch 750 loss: 0.6355397099256516
  batch 800 loss: 0.6558899468183518
  batch 850 loss: 0.713791669011116
  batch 900 loss: 0.675339109301567
LOSS train 0.67534 valid 0.87066, valid PER 25.94%
EPOCH 14:
  batch 50 loss: 0.6023306506872177
  batch 100 loss: 0.6113368892669677
  batch 150 loss: 0.6175638663768769
  batch 200 loss: 0.6263442689180374
  batch 250 loss: 0.6347859770059585
  batch 300 loss: 0.698374947309494
  batch 350 loss: 0.60923335313797
  batch 400 loss: 0.6223656982183456
  batch 450 loss: 0.6334756988286973
  batch 500 loss: 0.6177517938613891
  batch 550 loss: 0.6480715018510819
  batch 600 loss: 0.6546302008628845
  batch 650 loss: 0.7279415875673294
  batch 700 loss: 0.7194099652767182
  batch 750 loss: 0.67032790184021
  batch 800 loss: 0.6380347752571106
  batch 850 loss: 0.6864773988723755
  batch 900 loss: 0.6830073803663254
LOSS train 0.68301 valid 0.88453, valid PER 26.76%
EPOCH 15:
  batch 50 loss: 0.6396815848350524
  batch 100 loss: 0.6352519810199737
  batch 150 loss: 0.6499887657165527
  batch 200 loss: 0.6956852853298188
  batch 250 loss: 0.6550030028820037
  batch 300 loss: 0.6499672716856003
  batch 350 loss: 0.6292341375350952
  batch 400 loss: 0.6195178478956223
  batch 450 loss: 0.6189004129171372
  batch 500 loss: 0.6063853830099106
  batch 550 loss: 0.6289008277654647
  batch 600 loss: 0.6426770323514939
  batch 650 loss: 0.6291797560453415
  batch 700 loss: 0.6355594885349274
  batch 750 loss: 0.6481328630447387
  batch 800 loss: 0.6404614335298539
  batch 850 loss: 0.6097310197353363
  batch 900 loss: 0.6192336243391037
LOSS train 0.61923 valid 0.85253, valid PER 25.44%
EPOCH 16:
  batch 50 loss: 0.5741747426986694
  batch 100 loss: 0.5499446260929107
  batch 150 loss: 0.5590436327457428
  batch 200 loss: 0.5694778525829315
  batch 250 loss: 0.6009530276060104
  batch 300 loss: 0.605145793557167
  batch 350 loss: 0.6123870891332627
  batch 400 loss: 0.6054539722204209
  batch 450 loss: 0.5971050041913987
  batch 500 loss: 0.5942314839363099
  batch 550 loss: 0.6224917882680893
  batch 600 loss: 0.6011764889955521
  batch 650 loss: 0.6155143505334855
  batch 700 loss: 0.5878658324480057
  batch 750 loss: 0.6413758701086044
  batch 800 loss: 0.6118910700082779
  batch 850 loss: 0.5936041885614395
  batch 900 loss: 0.6109610986709595
LOSS train 0.61096 valid 0.84119, valid PER 24.70%
EPOCH 17:
  batch 50 loss: 0.5525289869308472
  batch 100 loss: 0.5492715919017792
  batch 150 loss: 0.5682598733901978
  batch 200 loss: 0.5583851248025894
  batch 250 loss: 0.5823446738719941
  batch 300 loss: 0.551103162765503
  batch 350 loss: 0.5392431485652923
  batch 400 loss: 0.5917572778463364
  batch 450 loss: 0.5950518071651458
  batch 500 loss: 0.5674786412715912
  batch 550 loss: 0.5844849175214768
  batch 600 loss: 0.607415246963501
  batch 650 loss: 0.5972829657793045
  batch 700 loss: 0.6017139798402786
  batch 750 loss: 0.580447010397911
  batch 800 loss: 0.5988580334186554
  batch 850 loss: 0.5859522718191147
  batch 900 loss: 0.5751226890087128
LOSS train 0.57512 valid 0.84755, valid PER 24.63%
EPOCH 18:
  batch 50 loss: 0.5201287096738816
  batch 100 loss: 0.5279032695293426
  batch 150 loss: 0.5728201931715011
  batch 200 loss: 0.5474150788784027
  batch 250 loss: 0.5421615540981293
  batch 300 loss: 0.5374074900150299
  batch 350 loss: 0.5591591191291809
  batch 400 loss: 0.5241068041324616
  batch 450 loss: 0.5378207272291183
  batch 500 loss: 0.5501689338684081
  batch 550 loss: 0.5603859406709671
  batch 600 loss: 0.5136195838451385
  batch 650 loss: 0.538311932682991
  batch 700 loss: 0.5637375569343567
  batch 750 loss: 0.5400596261024475
  batch 800 loss: 0.5684202116727829
  batch 850 loss: 0.5390818679332733
  batch 900 loss: 0.5881650125980378
LOSS train 0.58817 valid 0.87881, valid PER 26.12%
EPOCH 19:
  batch 50 loss: 0.5041010183095932
  batch 100 loss: 0.4918718361854553
  batch 150 loss: 0.5121258348226547
  batch 200 loss: 0.5303830367326736
  batch 250 loss: 0.5145609605312348
  batch 300 loss: 0.5222712230682373
  batch 350 loss: 0.5151973915100098
  batch 400 loss: 0.5189487659931182
  batch 450 loss: 0.5661957120895386
  batch 500 loss: 0.547061088681221
  batch 550 loss: 0.5091849607229233
  batch 600 loss: 0.5093199157714844
  batch 650 loss: 0.5514029240608216
  batch 700 loss: 0.5021959441900253
  batch 750 loss: 0.4827647095918655
  batch 800 loss: 0.5338141530752182
  batch 850 loss: 0.5125926226377487
  batch 900 loss: 0.518315253853798
LOSS train 0.51832 valid 0.85511, valid PER 24.58%
EPOCH 20:
  batch 50 loss: 0.44493465542793276
  batch 100 loss: 0.4946548718214035
  batch 150 loss: 0.4564828422665596
  batch 200 loss: 0.48783783733844754
  batch 250 loss: 0.4824486738443375
  batch 300 loss: 0.4937559813261032
  batch 350 loss: 0.4542931616306305
  batch 400 loss: 0.491419181227684
  batch 450 loss: 0.5180115211009979
  batch 500 loss: 0.5003959029912949
  batch 550 loss: 0.5962990587949752
  batch 600 loss: 0.5189563888311386
  batch 650 loss: 0.537186861038208
  batch 700 loss: 0.5698141241073609
  batch 750 loss: 0.5387507206201554
  batch 800 loss: 0.5870928996801377
  batch 850 loss: 0.5628118860721588
  batch 900 loss: 0.5559425407648086
LOSS train 0.55594 valid 0.88984, valid PER 25.26%
train_loss
[2.247820191383362, 1.495078055858612, 1.1393426287174224, 1.1679994237422944, 0.9800182712078095, 0.9513029658794403, 0.9006990623474121, 0.8319550716876983, 0.7684554815292358, 0.754235816001892, 0.7314693391323089, 0.7200730335712433, 0.675339109301567, 0.6830073803663254, 0.6192336243391037, 0.6109610986709595, 0.5751226890087128, 0.5881650125980378, 0.518315253853798, 0.5559425407648086]
valid_loss
[2.233346700668335, 1.426869511604309, 1.1914907693862915, 1.0640876293182373, 1.007513165473938, 0.9996398687362671, 0.9384427666664124, 0.9195706844329834, 0.8970353603363037, 0.9368470311164856, 0.9154710173606873, 0.8309792280197144, 0.870660126209259, 0.8845307230949402, 0.8525267839431763, 0.8411930799484253, 0.8475522398948669, 0.8788138628005981, 0.8551068902015686, 0.8898443579673767]
valid_per
[78.52952939608052, 43.12758298893481, 36.20850553259565, 32.88894814024796, 30.62924943340888, 30.555925876549793, 29.082788961471806, 28.08958805492601, 27.243034262098387, 28.75616584455406, 28.002932942274363, 25.556592454339423, 25.94320757232369, 26.75643247566991, 25.443274230102652, 24.696707105719238, 24.63004932675643, 26.116517797626983, 24.576723103586186, 25.2566324490068]
Training finished in 4.0 minutes.
Model saved to checkpoints/20231208_125702/model_12
Loading model from checkpoints/20231208_125702/model_12
SUB: 15.88%, DEL: 8.26%, INS: 2.39%, COR: 75.87%, PER: 26.53%
