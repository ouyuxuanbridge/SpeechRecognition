Namespace(seed=123, train_json='/rds/user/yo279/hpc-work/MLMI2/fbanks/train_fbank.json', val_json='/rds/user/yo279/hpc-work/MLMI2/fbanks/dev_fbank.json', test_json='/rds/user/yo279/hpc-work/MLMI2/fbanks/test_fbank.json', batch_size=4, num_layers=1, fbank_dims=23, model_dims=128, concat=1, lr=0.05, vocab='vocab_39.txt', report_interval=50, num_epochs=20, dropout_rate=0.5, clip_max_norm=1)
Total number of model parameters is 166952
EPOCH 1:
  batch 50 loss: 4.649625134468079
  batch 100 loss: 3.0172212076187135
  batch 150 loss: 2.596666040420532
  batch 200 loss: 2.384147353172302
  batch 250 loss: 2.279360656738281
  batch 300 loss: 2.192711386680603
  batch 350 loss: 2.062769124507904
  batch 400 loss: 2.08197203874588
  batch 450 loss: 2.0398207235336305
  batch 500 loss: 1.9879352235794068
  batch 550 loss: 1.9069839572906495
  batch 600 loss: 1.9080048370361329
  batch 650 loss: 1.8595889806747437
  batch 700 loss: 1.8941401886940001
  batch 750 loss: 1.9150364637374877
  batch 800 loss: 1.965848410129547
  batch 850 loss: 1.9052101397514343
  batch 900 loss: 1.9019257926940918
LOSS train 1.90193 valid 1.87466, valid PER 60.00%
EPOCH 2:
  batch 50 loss: 1.917912540435791
  batch 100 loss: 1.855014410018921
  batch 150 loss: 1.8426471066474914
  batch 200 loss: 2.0536283445358277
  batch 250 loss: 2.0741312646865846
  batch 300 loss: 1.96741774559021
  batch 350 loss: 1.8806192421913146
  batch 400 loss: 1.9703218197822572
  batch 450 loss: 1.8920753121376037
  batch 500 loss: 1.938187942504883
  batch 550 loss: 1.9103732848167418
  batch 600 loss: 1.8152262926101685
  batch 650 loss: 2.118825798034668
  batch 700 loss: 2.5826919078826904
  batch 750 loss: 2.3928744435310363
  batch 800 loss: 2.155685405731201
  batch 850 loss: 2.132943716049194
  batch 900 loss: 2.174738311767578
LOSS train 2.17474 valid 2.19030, valid PER 68.57%
EPOCH 3:
  batch 50 loss: 2.12411652803421
  batch 100 loss: 2.0696873235702515
  batch 150 loss: 2.086543776988983
  batch 200 loss: 2.1270657658576964
  batch 250 loss: 2.077721507549286
  batch 300 loss: 2.04518039226532
  batch 350 loss: 2.0569902300834655
  batch 400 loss: 2.067567410469055
  batch 450 loss: 2.0727064323425295
  batch 500 loss: 2.0789078402519228
  batch 550 loss: 2.027717869281769
  batch 600 loss: 2.0413932323455812
  batch 650 loss: 1.9786156058311462
  batch 700 loss: 2.048462681770325
  batch 750 loss: 2.066638078689575
  batch 800 loss: 2.023427095413208
  batch 850 loss: 2.0263877511024475
  batch 900 loss: 2.0418694829940796
LOSS train 2.04187 valid 2.11821, valid PER 61.91%
EPOCH 4:
  batch 50 loss: 2.102924008369446
  batch 100 loss: 2.0737664103507996
  batch 150 loss: 1.9986895608901978
  batch 200 loss: 2.0087905406951903
  batch 250 loss: 2.0090745282173157
  batch 300 loss: 2.0537060379981993
  batch 350 loss: 1.942225351333618
  batch 400 loss: 2.03185884475708
  batch 450 loss: 2.04444509267807
  batch 500 loss: 1.9531249523162841
  batch 550 loss: 1.9768597507476806
  batch 600 loss: 2.0408649134635923
  batch 650 loss: 1.9869615387916566
  batch 700 loss: 1.92402428150177
  batch 750 loss: 1.9193137693405151
  batch 800 loss: 1.8917499852180482
  batch 850 loss: 2.1513913750648497
  batch 900 loss: 2.147240936756134
LOSS train 2.14724 valid 2.11878, valid PER 62.28%
EPOCH 5:
  batch 50 loss: 2.03616117477417
  batch 100 loss: 2.0116679644584656
  batch 150 loss: 2.1004273533821105
  batch 200 loss: 2.1282795166969297
  batch 250 loss: 2.087981069087982
  batch 300 loss: 2.020196788311005
  batch 350 loss: 2.033676269054413
  batch 400 loss: 1.99615163564682
  batch 450 loss: 2.0284951305389405
  batch 500 loss: 2.004440927505493
  batch 550 loss: 1.9821464037895202
  batch 600 loss: 2.0490100312232973
  batch 650 loss: 2.009415690898895
  batch 700 loss: 2.0406572604179383
  batch 750 loss: 1.9608209562301635
  batch 800 loss: 1.9917537331581117
  batch 850 loss: 2.0336215329170226
  batch 900 loss: 2.00642049074173
LOSS train 2.00642 valid 2.09460, valid PER 58.88%
EPOCH 6:
  batch 50 loss: 2.016934688091278
  batch 100 loss: 1.95179368019104
  batch 150 loss: 1.916903417110443
  batch 200 loss: 1.9397600722312927
  batch 250 loss: 2.0313043904304506
  batch 300 loss: 1.949938063621521
  batch 350 loss: 1.995838725566864
  batch 400 loss: 1.9141820764541626
  batch 450 loss: 2.005882797241211
  batch 500 loss: 1.9322105693817138
  batch 550 loss: 2.029380781650543
  batch 600 loss: 1.9997579646110535
  batch 650 loss: 2.0529706358909605
  batch 700 loss: 1.9866893672943116
  batch 750 loss: 1.9621878695487975
  batch 800 loss: 1.9348493695259095
  batch 850 loss: 1.953450517654419
  batch 900 loss: 2.002874820232391
LOSS train 2.00287 valid 1.96298, valid PER 58.51%
EPOCH 7:
  batch 50 loss: 1.9584273624420165
  batch 100 loss: 1.9548659706115723
  batch 150 loss: 1.908556411266327
  batch 200 loss: 1.927072401046753
  batch 250 loss: 1.9394876170158386
  batch 300 loss: 1.9965054965019227
  batch 350 loss: 1.9626573157310485
  batch 400 loss: 1.9988849782943725
  batch 450 loss: 2.0055207943916322
  batch 500 loss: 2.0378489971160887
  batch 550 loss: 2.068591718673706
  batch 600 loss: 2.119629833698273
  batch 650 loss: 2.1187889170646668
  batch 700 loss: 2.222604112625122
  batch 750 loss: 2.1127519011497498
  batch 800 loss: 2.100692410469055
  batch 850 loss: 2.1358227014541624
  batch 900 loss: 2.1247610569000246
LOSS train 2.12476 valid 2.12840, valid PER 63.44%
EPOCH 8:
  batch 50 loss: 2.082463879585266
  batch 100 loss: 2.0768583536148073
  batch 150 loss: 2.070181429386139
  batch 200 loss: 2.0149337673187255
  batch 250 loss: 2.070036463737488
  batch 300 loss: 2.0075084018707274
  batch 350 loss: 2.0512402415275575
  batch 400 loss: 2.0596887612342836
  batch 450 loss: 2.0374992179870604
  batch 500 loss: 2.0300630402565
  batch 550 loss: 2.0358291912078856
  batch 600 loss: 2.084394385814667
  batch 650 loss: 2.0653626704216004
  batch 700 loss: 2.02415664434433
  batch 750 loss: 2.0792557191848755
  batch 800 loss: 2.019032075405121
  batch 850 loss: 2.0521346354484558
  batch 900 loss: 2.043101782798767
LOSS train 2.04310 valid 2.05724, valid PER 62.27%
EPOCH 9:
  batch 50 loss: 1.991539764404297
  batch 100 loss: 2.0542977547645567
  batch 150 loss: 2.078894453048706
  batch 200 loss: 1.985647406578064
  batch 250 loss: 2.0463812923431397
  batch 300 loss: 2.079631516933441
  batch 350 loss: 2.077430739402771
  batch 400 loss: 2.0217106676101686
  batch 450 loss: 2.0552289056777955
  batch 500 loss: 1.9690234279632568
  batch 550 loss: 2.009565396308899
  batch 600 loss: 2.0506062173843382
  batch 650 loss: 2.0421299743652344
  batch 700 loss: 2.082002110481262
  batch 750 loss: 2.0295145893096924
  batch 800 loss: 2.0635462737083436
  batch 850 loss: 2.0405131721496583
  batch 900 loss: 2.0075221037864686
LOSS train 2.00752 valid 2.02165, valid PER 59.11%
EPOCH 10:
  batch 50 loss: 1.9932728505134583
  batch 100 loss: 1.9772462892532348
  batch 150 loss: 2.0365691351890565
  batch 200 loss: 2.028946781158447
  batch 250 loss: 2.0187826585769653
  batch 300 loss: 1.9266181993484497
  batch 350 loss: 1.9985697436332703
  batch 400 loss: 2.053134000301361
  batch 450 loss: 1.9664305353164673
  batch 500 loss: 2.0371250224113466
  batch 550 loss: 2.0362334394454957
  batch 600 loss: 2.0363408088684083
  batch 650 loss: 1.9790369915962218
  batch 700 loss: 2.008807687759399
  batch 750 loss: 1.9397972059249877
  batch 800 loss: 1.9849965691566467
  batch 850 loss: 2.0286633944511414
  batch 900 loss: 2.0032868647575377
LOSS train 2.00329 valid 2.06021, valid PER 60.06%
EPOCH 11:
  batch 50 loss: 2.0439896106719972
  batch 100 loss: 2.023242735862732
  batch 150 loss: 1.9773609375953674
  batch 200 loss: 2.069463906288147
  batch 250 loss: 2.110502359867096
  batch 300 loss: 2.11747563123703
  batch 350 loss: 2.205713965892792
  batch 400 loss: 2.1711158967018127
  batch 450 loss: 2.1545027685165405
  batch 500 loss: 2.1172623467445373
  batch 550 loss: 2.0875656914711
  batch 600 loss: 2.0978128147125243
  batch 650 loss: 2.0938848423957825
  batch 700 loss: 2.0594939160346986
  batch 750 loss: 2.0485443902015685
  batch 800 loss: 2.021875545978546
  batch 850 loss: 2.1153873991966248
  batch 900 loss: 2.0882557797431947
LOSS train 2.08826 valid 2.10795, valid PER 62.90%
EPOCH 12:
  batch 50 loss: 2.0542534255981444
  batch 100 loss: 1.9852243757247925
  batch 150 loss: 2.0343208432197573
  batch 200 loss: 2.013286111354828
  batch 250 loss: 2.0842135381698608
  batch 300 loss: 1.9868090391159057
  batch 350 loss: 2.033784959316254
  batch 400 loss: 2.037046823501587
  batch 450 loss: 2.046543791294098
  batch 500 loss: 2.01571426153183
  batch 550 loss: 1.944389090538025
  batch 600 loss: 1.990778043270111
  batch 650 loss: 2.038256847858429
  batch 700 loss: 2.03369752407074
  batch 750 loss: 2.1558561730384826
  batch 800 loss: 2.1003755784034728
  batch 850 loss: 2.1215401315689086
  batch 900 loss: 2.0523867869377135
LOSS train 2.05239 valid 2.05307, valid PER 61.53%
EPOCH 13:
  batch 50 loss: 1.9932084608078002
  batch 100 loss: 2.028858025074005
  batch 150 loss: 2.0171352863311767
  batch 200 loss: 2.0482728123664855
  batch 250 loss: 2.092845857143402
  batch 300 loss: 2.036439435482025
  batch 350 loss: 2.038400990962982
  batch 400 loss: 2.0680175614356995
  batch 450 loss: 2.078624897003174
  batch 500 loss: 1.9945797896385193
  batch 550 loss: 1.998901252746582
  batch 600 loss: 2.040887041091919
  batch 650 loss: 1.9964963173866273
  batch 700 loss: 2.0341656470298766
  batch 750 loss: 2.018090705871582
  batch 800 loss: 2.037011756896973
  batch 850 loss: 2.10288423538208
  batch 900 loss: 2.180637083053589
LOSS train 2.18064 valid 2.25783, valid PER 65.67%
EPOCH 14:
  batch 50 loss: 2.12064227104187
  batch 100 loss: 2.139763355255127
  batch 150 loss: 2.084927146434784
  batch 200 loss: 2.140821270942688
  batch 250 loss: 2.0962646126747133
  batch 300 loss: 2.0488503313064577
  batch 350 loss: 2.020078413486481
  batch 400 loss: 2.0454934859275817
  batch 450 loss: 2.040769302845001
  batch 500 loss: 2.074125759601593
  batch 550 loss: 2.086870722770691
  batch 600 loss: 2.077339777946472
  batch 650 loss: 2.105599653720856
  batch 700 loss: 2.102384829521179
  batch 750 loss: 2.042154116630554
  batch 800 loss: 1.9908818888664246
  batch 850 loss: 2.0654474210739138
  batch 900 loss: 2.0171430349349975
LOSS train 2.01714 valid 2.11717, valid PER 61.28%
EPOCH 15:
  batch 50 loss: 2.1358535075187683
  batch 100 loss: 2.109215106964111
  batch 150 loss: 2.061696331501007
  batch 200 loss: 2.1635535526275635
  batch 250 loss: 2.0706386017799376
  batch 300 loss: 2.0773220348358152
  batch 350 loss: 2.0587506175041197
  batch 400 loss: 2.038346290588379
  batch 450 loss: 2.1194959807395937
  batch 500 loss: 2.0987967157363894
  batch 550 loss: 2.1369303965568545
  batch 600 loss: 2.1413545536994936
  batch 650 loss: 2.0961685514450075
  batch 700 loss: 2.073519790172577
  batch 750 loss: 2.070777027606964
  batch 800 loss: 2.052807021141052
  batch 850 loss: 2.0666480588912965
  batch 900 loss: 2.079545910358429
LOSS train 2.07955 valid 2.06919, valid PER 60.65%
EPOCH 16:
  batch 50 loss: 2.0354967522621155
  batch 100 loss: 1.9942767429351806
  batch 150 loss: 1.9854436993598938
  batch 200 loss: 2.0479307270050047
  batch 250 loss: 2.0612099099159242
  batch 300 loss: 2.069650785923004
  batch 350 loss: 2.074711935520172
  batch 400 loss: 2.053947455883026
  batch 450 loss: 2.037981028556824
  batch 500 loss: 1.9987820982933044
  batch 550 loss: 2.110188000202179
  batch 600 loss: 2.0716687750816347
  batch 650 loss: 2.0654311871528623
  batch 700 loss: 2.0546699476242067
  batch 750 loss: 2.062845940589905
  batch 800 loss: 2.0685668063163756
  batch 850 loss: 2.0492366194725036
  batch 900 loss: 1.9922025227546691
LOSS train 1.99220 valid 2.06368, valid PER 61.22%
EPOCH 17:
  batch 50 loss: 2.004220688343048
  batch 100 loss: 2.0157644534111023
  batch 150 loss: 1.9879677772521973
  batch 200 loss: 1.9898411536216736
  batch 250 loss: 2.015510857105255
  batch 300 loss: 2.011229815483093
  batch 350 loss: 1.9666900157928466
  batch 400 loss: 2.0603723073005678
  batch 450 loss: 2.1253565454483034
  batch 500 loss: 2.0947778248786926
  batch 550 loss: 2.0997983908653257
  batch 600 loss: 2.2040424060821535
  batch 650 loss: 2.1181770539283753
  batch 700 loss: 2.1054545140266416
  batch 750 loss: 2.1533701372146608
  batch 800 loss: 2.108488383293152
  batch 850 loss: 2.0579101777076723
  batch 900 loss: 2.0869039607048037
LOSS train 2.08690 valid 2.10207, valid PER 60.45%
EPOCH 18:
  batch 50 loss: 2.0828873085975648
  batch 100 loss: 2.1353479504585264
  batch 150 loss: 2.1898867297172546
  batch 200 loss: 2.2029341793060304
  batch 250 loss: 2.1870025897026064
  batch 300 loss: 2.167530653476715
  batch 350 loss: 2.1941198348999023
  batch 400 loss: 2.1604495739936826
  batch 450 loss: 2.185252435207367
  batch 500 loss: 2.179526767730713
  batch 550 loss: 2.1504092979431153
  batch 600 loss: 2.058502473831177
  batch 650 loss: 2.040857400894165
  batch 700 loss: 2.145273280143738
  batch 750 loss: 2.016431062221527
  batch 800 loss: 2.0543592500686647
  batch 850 loss: 2.0179980850219725
  batch 900 loss: 2.0958490133285523
LOSS train 2.09585 valid 2.07357, valid PER 61.41%
EPOCH 19:
  batch 50 loss: 1.976888427734375
  batch 100 loss: 2.0248098754882813
  batch 150 loss: 2.0585505032539366
  batch 200 loss: 2.128620655536652
  batch 250 loss: 2.0695037484169005
  batch 300 loss: 2.0934009385108947
  batch 350 loss: 2.0353677105903625
  batch 400 loss: 2.0922222113609314
  batch 450 loss: 2.0771234107017515
  batch 500 loss: 2.1149910163879393
  batch 550 loss: 2.0865063071250916
  batch 600 loss: 2.082187035083771
  batch 650 loss: 2.1250232100486754
  batch 700 loss: 2.1055335450172423
  batch 750 loss: 2.0418543100357054
  batch 800 loss: 2.121656482219696
  batch 850 loss: 2.1294393801689147
  batch 900 loss: 2.111262254714966
LOSS train 2.11126 valid 2.18263, valid PER 58.47%
EPOCH 20:
  batch 50 loss: 2.0951434898376466
  batch 100 loss: 2.1398604345321655
  batch 150 loss: 2.140739188194275
  batch 200 loss: 2.123455686569214
  batch 250 loss: 2.163464164733887
  batch 300 loss: 2.1784564566612246
  batch 350 loss: 2.114744391441345
  batch 400 loss: 2.1214977860450746
  batch 450 loss: 2.1763770389556885
  batch 500 loss: 2.199455904960632
  batch 550 loss: 2.3083423280715945
  batch 600 loss: 2.2298298573493955
  batch 650 loss: 2.300411410331726
  batch 700 loss: 2.3387850689888
  batch 750 loss: 2.2642895221710204
  batch 800 loss: 2.3332106018066407
  batch 850 loss: 2.286333296298981
  batch 900 loss: 2.2339772772789
LOSS train 2.23398 valid 2.23475, valid PER 63.60%
train_loss
[1.9019257926940918, 2.174738311767578, 2.0418694829940796, 2.147240936756134, 2.00642049074173, 2.002874820232391, 2.1247610569000246, 2.043101782798767, 2.0075221037864686, 2.0032868647575377, 2.0882557797431947, 2.0523867869377135, 2.180637083053589, 2.0171430349349975, 2.079545910358429, 1.9922025227546691, 2.0869039607048037, 2.0958490133285523, 2.111262254714966, 2.2339772772789]
valid_loss
[1.8746564388275146, 2.1902992725372314, 2.1182117462158203, 2.1187779903411865, 2.094601631164551, 1.9629807472229004, 2.1284008026123047, 2.057236671447754, 2.021653413772583, 2.0602052211761475, 2.1079518795013428, 2.053069829940796, 2.2578299045562744, 2.11716628074646, 2.069185256958008, 2.0636847019195557, 2.1020736694335938, 2.0735716819763184, 2.1826305389404297, 2.2347495555877686]
valid_per
[59.99866684442075, 68.57085721903746, 61.90507932275696, 62.27836288494867, 58.87881615784563, 58.50553259565391, 63.44487401679776, 62.27169710705239, 59.11211838421544, 60.058658845487265, 62.90494600719904, 61.53179576056525, 65.67124383415545, 61.278496200506595, 60.651913078256236, 61.21850419944007, 60.45193974136782, 61.40514598053593, 58.47220370617251, 63.60485268630849]
Training finished in 3.0 minutes.
Model saved to checkpoints/20231208_004731/model_1
Loading model from checkpoints/20231208_004731/model_1
SUB: 16.22%, DEL: 43.89%, INS: 0.44%, COR: 39.89%, PER: 60.56%
